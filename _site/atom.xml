<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>Tom 的Blog</title>
 <link href="http://tomsun.info/atom.xml" rel="self"/>
 <link href="http://tomsun.info"/>
 <updated>2015-03-31T13:37:16+08:00</updated>
 <id>http://tomsun.info</id>
 <author>
   <name>sunjianliang</name>
 </author>

 
 <entry>
   <title>goroutine contiguous stack</title>
   <link href="http://tomsun.info/2014/10/20/goroutine-contiguous-stack.html"/>
   <updated>2014-10-20T00:00:00+08:00</updated>
   <id>http://tomsun.info/2014/10/20/goroutine-contiguous-stack</id>
   <content type="html">&lt;p&gt;在我们学习关于golang goroutine的文章时，或多或少很多类似的断言：相比于linux pthread，在 golang中我们可以很轻松的创建100k＋的goroutine，而不用担心其带来的开销，其中一个原因是goroutine初始stack非常小，在当前release的1.3 版本中，一个goroutine初试创建只需要4K 的stack，而linux pthead 则需要2M或者更多的stack空间，那到底是不是这样的？&lt;/p&gt;

&lt;p&gt;如下在一个进程中创建100个线程，主进程和线程sleep的方式简单测试下pthread 线程初试创建占用的内存资源。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void* func(void* arg){ 
    while (true){ 
         usleep(1000000); 
    } 
}
int main(int argc, char* argv[]) {
    pthread_t tid; 
    int n = 10000; 
    while (n != 0) { 
         if (pthread_create(&amp;amp;tid, NULL, func, &amp;amp;n) != 0 ) {
             printf(&quot;create fail&quot;); 
         } 
         n = n - 1; 
    } 
    while (1) { usleep(10000000); } 
 } 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过ps看到的资源使用情况如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND 
12768    21828  4.9  1.0 81976776 83988 pts/3  Sl+  08:08   0:01 ./a.out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到创建10000个线程之后，该程序实际占用的内存（RSS）为83988KB，算下来每个线程所占用的内存空间才8K左右，远远不是很多文章中所说的1M或者时2M or more。&lt;/p&gt;

&lt;h2&gt;pthread 线程堆栈&lt;/h2&gt;

&lt;p&gt;通过strace分析phtread_create 得到如下结果&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mmap(NULL, 8392704, 
    PROT_READ|PROT_WRITE,MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, 
    -1, 0) = 0x7fa274760000
brk(0)                                  = 0x1fd2000
brk(0x1ff3000)                          = 0x1ff3000
mprotect(0x7fa274760000, 4096, PROT_NONE) = 0
clone(child_stack=0x7fa274f5ffd0,
     flags=CLONE_VM|CLONE_FS|CLONE_FILES|CLONE_SIGHAND|
           CLONE_THREAD|CLONE_SYSVSEM|CLONE_SETTLS|
           CLONE_PARENT_SETTID|CLONE_CHILD_CLEARTID,  
     parent_tidptr=0x7fa274f609d0, tls=0x7fa274f60700,    
     child_tidptr=0x7fa274f609d0) = 31316
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到，在调用pthread_create的时候，首先使用mmap分配了8392704 Byte（8196kB）堆栈空间，但是在创建线程的时候，如果不指定堆栈大小，理应使用系统定义的默认最大空间， 通过ulimit -s 可以看到值为8192kB&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hzsunjianliang@inspur1:~/github/golang$ ulimit -s
8192
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;mmap多映射了4k（一页），可以看到在mmap之后又调用mprotect将堆栈尾部空间的权限设置为PROT_NONE即不可读写和执行，所以基本可以判断多mmap的1页内存空间主要是用于内存溢出情况下的检测。最后将mmap返回的堆栈作为clone的一个参数创建一个线程。 通过mmap分配的内存并不会直接映射为实际使用的物理内存空间，只有当实际使用的时候，在发现当前虚拟地址空间没有分配实际无力内存的情况下，会触发操作系统缺页中断从而再分配实际物理内存。&lt;/p&gt;

&lt;h2&gt;golang  contiguous stack 实现&lt;/h2&gt;

&lt;p&gt;goroutine作为golang的独立调度单元，每个goroutine能够独立运行的重要元素为其独立的栈空间，golang 1.2的实现类似于pthread，分配固定大小的空间，由于是固定的所以即不能太大也不能太小。而Go1.3 引入了 contiguous stack，可以在goroutine初试创建时分配非常小的栈空间（1.3为4k，后续1.5roadmap中说到会减到2k），在使用过程中自动进行增长和收缩。这使得我们可以在golang中创建很多很多的goroutine而不用担心内存耗尽。这激发我们编写各种各样的并发模型而不用太担心其可能对内存照成很大的开销。&lt;/p&gt;

&lt;h2&gt;实现原理&lt;/h2&gt;

&lt;p&gt;golang在每次执行函数调用的时候，首先，其runtime会检测当前的栈空间是否足够使用，如果不够使用，会触发类似“缺页中断”，Go 的runtime会保存此事函数的上下文环境，然后malloc一块内存，将旧堆栈的内存copy到新的堆栈，并做一些合理的调整。当函数返回的时候，函数会在新的堆栈中继续运行，仿佛整个过程啥事都没发生过。所以理论上来说goroutine可以使用“无限大的堆栈空间”&lt;/p&gt;

&lt;h2&gt;实现细节&lt;/h2&gt;

&lt;p&gt;Go的运行库中，每个goroutine对应一个结构体G（类似于linux操作系统的中进程控制块），此结构中保存有stackbase 和stackguard用于定义其使用的栈信息，每次函数调用时候都会检测当前函数需要使用的栈空间是否够用，如果不够用就进行扩张。&lt;/p&gt;

&lt;p&gt;接下来我们分析golang的汇编代码进行分析&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main 
import  &quot;fmt&quot;  
func main(){ 
    a := 1 
    strb := &quot;hello &quot; 
    a = a + 1 
    strb += &quot;world&quot; 
    fmt.Print(a, strb) 
    main()  
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;go tool 6g -S continuousStack.go | head -8&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&quot;&quot;.main t=1 size=352 value=0 args=0 locals=0xb8
000000 00000 (continuousStack.go:5) TEXT         &quot;&quot;.main+0(SB),$184-0
000000 00000 (continuousStack.go:5) MOVQ    (TLS),CX
0x0009 00009 (continuousStack.go:5) LEAQ    -56(SP),AX
0x000e 00014 (continuousStack.go:5) CMPQ    AX,(CX)
0x0011 00017 (continuousStack.go:5) JHI ,26
0x0013 00019 (continuousStack.go:5) CALL,runtime.morestack00_noctxt(SB)
0x0018 00024 (continuousStack.go:5) JMP ,0
0x001a 00026 (continuousStack.go:5) SUBQ    $184,SP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面可以看到，在进入main函数之后，首先从TLS中取得第一个字段，也就是g－&gt;stackguard字段，然后将当前SP值减去函数预计将要使用的局部堆栈空间56byte，如果得到的值小于stackguard则表示当前栈空间不够使用，需要调用runtime.morestack分配更大的堆栈空间。&lt;/p&gt;

&lt;p&gt;more：[连续栈][1]&lt;/p&gt;

&lt;h2&gt;参考资料&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;https://github.com/tiancaiamao/go-internals/blob/master/ebook/03.5.md&lt;/li&gt;
&lt;li&gt;https://docs.google.com/document/d/1wAaf1rYoM4S4gtnPh0zOlGzWtrZFQ5suE8qr2sD8uWQ/pub&lt;/li&gt;
&lt;li&gt;http://stackoverflow.com/questions/6270945/linux-stack-sizes&lt;/li&gt;
&lt;li&gt;http://www.unix.com/unix-for-dummies-questions-and-answers/174134-kernel-stack-vs-user-mode-stack.html&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>goroutine 调度器</title>
   <link href="http://tomsun.info/2014/09/24/goroutine-scheduler.html"/>
   <updated>2014-09-24T00:00:00+08:00</updated>
   <id>http://tomsun.info/2014/09/24/goroutine-scheduler</id>
   <content type="html">&lt;p&gt;对golang的goroutine的内部实现感觉非常神秘，通过这段时间的学习基本了解了golang调度器实现原理。&lt;/p&gt;

&lt;h4&gt;golang调度器&lt;/h4&gt;

&lt;p&gt;虽然golang的最小调度单元为协程（goroutine），但是操作系统最小的调度单元依然还是线程，所以golang scheduler（golang调度器）其要做的工作是如何将众多的goroutine放在有限的线程上进行高效而公平的调度。&lt;/p&gt;

&lt;p&gt;操作系统的调度不失为高效和公平，比如&lt;a href=&quot;http://www.ibm.com/developerworks/library/l-completely-fair-scheduler/&quot;&gt;CFS&lt;/a&gt;调度算法，那么go为何要引入goroutine？原因很多，有人会说goroutine 相比于linux的pthread机制使用很方便。但是核心原因为goroutine的轻量级，无论是从进程到线程，还是从线程到协程，其核心都是为了使得我们的调度单元更加轻量级，我们可以轻易得创建几万几十万的goroutine而不用担心内存耗尽等问题。golang引入goroutine试图在语言内核层做到足够高性能得同时（充分利用多核优势、使用epoll高效处理网络／IO、实现垃圾回收等机制）尽量简化编程。&lt;/p&gt;

&lt;p&gt; ps：人类社会的发展是生产工具不断发展解放生产力的过程，语言的发展也是一样，从机器语言到汇编语言、C语言、面向对象C＋＋\ Java以及到现在层出不穷的动态语言。编程语言作为生产工具，其发展核心目的为最大化IT工作人员的生产力。从这点看，我很看好go语言，极简得编程之道简直大爱。&lt;/p&gt;

&lt;p&gt; 以下基于Daniel Morsing的&lt;a href=&quot;http://morsmachine.dk/go-scheduler&quot;&gt;一篇文章&lt;/a&gt;介绍goroutine调度器。&lt;/p&gt;

&lt;p&gt; 首先基于线程，用户态协程可以选择以下3种调度机制。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;N:1&lt;/strong&gt;  即多个用户态协程运行在一个os线程上，这种方式的优点是可以很快得进行上下文切换，但是缺点是不能利用多核优势。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1:1&lt;/strong&gt; 一个用户态协程对应一个os线程，这种方式得优点是可以利用到多核的优势，但是协程的调度完全依赖于os线程的调度，而os线程的调度的上线文切换的代价又比较大，从而导致这种模型调度的上下文切换代价比较大。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;M:N&lt;/strong&gt; golang scheduler 使用的m:n调度模型，即任意数量的用户态协程可以运行在任意数量的os线程上，这样不仅可以使得上线文切换更加轻量级，同时又可以充分利用多核优势。 为了实现这种调度机制，golang 引入如下3个大结构 &lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/files/2014/09/24/our-cast.jpg&quot; alt=&quot;our-cast.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;M：os线程（即操作系统内核提供的线程）&lt;/p&gt;

&lt;p&gt;G：goroutine，其包含了调度一个协程所需要的堆栈以及instruction pointer（IP指令指针），以及其他一些重要的调度信息。&lt;/p&gt;

&lt;p&gt;P：M与P的中介，实现m:n 调度模型的关键，M必须拿到P才能对G进行调度，P其实限定了golang调度其的最大并发度。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/files/2014/09/24/in-motion.jpg&quot; alt=&quot;in-motion.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如上图所示，2个M分别拿到context P在运行G，M只有拿到context P才能执行goroutine。被执行的goroutine在运行过程中调用 go func() ，会创建一个新的对应func() 的goroutine，并将这个goruotine加入到runqueue（就绪待调度的goroutine队列，如上图灰色部分所示），当前运行的goroutine在达到调度点（系统调用、网络IO、等待channel等）的时候，P会挂起当前运行的goroutine，从runqueue中pop一个goroutine，重新设置当前M的执行上下文继续执行（即设置为pop出来的goroutine对应的运行堆栈以及IP（instruction Point））。&lt;/p&gt;

&lt;p&gt;仔细观察我们可以发现，与golang的前一个版本的调度器不同，当前并不是采用全局的runqueue队列，而是每个P都对应有自己的一个local的runqueue，这样可以避免每一次调度都进行一次锁竞争，在32核机器上，锁竞争会导致性能变得非常糟糕。&lt;/p&gt;

&lt;p&gt;P的数量由用户设置的GOMAXPROCS决定，当前不论GOMAXPROCS设置为多大，P的数量最大为256。由于M必须拿到P才能够对G进行调度，所以P实际上限制了go的最大并发度，256对于现在的服务器已经足够使用了。因为P并不会由于当前调度的goroutine阻塞而不可用，只要当前还有可调度的goroutine，P始终会使用M继续进行调度运行。所以其实P只需要设置为当前CPU的最大核数即可。&lt;/p&gt;

&lt;p&gt;如果一切正常，调度器会以这样一种方式简单得运行，以下分析goroutine在两种例外情况下的行为。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;系统调用&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/files/2014/09/24/syscall.jpg&quot; alt=&quot;syscall.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;golang语言能够控制线程M在用户态执行情况下的调度行为，但是却不能控制线程在陷入内核之后的行为。即在我们调用system call陷入内核没有返回之前，go其实已经控制不了。那怎么办呢？如果当前陷入system call的线程在持有P的情况下Block很长时间，会导致其他可运行的goroutine由于没有可用的P而不能得到调度。&lt;/p&gt;

&lt;p&gt; 为了保证调度的并发型，即保证拿到P的M能够持续进行调度而不是block，golang 调度器也没有很好的办法，只能在进入系统调用之前从线程池拿一个线程或者新建一个线程的方式，将当前P交给新的线程M1从而使得runqueue中的goroutine可以继续进行调度。当前M0则block在system call中等待G0返回。 在G0返回之后需要继续运行，而继续运行的条件是必须拥有一个P，如果当前没有可用的P，则将G0放到全局的runqueue中等待调度，M0退出或者放入线程池睡觉去。而如果有空闲的P，M0在拿到P之后继续进行调度。（P的数量很好的控制了并发度）&lt;/p&gt;

&lt;p&gt; P在当前local runqueue中的G全部调度完之后从global runqueue中获取G进行调度，同样系统也会定期检查golobal runqueue中的G，以确保被放入global runqueue中的goroutine不会被饿死。&lt;/p&gt;

&lt;p&gt;  PS：golang对这层调度其实做了一定的优化，不是在一开始进行系统调用之前就新建一个新的M，而是使用一个全局监控的monitor（&lt;span style=&quot;color: #333333;&quot;&gt;sysmon），定期检查进入系统调用的M，只有进入时间过长才会新建一个M。另外golang 底层基本对所有的IO都异步化了，比如网络IO，golang底层在调用read返回EAGAIN错误的时候会将当前goroutine挂起，然后使用epoll监听这个网络fd上的可读事件，在当有数据可读的时候唤醒对应的goroutine继续进行调度。其中epoll事件管理线程即golang虚拟机的sysmon线程。Go语言网络库的基础实现相详见&lt;a href=&quot;http://skoo.me/go/2014/04/21/go-net-core/&quot; target=&quot;_blank&quot;&gt;此文&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;stealing work&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/files/2014/09/24/steal.jpg&quot; alt=&quot;steal.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;为了使得调度更加公平和充分，golang引入了&lt;a href=&quot;http://supertech.csail.mit.edu/papers/steal.pdf&quot; target=&quot;_blank&quot;&gt;work steal&lt;/a&gt;调度算法。 在P local runqueue上的goroutine全部调度完了之后，对应的M不会傻傻得等在那里睡觉，而首先会尝试从global  runqueue中获取goroutine进行调度。如果golbal runqueue中没有goroutine，如上图所示，当前M会从别的M对应P的local runqueue中抢一半的goroutine放入自己的P中进行调度。&lt;/p&gt;

&lt;h4&gt;goroutine状态迁移&lt;/h4&gt;

&lt;p&gt;从上面的介绍可以发现，调度器会使goroutine在各种状态来回切换。下图使用&quot;goroutine状态迁移图&quot;来形象得描述goroutine在调度周期中的生老病死。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/files/2014/09/24/goroutinestate.jpg&quot; alt=&quot;goroutinestate.jpg&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title> 存储一致性之复制</title>
   <link href="http://tomsun.info/2014/09/02/stroage_consistency_replicate.html"/>
   <updated>2014-09-02T00:00:00+08:00</updated>
   <id>http://tomsun.info/2014/09/02/stroage_consistency_replicate</id>
   <content type="html">&lt;p&gt;难得一见的关于&lt;a href=&quot;http://www.ibm.com/developerworks/library/l-completely-fair-scheduler/&quot;&gt;分布式理论系列文章&lt;/a&gt;，非常之精彩，通读这一系列文章，发现作者没有站在专家的立场上以复杂得方式分析复杂的问题，而是从工程师的角度非常恰当得介绍一系列分布式的理论，对分布式理论各个方面总结概括得非常到位，令人赏心悦目。&lt;/p&gt;

&lt;p&gt;以下为自己翻译的第四部分，英文原文详见: &lt;a href=&quot;http://book.mixu.net/distsys/replication.html&quot;&gt;Replication&lt;/a&gt;。&lt;/p&gt;

&lt;h1&gt;译文&lt;/h1&gt;

&lt;h2&gt;1 复制问题&lt;/h2&gt;

&lt;p&gt;复制问题是分布式系统中众多问题之一，我选择把重点放在与此相关的一些方面，比如leader选举、故障检测、互斥锁定以及全局快照，因为往往这些问题是大家最感兴趣的。例如，很多并行数据库的区别之一往往就是它们的复制。复制功能本身引入了很多的问题，如leader选举、错误检测、一致性以及原子光广播等。&lt;/p&gt;

&lt;p&gt;复制是一个多节通讯问题。什么样的布局和消息交互模式能够满足我们想要的性能及可用性需求？在网络分区及节点故障情况下如何保证容错性、持久性？&lt;/p&gt;

&lt;p&gt;实现复制的方式有很多，这里不会讨论某种具体的方式而是从更高的概括性角度讨论复制相关的问题。这样有助于从整体方面进行把握，而不是局限于某个具体形态。因为我的目标是探索设计的空间而不是具体的某个算法。&lt;/p&gt;

&lt;p&gt;首先我们简单定义下复制问题：假设初始状态一样的数据库，客户端发送请求来改变各副本的状态。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/files/2014/09/replication-both-300x206.png&quot; alt=&quot;replication-both-300x206&quot; /&gt;&lt;/p&gt;

&lt;p&gt;通信的流程可以被分解为如下阶段：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;（Request）客户端发送请求到服务端&lt;/li&gt;
&lt;li&gt;（Synchronous）进行同步副本复制&lt;/li&gt;
&lt;li&gt;（Response）响应返回给客户端&lt;/li&gt;
&lt;li&gt;（Asynchronous）进行异步副本复制 （同步复制下这步缺省）&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;这种松散的模式是基于此文&lt;a href=&quot;https://www.google.com/search?q=understanding+replication+in+databases+and+distributed+systems&quot;&gt;understanding replication in databases and distributed systems&lt;/a&gt;的。注意其中每阶段消息的具体通信方式取决于特定的算法，我会尝试绕过具体的算法讨论这些问题。&lt;/p&gt;

&lt;p&gt;基于如上的基本步骤，我们能够创造什么样的消息交互方式呢？不同的消息交互方式会对性能和可行性造成什么样的影响？&lt;/p&gt;

&lt;h3&gt;1.2 Synchronous replication-同步复制&lt;/h3&gt;

&lt;p&gt;所述第一种模式为同步复制，如下图所示:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/files/2014/09/replication-sync-300x274.png&quot; alt=&quot;replication-sync-300x274&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到流程如下：首先，客户端发起请求；然后，进行同步复制；最后，客户端待同步完成后获得应答。在同步阶段，S1与其他服务节点通信，直到收到所有其他节点的响应，最后，S1通知客户端操作结果（成功或者失败）.&lt;/p&gt;

&lt;p&gt;整个过程似乎简单明了。抛开同步的具体算法，我们讨论这种消息交互的特点：首先，它是Write N of N的方式，在响应返回之前，它必须被所有服务节点看到并确认。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;优缺点&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;从性能角度来说，这意味着系统性能取决于最慢的那台服务器（木桶短板），并且系统对网络延迟非常敏感。&lt;/p&gt;

&lt;p&gt;给定 Write N of N 方式，系统无法忍受任何一台服务器的故障。当一台服务器故障，系统不能完成N个节点的写操作，系统即无法继续运行。在此情况下，只能提供对数据的读取服务，在这样的系统设计下，修改更新将不能继续进行。&lt;/p&gt;

&lt;p&gt;这样的安排可以提供很强的持久化保证。当响应成功返回，客户端可以确认所有N台服务器都已经收到并且持久化。所有N个副本丢失才会导致这个更新丢失。&lt;/p&gt;

&lt;h3&gt;1.3 Asynchronous replication – 异步复制&lt;/h3&gt;

&lt;p&gt;让我们对比下第二种模式—异步复制（也称为被动复制、拉复制或懒惰复制），正如你已经猜到的，z是同步模式的对立模式。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/files/2014/09/replication-async-300x263.png&quot; alt=&quot;replication-async-300x263&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这里，主节点（leader/coordinator）在更新刷新到本地之后会立即响应客户端。不会阻塞得进行同步工作，客户端不用被迫等待很多轮服务器间通讯。 在以后的某个阶段会进行异步复制，具体的方式取决于特定的算法。&lt;/p&gt;

&lt;p&gt;我们抛开具体的算法细节来讨论这种模式。ok，这是 Write 1 of N的方式；响应会立刻返回，在随后的某个时间点，更新才会传播到其他节点。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;优缺点&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;从性能的角度来说，这意味着系统非常快：客户端不需要花费任何额外的时间等待系统内部做好自己的工作，该系统也不容易受到网络延迟的影响。&lt;/p&gt;

&lt;p&gt;这样的方式只能提供较弱的，概率性的持久性保证。如果一切正常，该数据最终复制被到所有N台机器，然而，如果在此之前包含数据的那台服务器故障，那么数据可能会永久丢失。&lt;/p&gt;

&lt;p&gt;给定 Write 1 of N的方式，只要有一个节点可用，该系统继续保持可用。但是这种偷懒的做法没有提供很好的持久性和一致性保证，如果故障发生，可以继续写入系统，但是不能保证可以读取到你之前写入的数据。&lt;/p&gt;

&lt;p&gt;最后，值得一提的是被动复制无法保证系统中所有节点总是包含相同的状态。如果允许在多个节点进行写，且不需要其他节点同步协调，那么会遇到冲突或者分歧的风险：在不同的地方可能会读到不同的结果（特别是节点出现故障和恢复的情况下），全局约束无发得到保障。&lt;/p&gt;

&lt;p&gt;我还没有讨论这两种通讯模式下的读模式。读模式需要遵循写入模式，在读的情况下，我们往往希望尽可能少得与节点通信，这些将会下文的多数派（quorum）部分详细讨论。&lt;/p&gt;

&lt;p&gt;以上我们只讨论了两种基本的模式，并没有深入某个特定的算法。至此，我们应该能够想到很多可能的消息交互方式以及它们在性能、持久性、可用性等方面的特点。&lt;/p&gt;

&lt;h2&gt;2 An overview of major replication approaches&lt;/h2&gt;

&lt;p&gt;在讨论了两种基本的复制方式：同步异步之后，让我们来看看主要的复制算法。&lt;/p&gt;

&lt;p&gt;有许多不同的方法对复制技术进行分类，基于第一部分同步异步之后，接下来会从以下两个方面进行介绍。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;防止分歧（单拷贝系统）的复制方法&lt;/li&gt;
&lt;li&gt;可能产生分歧（多master系统）的复制方法&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;第一种方法具有“behave like a single system”的特点，在局部故障发生的时候，系统保证只有单一副本是出于激活状态（不会产生副本间分歧），此外，该系统可以保证副本总是一致的，也就是所谓的共识问题(consensus problem)。&lt;/p&gt;

&lt;p&gt;所谓共识，就是一些进程（或者计算机），就一个对象的值达成一致协定。更加正式概括如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Agreement: 约定，每一个正确的process必须决定相同的值。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Intergrity: 完整性，每一个正确的process至多决定一个值，并且如果决定了这个值，这个值肯定是其中一个process提出的。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Termination：收敛性，所有process最终会达成一个一致的决定。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Validity：有效性&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;互斥问题、leader选举、多播以及原子广播都是属于需要达成共识的问题。维护一致性的复制系统必须通过某种方式解决这个问题。&lt;/p&gt;

&lt;p&gt;对维护副本一致性算法可以进行如下分类:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1N 消息 （异步 primary/backup）&lt;/li&gt;
&lt;li&gt;2N 消息 （同步 primary/backup）&lt;/li&gt;
&lt;li&gt;4N 消息 （两阶段提交、Multi-Paxos）&lt;/li&gt;
&lt;li&gt;6N 消息  (3阶段提交协议、基本Paxos（没有Leader)）&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;这些算法在容错性方面各不相同，我之所以通过消息的交互次数来区分这些协议主要目的是想回答一个问题，即&lt;strong&gt;“每多一次附加的消息交互为了换来什么？”&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;下图引用Ryan Barret的图片来描述不同算法的基本特点。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/files/2014/09/google-transact09.png&quot; alt=&quot;google-transact09&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图中包含&lt;strong&gt;一致性&lt;/strong&gt;、&lt;strong&gt;延迟&lt;/strong&gt;、&lt;strong&gt;吞吐量&lt;/strong&gt;、&lt;strong&gt;数据丢失&lt;/strong&gt;及&lt;strong&gt;故障切换&lt;/strong&gt;这些系统特性。我们可以追溯到之前提到的两种复制方法：同步复制及异步复制；当选择等待(blocking)，你会得到更差的性能却更强的数据保证。当我们讨论分区容忍性时（网络延迟或故障）两阶段协议（2PC）和多数派协议（Quorum）在吞吐量上会存在很大差别。&lt;/p&gt;

&lt;p&gt;图中，弱一致性算法和最终一致性算法被笼统得归类为&lt;strong&gt;gossip&lt;/strong&gt;，我会在接下来的第五章详细讨论一些弱一致性复制方法－gossip及quorum。&lt;/p&gt;

&lt;p&gt;值得注意的是，弱一致性系统通用的算法较少，却有很多可选的方法。因为对待这样的分布式系统可以简单得看成是多个节点而非整体系统，这类系统没有明确得需要解决的问题，更多的是告诉大家（使用）我是弱一致性的，具备所有弱一致性系统所具有的特点。&lt;/p&gt;

&lt;p&gt;接下来我们先来看维护单一副本一致性的系统。&lt;/p&gt;

&lt;h3&gt;2.1 Primay/backup replication&lt;/h3&gt;

&lt;p&gt;主从复制可能是最基本最常用的复制方法，所有的更新都发送到主节点，然后将操作日志通过网络复制到备份节点，有两个变种：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;异步主从复制&lt;/li&gt;
&lt;li&gt;同步主从复制
同步需要两次信息交换（“更新” + “确认”），而异步只需要一次“更新”。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;主从复制非常之普遍。例如，默认情况下，MySQL和MongoDB的复制使用异步主从方式。所有操作都是由主节点串行并持久化之后异步复制到备份服务器。&lt;/p&gt;

&lt;p&gt;正如我们在前面异步背景下讨论的，任何一种异步复制算法只能提供弱持久化保证。在Mysql中表现为复制滞后，如果主失败，尚未被发送到备份则有可能会导致更新丢失。&lt;/p&gt;

&lt;p&gt;同步主从方式保证数据在从节点持久化之后响应客户端，这就需要客户端等待，但是这种方式同样只能提供比较弱的保证，考虑如下简单的失败场景：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;主副本收到写请求并发送到从节点&lt;/li&gt;
&lt;li&gt;从副本持久化并响应主副本&lt;/li&gt;
&lt;li&gt;主副本在响应客户端之前出故障&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;这种情况下，客户端只能假设请求失败，但是从节点却提交了更新，如果直接将从节点提升为主节点，则会出错，这时候就必须人工介入了。&lt;/p&gt;

&lt;p&gt;这里简化了讨论，虽然所有主备算法遵循基本一样的消息交换方法，但是在故障恢复等方面会有所不同。
基于主从复制的方案只能提供尽力而为的保证（节点的异常很容易会造成数据丢失或者错误更新），并且非常容易受到网络延迟的影响。&lt;/p&gt;

&lt;p&gt;基于主备方式的关键是，它们只能提供尽力而为的保证（节点在不合时宜的失败或者不正确的更新都有可能导致更新丢失）。此外，P/B方案也非常容易受到网络分区的影响。&lt;/p&gt;

&lt;p&gt;为了避免突然的故障导致不能保证一致性，我们需要添加新一轮消息，也就是接下来讨论的”两阶段提交协议”（2PC）。&lt;/p&gt;

&lt;h3&gt;2.2 Two phase commit (2PC)&lt;/h3&gt;

&lt;p&gt;两阶段提交（2PC）在许多经典的关系数据库中使用，例如，MySQL 集群使用2PC协议实现同步复制。消息基本如下&lt;/p&gt;

&lt;p&gt;[ Coordinator ] -&gt; OK to commit?         [ Peers ]
&amp;lt;- Yes / No&lt;/p&gt;

&lt;p&gt;[ Coordinator ] -&gt; Commit / Rollback [ Peers ]
&amp;lt;- ACK&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;第一阶段（投票阶段）&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;coordinator协调者发送更新操作到所有的参与者（participants）,每个参与者处理更新并且投票提交请求(commit)或者取消提交(abort)，当选择提交请求的时候，更新操作会持久化到临时区（write-ahead log），直到第二阶段完成之前，更新始终是临时的。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;第二阶段（决定阶段）&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;coordinator决定最终结果并且通知参与者。如果所有参与者都投票“提交请求(commit)”,更新会从临时区区出来进行持久化。&lt;/p&gt;

&lt;p&gt;在提交请求并持久化之前进行第二阶段的确认是有效的，因为这样在相关节点失败情况下允许回滚操作。然而在之前提到的主备协议中没有回滚，这会导致多个副本之间产生分歧。&lt;/p&gt;

&lt;p&gt;2PC协议很容易出现阻塞，因为单个节点的故障（参与者或协调者）都会导致系统无法继续运行。然而恢复由于有第二个阶段的存在恢复往往是可行的。注意2PC协议假设数据是持久保存的并且所有节点不会丢失数据并且不会永远crash。实际上在持久化存储失效情况下数据丢失仍旧是可能的。&lt;/p&gt;

&lt;p&gt;两阶段协议的恢复细节非常复杂这这里不会详细进行说明，其主要工作是保证数据持久化并且保证恢复正确（即根据这一轮提交的结果进行redoing或者undoing）&lt;/p&gt;

&lt;p&gt;正如我们在&lt;a href=&quot;http://book.mixu.net/distsys/abstractions.html&quot;&gt;上一节&lt;/a&gt;中提到的CAP，2PC协议属于CA，不具有分区容错特性。2PC不能处理网络分区的错误场景，在节点失效（或者分区）情况下只能等到恢复之后才能继续运行。如果coordinator失败必须进行人工介入。2PC协议同样对网络延迟非常敏感。因为2PC还是采用了write N of N的方式，直到最慢的节点确认之后写入才能继续进行。&lt;/p&gt;

&lt;p&gt;2PC协议在性能和容错性方面做了权衡取舍，在传统的关系型数据库中非常流行。然而，当前新的系统经常使用具有分区容错性的一致性算法。因为此类算法可以在短暂的网络分区之后自动恢复并且能够更加优雅得处理节点之间的延迟。&lt;/p&gt;

&lt;p&gt;接下来让我们继续分析分区容错性一致性算法。&lt;/p&gt;

&lt;h3&gt;2.3 Partition tolerant consensus algorithms(分区容错性一致性算法)&lt;/h3&gt;

&lt;p&gt;我们接下来讨论的分区容错一致性算法为维护单副本一致性的容错算法。还有另外一类容错算法:容忍拜占庭（Byzantine)错误，这样的算法很少应用于商业系统，因为这类系统非常昂贵并且难以实现，因此这里不会涉及到此类算法。&lt;/p&gt;

&lt;p&gt;谈到具备分区容忍特性的一致性算法，其中最知名的为Paxos算法。但是由于它非常难以实现和解释。我会把重点放在更加容易教授和实现的算法—Raft算法。让我们先来看下网络分区和分区容忍一致性算法的一般特性。&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;什么是网络分区？&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;网络分区是指：到一个或者多个节点的网络链接出现故障。那些无法到达的节点本身可能继续保持活跃，甚至可以接受来自客户端的请求。正如我们在前面章节所学到的CAP理论，在发生网络分区的时候并不是所有的系统都能够从容应对。&lt;/p&gt;

&lt;p&gt;网络分区之所以如此棘手，是由于在分区发生的时候几乎不可能区分节点是故障宕机还是网络故障导致不可达。如果是网络分区，但是节点并没有出现故障，系统很可能分裂成两个同时激活的分区。下面两张图说明了网络分区和节点出现故障的情况，非常相似。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;系统包含两个节点，节点故障 vs 网络分区:&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/media/files/2014/09/system-of-2-300x87.png&quot; alt=&quot;system-of-2-300x87&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;系统包含三个节点，节点故障 vs 网络分区:&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/media/files/2014/09/system-of-2-300x87.png&quot; alt=&quot;system-of-3-300x138&quot; /&gt;&lt;/p&gt;

&lt;p&gt;保证单副本强一致性的系统必须使用某些方法来打破这种对称的僵局：否则，它会分裂成独立的系统，不能再维持单副本一致性。&lt;/p&gt;

&lt;p&gt;由于CAP理论表明网络分区是不可能避免的，所以具备分区容忍能力的系统在网络分区发生的时候必须确保只有一个分区仍然有效。&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Majority decisions（多数派决定）&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;这就是为什么分区容忍一致性算法依赖于多数派投票（即CAP理论）。在更新的时候，依赖于多数派节点，而不是所有节点（2PC协议），这使得此类协议可以容忍少数节点宕机以及网络故障导致的延迟和不可达。在N个节点中只需要（N/2 + 1）个节点存活并且可达，系统继续正常运行。&lt;/p&gt;

&lt;p&gt;分区容忍一致性算法使用奇数个节点（例如：3、5、7）。2个节点无法形成有效多数派；如果节点数为3，则可以容忍1个节点故障；节点数为5则可以容忍2个节点故障。&lt;/p&gt;

&lt;p&gt;在网络分区发生的时候，两个分区将不对称。其中一个分区包含多数派（N/2 +1）个数个节点。少数派分区将停止处理操作，以防止两个分区发生分歧；多数派分区继续正常运行。这样可以确保系统中只有一个分区能够正常运行。&lt;/p&gt;

&lt;p&gt;多数派在容忍分歧方面同样非常有效：如果出现骚动或者失败，节点的投票可能各不相同，但是多数派的决定只有一个，暂时的分歧会导致协议block但是不会违反单副本一致的特性。&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Role (角色)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;构建此类系统有两种方法：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;所有的节点角色都相同，包含相同的功能&lt;/li&gt;
&lt;li&gt;节点具有单独不同的角色和不同的功能&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;一致性复制算法一般选择第2种方式：即选定某节点为leader或者master的方式，这样可以使得系统更加高效。这是由于所有的更新操作必须通过leader节点序列化，非leader节点只需要转发请求即可。（减小一轮消息交互）&lt;/p&gt;

&lt;p&gt;固定不同的角色并不排除系统在leader节点故障情况下的恢复。正常情况下指定不同角色并不表示在失败之后重新分配角色不能使得系统恢复；而是说明系统在选举出leader之后可以一直持续正常运行直到出现下一次节点或者网络故障。&lt;/p&gt;

&lt;p&gt;Paxos和Raft算法使用不同角色的方式。在一般情况下，leader节点（在paxos中为”proposer”）负责协调（即2PC中的coordination），其他节点则为follower（在paxos中为”accptors”或者”leaderners”）。&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Epoch(轮次)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Paxos算法和Raft算法每一轮正常的流程称为epoch（Raft中为”term”)。在对应每一个epoch期间只有一个节点被指定为leader。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/files/2014/09/epoch.png&quot; alt=&quot;epoch&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在选举完成之后，同一个leader始终会成为该轮次（epoch）阶段的coordinator（协调者）。从上图(摘自Raft)可以看到，leader节点的宕机会导致该轮次（epoch）立即结束。&lt;/p&gt;

&lt;p&gt;Epoch(轮次)在协议中充当逻辑时钟。这样可以允许节点能够辨别某些宕机或者delay之后继续加入的节点—“那些被分区或者停止运作的节点对应的epoch会比当前的小”；这使得某些尚未成功提交的请求被忽略，以确保不会使系统产生二义性。&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Leader changes via duels&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;所有节点刚开始的角色都是follower；在启动之后其中一个节点会被选举会leader。在正常操作流程中，leader会保持和follower之间的心跳以使系统可以检测leader失效或者产生网络分区。&lt;/p&gt;

&lt;p&gt;当follower节点检测到leader无响应，它会切换到中间状态（Raft中成为”candidate”状态）。在这个状态下，节点对当前自身的epoch/term做自增（epoch++），并发起leader选举竞选成为此轮epoch新的leader。&lt;/p&gt;

&lt;p&gt;为了成为leader，必须获得过半数的投票。分配选票的方式为FIFO方式，leader会最终被选举出来。一般来说，在每次竞选尝试中会随机等待一段时间以减少同时进行竞选的节点数。&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Numbered proposals within an epoch（一个轮次中带编号的请求）&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;在每一轮次中，leader会对每次需要表决的值进行提案（即序列化update command），在每一个轮次中，每个提案对应的数字是唯一且严格递增的。&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Normal operation&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;在正常运行期间，所有提案都必须经过leader节点。当客户端提交一个提案（如更新操作），leader联系所有多数派中的节点，如果当前没有leader竞选请求存在（基于多数派中follower的响应），leader提案值有效。并且如果其多数派中的follower accept该提案，那么这个提案被接受。&lt;/p&gt;

&lt;p&gt;由于很可能另外一个节点也正尝试作为一个leader进行提案，必须保证，一旦一个提案被accept，它的值永远无法被改变。否则，一个已经成功提交的请求可能会被撤销。Lamport在paxos算法中描述如下：&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;P2: If a proposal with value v is chosen, then every higher-numbered proposal that is chosen has value v.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;这一限制需要所有的follower和propser遵循:一旦一个提案值被一个多数派接受，那么这个提案值不能被改变.（注意”提案值不能改变”对应于算法的一次执行。典型的复制算法对每一次提交执行一次一致性算法，为了解释得更加简单易懂，往往大家专注于算法的一直执行进行详细讨论）&lt;/p&gt;

&lt;p&gt;为了保证这个特性，提案者必须首先询问follower他们已经接受的编号最大的提案对应的值。如果提案者发现已经存在一个提案，那么它必须试图完成这个已经存在的提案，而不是进行重新提案。Lamport在paxos算法中描述如下：&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;P2b. If a proposal with value v is chosen, then every higher-numbered proposal issued by any proposer has value v.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;更加具体的：&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;P2c. For any v and n, if a proposal with value v and number n is issued [by a leader], then there is a set S consisting of a majority of acceptors [followers] such that either (a) no acceptor in S has accepted any proposal numbered less than n, or (b) v is the value of the highest-numbered proposal among all proposals numbered less than n accepted by the followers in S.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;这是Paxos算法的核心，同样也是其他类Paxos衍生算法的核心。提案的值直到协议的第二个阶段才能选定。提案者某些情况下必须重新进行第一阶段以保证他们可以自由得对当前一轮提案赋予自己的值。&lt;/p&gt;

&lt;p&gt;如果之前已经有多个提案存在(可能是还未决定的提案)，那么会选举标号最大的提案对应的值。提案者只有在其多数派中的节点没有一个提案竞争者的前提下才能选取自己的提案值。同时提案者要求follower见到此提案的同时，不能accept比这个提案编号更小的提案。&lt;/p&gt;

&lt;p&gt;把这两个部分结合起来，在Paxos算法中达成一个决定需要两轮消息交互。&lt;/p&gt;

&lt;p&gt;[ Proposer ] -&gt; Prepare(n)]                                      [ Followers ]&lt;/p&gt;

&lt;p&gt;&amp;lt;- Promise(n; previous proposal number
and previous value if accepted a
proposal in the past)&lt;/p&gt;

&lt;p&gt;[ Proposer ] -&gt; AcceptRequest(n, own value or the value          [ Followers ]&lt;/p&gt;

&lt;p&gt;associated with the highest proposal number
reported by the followers)&lt;/p&gt;

&lt;p&gt;&amp;lt;- Accepted(n, value)&lt;/p&gt;

&lt;p&gt;在prepare阶段，proposer（提案者）可以了解任何处于竞争状态或者之前已经决定的提案。第二个阶段（accept阶段）选举一个新的值或者之前已经被accept过的值。在某些情况下，假设同时两个proposer处于active状态（即同时进行提案）或者多数派节点故障情况下，可能没有一个propossal被多数派accept。但是这是可以接受的，因为成功的提案最终会收敛为一个有效的值。&lt;/p&gt;

&lt;p&gt;事实上，根据FLP理论，当消息传递边界不存在的情况下，一致性算法只能在safety或者liveness间二选一。Paxos算法选择放弃liveness保证safety：即提案可能无休止的进行下去，直到没有竞争leader（多个proposal）并且一个多数派accept提案。&lt;/p&gt;

&lt;p&gt;当然，实现这种算法非常复杂，即使在专家手中，一些很小的关注点可能会导致非常大的代码量。&lt;/p&gt;

&lt;p&gt;实用优化：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;通过leader租期（而不是心跳）避免重复的leader选举。&lt;/li&gt;
&lt;li&gt;在leader确定的稳定状态下避免第一阶段的propose消息交互。&lt;/li&gt;
&lt;li&gt;确保follower和proposers持久化的消息不被损坏。&lt;/li&gt;
&lt;li&gt;集群中节点的角色以安全的方式转换（在Paxos算法中依赖任意一个多数派总是有一个节点是相交的）&lt;/li&gt;
&lt;li&gt;在副本节点crash，磁盘丢失或者新节点加入情况下需要以安全有效的方式进行副本恢复。&lt;/li&gt;
&lt;li&gt;在合理的时间（均衡存储、容错需求）进行快照以及垃圾回收需要保证安全性。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;google “Paxos made live”这篇文章详细讨论了这系列挑战。&lt;/p&gt;

&lt;h3&gt;2.4 Partition-tolerant consensus algorithms: Paxos, Raft, ZAB&lt;/h3&gt;

&lt;p&gt;希望以上的讨论让你对“分区容忍性算法”如何工作有了基本的认识。我建议你通过进一步阅读了解不同算法的实现细节。&lt;/p&gt;

&lt;h4&gt;2.4.1 Paxos&lt;/h4&gt;

&lt;p&gt;paxos算法是设计具备分区容忍特性的强一致性系统必须了解的算法。该算法被许多google的系统使用。&lt;a href=&quot;http://research.google.com/pubs/pub36971.html&quot;&gt;BigTable/Megastore&lt;/a&gt;、GFS、&lt;a href=&quot;http://research.google.com/archive/spanner.html&quot;&gt;Spanner&lt;/a&gt;中使用的&lt;a href=&quot;http://research.google.com/archive/chubby.html&quot;&gt;Chubby lock Mananger&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;Paxos以希腊的一座岛屿的名词命名。由Leslie Lamport在1998年发表的”The Part-Time Parliament”文章中首次发表。Paxos算法被认为难以实现的，所以后续工业界发表了一系列文章探讨其实现细节（在后面的further reading 中可以看到）。&lt;/p&gt;

&lt;p&gt;Paxos算法中描述的往往是一致性算法的一次执行。而实际执行过程中，往往需要考虑高效得运行多轮一致性算法。这使得很多有兴趣（致力于）搭建基于Paxos协议的系统的开发者设计了很多基于paxos协议的扩展协议。此外，实际实现过程中还有很多挑战，比如如何维护集群成员的成员关系等。&lt;/p&gt;

&lt;h4&gt;2.4.2 ZAB&lt;/h4&gt;

&lt;p&gt;ZAB是Apache Zookeeper所使用的原子广播协议。Zookeeper 为分布式系统提供了协调者的角色（coordination primitives）。很多基于Hadoop的分布式系统（HBase、Storm、Kafka）都使用Zookeeper作为协调者（coordination）。Zookeeper基本上算是Chubby的开源实现版本。从技术上来讲原子广播和单纯的一致性协议问题有所不同，但是这类算法同样归属于”强一致性分布式容错算法”。&lt;/p&gt;

&lt;h4&gt;2.4.3 Raft&lt;/h4&gt;

&lt;p&gt;Raft是近期（2013年）加入本家族的算法。Raft比Paxos算法更加易于理解和学习，但是提供和Paxos算法同样的保证。特别的是，该算法的不同的部分被更加清理得分离开来，发表的paper中详细讨论了集群成员关系的变化。Raft在近期被类似zookeeper的开源系统etcd使用。&lt;/p&gt;

&lt;h3&gt;2.5 Replication methods with strong consistency&lt;/h3&gt;

&lt;p&gt;在以上本章节中，我们讨论了强一致性复制方法。从同步和异步开始对比，从简单开始逐步到能够容忍更加复杂故障的算法。以下总结了各种算法的关键特征：&lt;/p&gt;

&lt;p&gt;Primary/Backup&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Single, static master&lt;/li&gt;
&lt;li&gt;Replicated log, slaves are not involved in executing operations&lt;/li&gt;
&lt;li&gt;No bounds on replication delay&lt;/li&gt;
&lt;li&gt;Not partition tolerant&lt;/li&gt;
&lt;li&gt;Manual/ad-hoc failover, not fault tolerant, “hot backup”&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;2PC&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Unanimous vote: commit or abort&lt;/li&gt;
&lt;li&gt;Static master&lt;/li&gt;
&lt;li&gt;2PC cannot survive simultaneous failure of the coordinator and a node during a commit&lt;/li&gt;
&lt;li&gt;Not partition tolerant, tail latency sensitive&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Paxos&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Majority vote&lt;/li&gt;
&lt;li&gt;Dynamic master&lt;/li&gt;
&lt;li&gt;Robust to n/2-1 simultaneous failures as part of protocol&lt;/li&gt;
&lt;li&gt;Less sensitive to tail latency&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;2.5.1 Further reading&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Primary-backup and 2PC&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://scholar.google.com/scholar?q=Replication+techniques+for+availability&quot;&gt;Replication techniques for availability&lt;/a&gt; - Robbert van Renesse &amp;amp; Rachid Guerraoui, 2010&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx&quot;&gt;Concurrency Control and Recovery in Database Systems&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;strong&gt;Paxos&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://research.microsoft.com/users/lamport/pubs/lamport-paxos.pdf&quot;&gt;The Part-Time Parliament&lt;/a&gt; – Leslie Lamport&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://research.microsoft.com/users/lamport/pubs/paxos-simple.pdf&quot;&gt;Paxos Made Simple&lt;/a&gt; – Leslie Lamport, 2001&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://research.google.com/archive/paxos_made_live.html&quot;&gt;Paxos Made Live - An Engineering Perspective&lt;/a&gt; – Chandra et al&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://scholar.google.com/scholar?q=Paxos+Made+Practical&quot;&gt;Paxos Made Practical&lt;/a&gt; – Mazieres, 2007&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://groups.csail.mit.edu/tds/paxos.html&quot;&gt;Revisiting the Paxos Algorithm&lt;/a&gt; – Lynch et al&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://research.microsoft.com/lampson/58-Consensus/Acrobat.pdf&quot;&gt;How to build a highly available system with consensus&lt;/a&gt; – Butler Lampson&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://research.microsoft.com/en-us/um/people/lamport/pubs/reconfiguration-tutorial.pdf&quot;&gt;Reconfiguring a State Machine - Lamport et al – changing&lt;/a&gt; cluster membership&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.20.4762&quot;&gt;Implementing Fault-Tolerant Services Using the State Machine Approach: a Tutorial&lt;/a&gt; – Fred Schneider&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;strong&gt;Raft and ZAB&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf&quot;&gt;In Search of an Understandable Consensus Algorithm&lt;/a&gt;, Diego Ongaro, John Ousterhout, 2013&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.youtube.com/watch?v=YbZ3zDzDnrw&quot;&gt;Raft Lecture&lt;/a&gt; – User Study&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://research.yahoo.com/pub/3274&quot;&gt;A simple totally ordered broadcast protocol&lt;/a&gt;- Junqueira, Reed&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://research.yahoo.com/pub/3514&quot;&gt;ZooKeeper Atomic Broadcast&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title> 存储系统一致性与可用性（二）</title>
   <link href="http://tomsun.info/2014/08/12/stroage_consistency_avaliable_post2.html"/>
   <updated>2014-08-12T00:00:00+08:00</updated>
   <id>http://tomsun.info/2014/08/12/stroage_consistency_avaliable_post2</id>
   <content type="html">&lt;h1&gt;一个分布式kv引擎中使用的一致性协议&lt;/h1&gt;

&lt;p&gt;发现现在很多开源系统在高可用，性能，可扩展性等很多方面没有一个很完善的方案。希望设计一个 k/v 存储系统，系统支持强一致性，并且在 C，A，P 这三方面都较为均衡，系统具有一定可扩展性。&lt;/p&gt;

&lt;p&gt;以下详细介绍自己实现的一个分布式kv系统中使用的分布式协议方案，系统使用 Primary/backup 与 Qurom 协议相结合的协议方案来实现系统的强一致性及高可用性(Qurom 协议 + leader(Master))。&lt;/p&gt;

&lt;h2&gt;1 分布式协议&lt;/h2&gt;

&lt;h3&gt;1.1 协议正常运行流程&lt;/h3&gt;

&lt;p&gt;以下以多数派为3情况即三个副本情况进行举例说明。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/files/2014/08/master.png&quot; alt=&quot;master&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Leader在接收到对应的写请求W之后，首先对W进行序列化（与Basic Paxos协议相比，Leader这里的行为相当于其proposer和acceptor），然后将W 记录到日志中，并且是同步的方式（与basic paxos协议相比leader这里的行为相当于learner），同时在记录日志的同时，Leader并行得将序列化之后的写操作W 发送到各个Follower。&lt;/p&gt;

&lt;p&gt;Follower在接受到对应的proposal W之后判断当前接收到的W的序列号是否是递增的（即是否比前一请求的序列号大1），如果是，将对应的的W同步到日志中，并且应答Leader。（与Basic Paxos相比，Follower这里的行为相当于是acceptor和learner）。&lt;/p&gt;

&lt;p&gt;Leader在对应的W已经同步到日志中并且至少接收到半数Follower的应答后，就可以将对应的写W放入到memtable，然后应答客户端。&lt;/p&gt;

&lt;p&gt;至此，client可以从Leader上查询到最新提交的值，但是在Follower却不能，因为Follower并不知道有一个多数派已经成功提交了写请求，所以不能将对应的W放入memtable。&lt;/p&gt;

&lt;p&gt;所以Leader会周期性得将对应的已经提交memtable的最大的序列号(commit 点)发送到对应的Follower，Follower在接收到该消息之后，将小于等于该序列号的所有日志放入到memtable中，此时，才可以在Follower上查询到W的最新值。&lt;/p&gt;

&lt;p&gt;points：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在副本数为3的情况下，只要有两个节点存活(即多数派存活)，系统可继续正常运行。&lt;/li&gt;
&lt;li&gt;写请求之间的发送是并行的，没有必要等第一个请求完成之后再发送第二个请求。&lt;/li&gt;
&lt;li&gt;对磁盘的写操作和数据发送过程是并行的。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;可以看到以上协议，在副本书为3情况下，只需要一个RTT(Round-Trip-Time)即可完成一次成功得写入操作&lt;/p&gt;

&lt;p&gt;更多优化：采用group commit 的方式在一个请求中多包装几次写请求。&lt;/p&gt;

&lt;h3&gt;1.2 Leader选举与恢复&lt;/h3&gt;

&lt;p&gt;Leader 选举与恢复成功的前提是：保证之前已经成功提交的数据不能丢失。 （因为这些数据很可能已经成功返回给客户端了，系统不能否认已经成功执行的写操作）。&lt;/p&gt;

&lt;p&gt;在有三个副本的情况下，至少有两个节点将数据同步到日志中才能成功返回客户端， 所以在 Leader 宕机之后， 只要有一个多数派存活 （这里为 2 个节点） ， 那么其中至少一个节点包含了当前所有已经成功提交的数据。所以 Leader 选举算法的基本原理为：在当前有一个多数派 Follower 参与的情况下 （当前情况为 2 个节点） ， 选举 LSN(log sequence number) 最大的节点作为新的 Leader即可。如下图中节点 B 满足条件被选举为新的 Leader。&lt;/p&gt;

&lt;p&gt;Leader 宕机之后重新选举出来的 Leader 必须进行恢复。 如下图所示， 在 B 成为 Leader之后，对于从 CMT (commit点)到 LSN 之间的数据都必须重新提案执行。因为处于 CMT 与 LSN 之间的数据可能已经提交成功返回客户端 (CMT 不是实时持久化)。也可能是还没有提交成功的数据，为了保证可用性， 即已经成功写入的数据在系统恢复之后状态还是成功写入， Leader 必须将CMT 与 LSN 之间的数据进行恢复， 在至少一个 Follower 追上 Leader 的时候， 系统才能重新形成一个能够正常运行的多数派，才能够继续接受客户端的写操作。因为 follower 只能接收序列号连贯的请求， 若 follower 没有追上 Leader， 那么即使 Leader继续接收写请求，也不能得到 follower 的正常应答） 。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/files/2014/08/leaderfailover.png&quot; alt=&quot;leaderfailover&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ps: 可以看到这里 CMT和LSN的格式都是类似1.20这样的格式，这里1为epoch，20为序列号，epoch的作用在follower恢复小结会介绍。&lt;/p&gt;

&lt;h3&gt;1.3 Leader选举与恢复&lt;/h3&gt;

&lt;p&gt;Follower 的恢复与本地恢复方式有所不同，这里的恢复分为本地恢复和远程恢复。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/files/2014/08/followerfailover.png&quot; alt=&quot;followerfailover&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如图2.6所示，checkpoint之前的数据已经全部持久化存入数据库引擎，没有必要再进行恢复，对于checkpoint与CMT之间的数据，都已经成功提交，所以可以从本地日志中直接读取进行恢复，在CMT与LSN之间的数据为尚未确认的数据，需要进行远程恢复；这些数据可能是已经成功提交的数据，也有可能需要被恢复的数据，还有可能需要被抛弃的数据。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;成功提交数据的情况&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/media/files/2014/08/follower1.png&quot; alt=&quot;follower1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如上图所示，在 state0 状态，三个节点正常运行，并且日志都已经记录到 1.30；但是在 state1 状态 Follower B 宕机， 还有一个 Follower 存活， 系统可继续运行， 随后 Leader A 收到之前发送写请求的应答 （LSN 为 1.21 到 1.30 的请求的应答），更新 CMT 为 1.30， 并且将CMT 更新发送到 Follower C； 到了 state2 状态， Follower B 恢复， 此时其从 CMT 到 LSN 的日志记录 （l.20 到 1.30） 需要进行恢复， Follower 向 Leader 发送当前需要确认的日志记录的LSN （即 1.21） ， Leader 收到请求后将 LSN 为 1.21 到 1.30 的日志记录打包发送给 Follower，Follower 收到之后判断当前日志是否已经记录，对于 LSN 为 1.21 到 1.30 的数据，显然Follower 都已经记录，无需重复记录，并且由于这些日志记录的状态为 CMT，所以直接更新 CMT。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;需要抛弃数据的情况&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/media/files/2014/08/follower2.png&quot; alt=&quot;follower2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如上图 所示，在 state1 的时候 A,C 两节点的宕机，系统暂停服务，在 state2 的时候A 节点恢复，系统进行 leader 选举，A 成为 leader 并进行恢复后继续接受接收了一些写请求。在 state 3 的时候 C 节点恢复。但是此时 A,B 两节点的从 1.30 到 1.40 之间的数据与 C节点对应阶段的数据其实是不一致的。 如果再进行一次宕机， 即 A 宕机器之后， 虽然有 B,C两个节点存活，其实这个系统进入了一个不一致的模棱两可的状态（即实际上 B 节点的1.20~1.40 直接的数据是有效的，但是在只有 B，C 两节点情况下）。&lt;/p&gt;

&lt;p&gt;那么如何来解决这个问题呢？ 引入 epoch（选举轮次） 来解决这个问题， 在每次 Leader 宕机恢复之后， 升级 epoch 之后才能够继续对新的写请求进行序列化。这样就能够重用之前使用过的序列号(sequence)，而不造成节点恢复时可能造成的不一致性。&lt;/p&gt;

&lt;p&gt;如下图所示，在 state3 节点 C 进行恢复的时候，对于 1.21 至 1.31 的数据能够正常恢复，但是对于其记录的从 1.32 至 1.40 的数据由于之前的 Leader 宕机并且对序列号为 22 至40 的数据重新进行了序列化，所以其 1.30 到 1.40 的数据作废，必须从 Leader 获取从 2.32到 2.40 的数据进行恢复，然后进入下一状态才能继续正常运行。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/files/2014/08/follower3.png&quot; alt=&quot;follower3&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;1.4 关于多leader无法避免的问题&lt;/h3&gt;

&lt;p&gt;简单的问题在分布式环境中就会变得不那么简单。 Leader 的选举问题有时候无法避免出现两个 leader 的情况。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/files/2014/08/multileader.png&quot; alt=&quot;multileader&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在 state1 的时候，A 出现网络异常，刷新 leader 租赁期超时。这个时候 A 在这个超时事件上结束 leader A，但是这时候由于操作系统进程调度等方面的原因， A 节点还是处于leader 状态，B 与 C 发现在 zookeeper 上的 leader 节点丢失，进行 leader 选举，C 节点成为 leader， 这个时候系统中出现了两个 leader （这种情况就是典型的脑裂情况） 。 这个时候如果继续运行下去，系统会出错。&lt;/p&gt;

&lt;p&gt;那么为了保证一致性。提高系统的可用性，只能退而求其次，在同时存在两 Leader 的情况下，保证只有一个 Leader 能够正常运行。&lt;/p&gt;

&lt;p&gt;本协议是一多数派协议， 有天然的抗脑裂特性。 其实 B 节点只可能与一个对应的 Leader建立相匹配的 follower 关系，也就是说，在同一时刻，A,C 节点只可能有一个节点与节点 B建立可运行的多数派，所以能够保证写的一致性。&lt;/p&gt;

&lt;p&gt;对于读，显然也不能通过仅仅读取 leader 来保证数据强一致。 强一致性读实现方式：&lt;/p&gt;

&lt;p&gt;leader 读数据 + zookeeper 确认 leader：但是这种方式在系统分布式处理的时候会使得系统的可扩展性瓶颈在 zookeeper。 （ps： 协议借助 zookeeper 尽心 leader选举，zookeeper 上有最新的 leader 的信息） leader 读数据 + follower 读状态方式实现强一致性读： 即从 leader 读最新的数据，然后随便读一个 follower， 查看 leader 与 follower 的选举轮次号 epoch 是否一致。一致，则返回数据，不一致则要主动去 zookeeper 上查找当前 leader 节点信息，从当前新的 leader 上读取最新数据。&lt;/p&gt;

&lt;h3&gt;1.5 节点状态变迁&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/media/files/2014/08/statechange.png&quot; alt=&quot;statechange&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title> 存储系统一致性与可用性</title>
   <link href="http://tomsun.info/2014/07/26/stroage_consistency_avaliable_post1.html"/>
   <updated>2014-07-26T00:00:00+08:00</updated>
   <id>http://tomsun.info/2014/07/26/stroage_consistency_avaliable_post1</id>
   <content type="html">&lt;h3&gt;1. 关于可用性&lt;/h3&gt;

&lt;p&gt;提供系统可用性的关键是在相关组件失效情况下，系统能多快恢复并继续正确得提供服务。&lt;/p&gt;

&lt;h4&gt;1.1 如何恢复服务&lt;/h4&gt;

&lt;p&gt;How to recover a single node from power failure。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;wait for reboot
Data is durable, but service is unavailable temporarily。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use multiple nodes to provide service
Another node takes over to provide service, How to make sure nodes respond in the same way?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;对于无状态的服务器而言，没有关系。对于有状态的服务器而言,如何去保证 take over的服务器提供正确的服务?&lt;/p&gt;

&lt;h3&gt;1.2 两种基本方法&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;多副本&lt;/li&gt;
&lt;li&gt;纠删码
纠删码基本原理是把一份数据分割成多份，计算出若干冗余块，比如 切割成26份数据再使用纠删算法计算4份冗余块，可以使得任意丢4块数据还能恢复数据进行服务，核心还是通过数据冗余来实现一定的高可用。以下内容只针对简单多副本展开讨论，本文不对纠删码进行详述。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;对于简单多副本要保证多个副本的状态是一致的，才能保证另一个副本对应服务器能提供正确的服务。如何实现多副本之间的一致性?以下讨论相关理论及实现.&lt;/p&gt;

&lt;h2&gt;2. 理论&lt;/h2&gt;

&lt;p&gt;实现多副本一致的一个基本理论为 replicated stat machine，即副本状态机：副本所处的初始状态相同；每个副本执行的操作顺序相同，并且每一个操作都相同； 那么最终所有副本的状态是一致。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/files/2014/07/OPaOPb.png&quot; alt=&quot;OpaaOPb&quot; /&gt;&lt;/p&gt;

&lt;p&gt;那如何保证在多个 client 并发操作下的保证操作的顺序性？以下分析 primary-backup协议，Qurom，paoxs 协议等协议&lt;/p&gt;

&lt;h3&gt;2.1 Master Slave&lt;/h3&gt;

&lt;p&gt;Master / Slave 相对来讲是较简单和自然而然的方式，Master 决定操作的顺序，Slave 节点执行序列化之后的操作。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/files/2014/07/Primary.png&quot; alt=&quot;Primary.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Master Slave 协议较为简单， 但是其可用性不是很高， 在其中某一个节点宕机的情况下，系统无法继续运行。&lt;/p&gt;

&lt;p&gt;如下图所示，在 state0 状态 Master 与 Slave 的 LSN(log sequence number)为 10，处于一致的状态，在 state1 时 Slave 发生宕机，Master 继续接受写请求，到 state2 时 Master 也生宕机，在 state3 时 Slave 恢复，但是 Master 与 Slave 的状态不一致，所以 Slave 即不能提供读也不能提供写， 此时即发生阻塞。 若想继续提供服务， 必须进行人工干预。 Master/slaver
协议是一种阻塞式的协议。 （数据库中的 “两阶段提交协议” 就是一种典型的阻塞式的协议）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/files/2014/07/3.png&quot; alt=&quot;3.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;2.2  Quroum/(NRW)&lt;/h3&gt;

&lt;p&gt;Quorum 机制是一种简单有效的副本管理机制。NWR 协议是其中的典型，NWR 为Amazon 公司设计的分布式 kv 系统 dynamo 中使用的分布式副本协议。&lt;/p&gt;

&lt;p&gt;其中 N 为副本的数目， W 是写成功需要写的份数， R 为读成功需要读的份数， 保证 R+W&gt;N，即能读到正确的数据。&lt;/p&gt;

&lt;p&gt;假设 N=3，W=1，R=3，即如果副本数目为 3，写 1 份即算成功，那么至少读 3 份才能读到写入的数据。 并且为了防止写丢失， 还必须用 “last write wins” 的方式解决写冲突问题。&lt;/p&gt;

&lt;p&gt;NRW 可能出现数据丢失情况&lt;/p&gt;

&lt;p&gt;如下图 所示， 例如用户先调用 OP1 写数据 obj1 的 A 副本成功返回， 然后用户调用 OP2写数据 obj1 的副本 C 成功后返回，此时使用”last write wins”的进行合并使得最终副本 A 和C 上的数据完全一致。但是如果 A 与 B 上的时钟不一致，比如 A 的时钟比 C 快，那么在数据整合的时候会出现先写的数据副本 A 上的操作覆盖后写的数据 C， 导致后写的 OP2 丢失，这显然不是用户想看到的信息。所以系统必须排除这种状况才能保证数据的最终一致性， 否则就是弱一致性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/files/2014/07/qurom.png&quot; alt=&quot;qurom.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;时钟同步：在分布式系统中本身就是很难事件的东西。&lt;/p&gt;

&lt;p&gt;结论:NRW 其实也只是看上去很美的东西。但是 dynamo 为了使得系统能够有更好的可扩展性及可用性， 放宽对一致性的要求， 不支持强一致性， 而支持最终一致性甚至是弱一致性。 （弱一致性估计是很难使用户接受）&lt;/p&gt;

&lt;h3&gt;2.3 paxos协议&lt;/h3&gt;

&lt;p&gt;paxos 协议中每个节点都是对等的，每个节点都可以接收客户端的写请求，并尝试完成客户端的写请求，其核心是基于一个多数派的抢占机制式协议。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/files/2014/07/basicPaxos.png&quot; alt=&quot;basicPaxos.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;2.4 关于CAP&lt;/h3&gt;

&lt;p&gt;各种分布式协议所面临的问题就是分布式文件系统的三大难题:副本一致性，系统可用性，分区容忍性（网络异常的容忍能力）。&lt;/p&gt;

&lt;p&gt;CAP理论明确提出了不要妄图设计一种对 CAP 三大属性都完全拥有的系统，因为这种系统在理论上就已经被证明不存在。设计系统的时候需要在 C、A、P 这三方面有所折中。&lt;/p&gt;

&lt;p&gt;Primary/backup：MySQL：具有完全的 C，很糟糕的 A，很糟糕的 P， （任何一个节点宕机都会导致服务中断）
Quroum 协议：Dynamo/cassandra，有一定的 C，有较好的 A，也有较好的 P，是一种较为平衡的分布式协议。
Paxos 协议：Spanner , Chubbuy, Zookeeper, megastore 具有完全的 C，较好的 A， 较好的 P。
发现很多开源系统在高可用、性能、可扩展性等很多方面没有一个完善的方案。希望设计一个 k/v 存储系统，系统支持强一致性，并且在 C，A，P 这三方面都较为均衡，系统具有一定可扩展性，详见下文。&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Nginx And Go Http 并发性能</title>
   <link href="http://tomsun.info/2014/01/20/golang-vs-nginx-test.html"/>
   <updated>2014-01-20T00:00:00+08:00</updated>
   <id>http://tomsun.info/2014/01/20/golang-vs-nginx-test</id>
   <content type="html">&lt;h2&gt;1 测试硬件&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;Intel(R) Xeon(R) CPU           X3440  @ 2.53GHz
cpu cache size  : 8192 KB
DRAM：8G
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;2 测试软件&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;2.6.32-5-amd64 #1 SMP
nginx：ngx_openresty-1.4.3.4 
go :go version go1.3.3 linux/amd64
ab：ApacheBench, Version 2.3 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3 测试配置&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3&gt;3.1  一些内核配置&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;/proc/sys/fs/file-max                    3145728
/proc/sys/fs/nr_open                     1048576
/proc/sys/net/core/netdev_max_backlog    1000
/proc/sys/net/core/rmem_max              131071
/proc/sys/net/core/wmem_max              131071
/proc/sys/net/core/somaxconn             128
/proc/sys/net/ipv4/ip_forward            0
/proc/sys/net/ipv4/ip_local_port_range   8192   65535
/proc/sys/net/ipv4/tcp_fin_timeout       60
/proc/sys/net/ipv4/tcp_keepalive_time    7200
/proc/sys/net/ipv4/tcp_max_syn_backlog   2048
/proc/sys/net/ipv4/tcp_max_tw_buckets    1048576
/proc/sys/net/ipv4/tcp_no_metrics_save   0
/proc/sys/net/ipv4/tcp_syn_retries       5
/proc/sys/net/ipv4/tcp_synack_retries    5
/proc/sys/net/ipv4/tcp_tw_recycle        0
/proc/sys/net/ipv4/tcp_tw_reuse          0
/proc/sys/vm/min_free_kbytes             11489
/proc/sys/vm/overcommit_memory           0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;3.2 Nginx&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;worker_processes  8;
events {
    worker_connections  2046;
    use epoll;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    location / {
            root   html;
            index  index.html index.htm;
    }
    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
            root   html;
    }
    location /ab-test {
            proxy_http_version 1.1;
            proxy_set_header Connection &quot;&quot;; 
            content_by_lua &#39;ngx.print(&quot;aaa---here omit other char a, total 512--- aaaaaa&quot;)&#39;;    
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;3.3 golang 测试代码&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/work-jlsun/golang/blob/develop/go512server.go&quot;&gt;go512server.go&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;4 测试方法&lt;/h2&gt;

&lt;h3&gt;4.1 测试工具及命令&lt;/h3&gt;

&lt;p&gt;使用 ab测试不同并发场景下nginx和golang http 服务的性能，测试数据大小512Byte。&lt;/p&gt;

&lt;p&gt;测试命令示例：ab -n 1000000 -c 5000  -k  &quot;http://127.0.0.1:8081/512b&quot;&lt;/p&gt;

&lt;p&gt;(ps: 所有测试结果，都是3次之后取平均值)&lt;/p&gt;

&lt;h3&gt;4.2  测试结果&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;短连接场景&lt;/li&gt;
&lt;/ul&gt;


&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;并发请求量 &lt;/th&gt;
&lt;th&gt; 100 &lt;/th&gt;
&lt;th&gt; 200 &lt;/th&gt;
&lt;th&gt; 500 &lt;/th&gt;
&lt;th&gt; 1000 &lt;/th&gt;
&lt;th&gt; 2000 &lt;/th&gt;
&lt;th&gt; 5000&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;nginx(tps) &lt;/td&gt;
&lt;td&gt; 12741.62   &lt;/td&gt;
&lt;td&gt; 12598.08     &lt;/td&gt;
&lt;td&gt;  11917.15   &lt;/td&gt;
&lt;td&gt;  12016.63 &lt;/td&gt;
&lt;td&gt; 11640.36   &lt;/td&gt;
&lt;td&gt;   6047.29&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;go（tps）  &lt;/td&gt;
&lt;td&gt;  11310.32   &lt;/td&gt;
&lt;td&gt; 11208.87     &lt;/td&gt;
&lt;td&gt; 10731.40    &lt;/td&gt;
&lt;td&gt;  10757.3  &lt;/td&gt;
&lt;td&gt; 10750.26   &lt;/td&gt;
&lt;td&gt;     10869.80&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;ps： 端连接情况下 并发5000 情况下， nginx情况不知道是为什么（nginx进程cpu利用看起来不是很均衡）&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;长连接场景&lt;/li&gt;
&lt;/ul&gt;


&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;并发请求量 &lt;/th&gt;
&lt;th&gt; 100 &lt;/th&gt;
&lt;th&gt; 200 &lt;/th&gt;
&lt;th&gt; 500 &lt;/th&gt;
&lt;th&gt; 1000 &lt;/th&gt;
&lt;th&gt; 2000 &lt;/th&gt;
&lt;th&gt; 5000&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;nginx（tps）    &lt;/td&gt;
&lt;td&gt;     61249.81   &lt;/td&gt;
&lt;td&gt;     60672.71  &lt;/td&gt;
&lt;td&gt;   59548.39  &lt;/td&gt;
&lt;td&gt;   55287.55    &lt;/td&gt;
&lt;td&gt;           58375.65    &lt;/td&gt;
&lt;td&gt;     60662.44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;go（tps）    &lt;/td&gt;
&lt;td&gt;    55257.64       &lt;/td&gt;
&lt;td&gt;     53288.23 &lt;/td&gt;
&lt;td&gt;   49006.64  &lt;/td&gt;
&lt;td&gt;  46362.55  &lt;/td&gt;
&lt;td&gt;             48042.18      &lt;/td&gt;
&lt;td&gt;    47855.02&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;ul&gt;
&lt;li&gt;golang + nginx （golang as proxy）&lt;/li&gt;
&lt;/ul&gt;


&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;并发请求量 &lt;/th&gt;
&lt;th&gt; 100 &lt;/th&gt;
&lt;th&gt; 200 &lt;/th&gt;
&lt;th&gt; 500 &lt;/th&gt;
&lt;th&gt; 1000 &lt;/th&gt;
&lt;th&gt; 2000 &lt;/th&gt;
&lt;th&gt; 5000&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;go（tps）  &lt;/td&gt;
&lt;td&gt;   31535.37   &lt;/td&gt;
&lt;td&gt; 29081.96  &lt;/td&gt;
&lt;td&gt;  30250.24  &lt;/td&gt;
&lt;td&gt;   28921.48  &lt;/td&gt;
&lt;td&gt;  26631.12  &lt;/td&gt;
&lt;td&gt;    25333.64&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;1: &lt;a href=&quot;https://github.com/work-jlsun/golang/blob/develop/go512server.go&quot;&gt;golang  proxy 代码&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2: golang作为proxy的时候性能基本为非proxy的一半左右，这个是可以理解的，一个请求的响应时间就是nginx + go两层的响应时间。&lt;/p&gt;

&lt;p&gt;3: 使用golang自带的httpclient连接后端的nginx&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nginx + golang （nginx as proxy）&lt;/li&gt;
&lt;/ul&gt;


&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;并发请求量 &lt;/th&gt;
&lt;th&gt; 100 &lt;/th&gt;
&lt;th&gt; 200 &lt;/th&gt;
&lt;th&gt; 500 &lt;/th&gt;
&lt;th&gt; 1000 &lt;/th&gt;
&lt;th&gt; 2000 &lt;/th&gt;
&lt;th&gt; 5000&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;TPS &lt;/td&gt;
&lt;td&gt; 43336.19    &lt;/td&gt;
&lt;td&gt; 41722.05   &lt;/td&gt;
&lt;td&gt; 37984.94  &lt;/td&gt;
&lt;td&gt;  34033.42  &lt;/td&gt;
&lt;td&gt;  29489.74  &lt;/td&gt;
&lt;td&gt;         25693.03&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;ul&gt;
&lt;li&gt;golang proxy简单稳定性测试&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;5000 并发测试 30 分钟，tps 达到24751，基本没有因为随着时间的增长而对性能造成很大的影响，资源使用也比较稳定

1000并发测试 1小时 ， tsp达到 27651.10，基本没有因为随着时间的增长而对性能造成很大的影响，资源使用比较稳定
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;5 测试结论&lt;/h2&gt;

&lt;p&gt;基于以上4中场景，上百组测试下，得出一下简单结论&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;golang表现还是较为出色，相比于标杆Nginx性能差20%左右&lt;/li&gt;
&lt;li&gt;golang在高并发压力测试下稳定性还是不错，可以接受的&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;当前我们基于简单测试环境下的测试验证golang，现实环境远远比测试环境复杂，后续我们会在NosMedia 开发测试上线过程中不断总结经验。&lt;/p&gt;

&lt;h2&gt;6 参考资料&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://gist.github.com/hgfischer/7965620&quot;&gt;golang test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.lifeibo.com/blog/2013/01/28/ngx-lua-and-go.html&quot;&gt;nginx-lua vs golang&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;7 坑&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;goang http 长连接问题&lt;/li&gt;
&lt;li&gt;Connection reset  by peer (104)&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 
</feed>
