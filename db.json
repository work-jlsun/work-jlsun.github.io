{"meta":{"version":1,"warehouse":"1.0.3"},"models":{"Asset":[{"_id":"themes/maupassant/source/js/totop.js","path":"js/totop.js","modified":0},{"_id":"themes/maupassant/source/js/jquery.min.js","path":"js/jquery.min.js","modified":0},{"_id":"themes/maupassant/source/js/fancybox.pack.js","path":"js/fancybox.pack.js","modified":0},{"_id":"themes/maupassant/source/fonts/icomoon.woff","path":"fonts/icomoon.woff","modified":0},{"_id":"themes/maupassant/source/fonts/icomoon.ttf","path":"fonts/icomoon.ttf","modified":0},{"_id":"themes/maupassant/source/fonts/icomoon.svg","path":"fonts/icomoon.svg","modified":0},{"_id":"themes/maupassant/source/fonts/icomoon.eot","path":"fonts/icomoon.eot","modified":0},{"_id":"themes/maupassant/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0},{"_id":"themes/maupassant/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0},{"_id":"themes/maupassant/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0},{"_id":"themes/maupassant/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0},{"_id":"themes/maupassant/source/css/style.scss","path":"css/style.scss","modified":0},{"_id":"themes/maupassant/source/css/pure-min.css","path":"css/pure-min.css","modified":0},{"_id":"themes/maupassant/source/css/normalize.css","path":"css/normalize.css","modified":0},{"_id":"themes/maupassant/source/css/jquery.fancybox.css","path":"css/jquery.fancybox.css","modified":0},{"_id":"themes/maupassant/source/css/grids-responsive-min.css","path":"css/grids-responsive-min.css","modified":0},{"_id":"source/media/fonts/telex-regular-webfont.woff","path":"media/fonts/telex-regular-webfont.woff","modified":0},{"_id":"source/media/fonts/telex-regular-webfont.ttf","path":"media/fonts/telex-regular-webfont.ttf","modified":0},{"_id":"source/media/fonts/telex-regular-webfont.svg","path":"media/fonts/telex-regular-webfont.svg","modified":0},{"_id":"source/media/fonts/telex-regular-webfont.eot","path":"media/fonts/telex-regular-webfont.eot","modified":0},{"_id":"source/media/files/2015/10/9.png","path":"media/files/2015/10/9.png","modified":0},{"_id":"source/media/files/2015/10/8.png","path":"media/files/2015/10/8.png","modified":0},{"_id":"source/media/files/2015/10/7.png","path":"media/files/2015/10/7.png","modified":0},{"_id":"source/media/files/2015/10/6.png","path":"media/files/2015/10/6.png","modified":0},{"_id":"source/media/files/2015/10/5.png","path":"media/files/2015/10/5.png","modified":0},{"_id":"source/media/files/2015/10/4.png","path":"media/files/2015/10/4.png","modified":0},{"_id":"source/media/files/2015/10/3.png","path":"media/files/2015/10/3.png","modified":0},{"_id":"source/media/files/2015/10/2.png","path":"media/files/2015/10/2.png","modified":0},{"_id":"source/media/files/2015/10/13.png","path":"media/files/2015/10/13.png","modified":0},{"_id":"source/media/files/2015/10/12.png","path":"media/files/2015/10/12.png","modified":0},{"_id":"source/media/files/2015/10/11.png","path":"media/files/2015/10/11.png","modified":0},{"_id":"source/media/files/2015/10/10.png","path":"media/files/2015/10/10.png","modified":0},{"_id":"source/media/files/2015/10/1.png","path":"media/files/2015/10/1.png","modified":0},{"_id":"source/media/files/2015/09/storagearch.jpg","path":"media/files/2015/09/storagearch.jpg","modified":0},{"_id":"source/media/files/2015/09/facebookcache.jpg","path":"media/files/2015/09/facebookcache.jpg","modified":0},{"_id":"source/media/files/2015/06/treelevel.jpg","path":"media/files/2015/06/treelevel.jpg","modified":0},{"_id":"source/media/files/2015/06/sweep.jpg","path":"media/files/2015/06/sweep.jpg","modified":0},{"_id":"source/media/files/2015/06/span.jpg","path":"media/files/2015/06/span.jpg","modified":0},{"_id":"source/media/files/2015/06/smallandbig.jpg","path":"media/files/2015/06/smallandbig.jpg","modified":0},{"_id":"source/media/files/2015/06/sizeclass.jpg","path":"media/files/2015/06/sizeclass.jpg","modified":0},{"_id":"source/media/files/2015/06/malloc.jpg","path":"media/files/2015/06/malloc.jpg","modified":0},{"_id":"source/media/files/2015/06/init.jpg","path":"media/files/2015/06/init.jpg","modified":0},{"_id":"source/media/files/2014/12/18/yamdroktso@2x.jpg","path":"media/files/2014/12/18/yamdroktso@2x.jpg","modified":0},{"_id":"source/media/files/2014/12/18/yamdroktso.jpg","path":"media/files/2014/12/18/yamdroktso.jpg","modified":0},{"_id":"source/media/files/2014/09/system-of-3-300x138.png","path":"media/files/2014/09/system-of-3-300x138.png","modified":0},{"_id":"source/media/files/2014/09/system-of-2-300x87.png","path":"media/files/2014/09/system-of-2-300x87.png","modified":0},{"_id":"source/media/files/2014/09/replication-sync-300x274.png","path":"media/files/2014/09/replication-sync-300x274.png","modified":0},{"_id":"source/media/files/2014/09/replication-both-300x206.png","path":"media/files/2014/09/replication-both-300x206.png","modified":0},{"_id":"source/media/files/2014/09/replication-async-300x263.png","path":"media/files/2014/09/replication-async-300x263.png","modified":0},{"_id":"source/media/files/2014/09/google-transact09.png","path":"media/files/2014/09/google-transact09.png","modified":0},{"_id":"source/media/files/2014/09/epoch.png","path":"media/files/2014/09/epoch.png","modified":0},{"_id":"source/media/files/2014/09/24/syscall.jpg","path":"media/files/2014/09/24/syscall.jpg","modified":0},{"_id":"source/media/files/2014/09/24/steal.jpg","path":"media/files/2014/09/24/steal.jpg","modified":0},{"_id":"source/media/files/2014/09/24/our-cast.jpg","path":"media/files/2014/09/24/our-cast.jpg","modified":0},{"_id":"source/media/files/2014/09/24/in-motion.jpg","path":"media/files/2014/09/24/in-motion.jpg","modified":0},{"_id":"source/media/files/2014/09/24/goroutinestate.jpg","path":"media/files/2014/09/24/goroutinestate.jpg","modified":0},{"_id":"source/media/files/2014/09/05/bridge-to-wonderland.jpg","path":"media/files/2014/09/05/bridge-to-wonderland.jpg","modified":0},{"_id":"source/media/files/2014/08/statechange.png","path":"media/files/2014/08/statechange.png","modified":0},{"_id":"source/media/files/2014/08/multileader.png","path":"media/files/2014/08/multileader.png","modified":0},{"_id":"source/media/files/2014/08/master.png","path":"media/files/2014/08/master.png","modified":0},{"_id":"source/media/files/2014/08/leaderfailover.png","path":"media/files/2014/08/leaderfailover.png","modified":0},{"_id":"source/media/files/2014/08/followerfailover.png","path":"media/files/2014/08/followerfailover.png","modified":0},{"_id":"source/media/files/2014/08/follower3.png","path":"media/files/2014/08/follower3.png","modified":0},{"_id":"source/media/files/2014/08/follower2.png","path":"media/files/2014/08/follower2.png","modified":0},{"_id":"source/media/files/2014/08/follower1.png","path":"media/files/2014/08/follower1.png","modified":0},{"_id":"source/media/files/2014/07/qurom.png","path":"media/files/2014/07/qurom.png","modified":0},{"_id":"source/media/files/2014/07/basicPaxos.png","path":"media/files/2014/07/basicPaxos.png","modified":0},{"_id":"source/media/files/2014/07/Primary.png","path":"media/files/2014/07/Primary.png","modified":0},{"_id":"source/media/files/2014/07/OPaOPb.png","path":"media/files/2014/07/OPaOPb.png","modified":0},{"_id":"source/media/files/2014/07/3.png","path":"media/files/2014/07/3.png","modified":0},{"_id":"source/media/files/2014/07/24/ladder.jpg","path":"media/files/2014/07/24/ladder.jpg","modified":0},{"_id":"source/media/files/2014/07/02/ibanez.jpg","path":"media/files/2014/07/02/ibanez.jpg","modified":0},{"_id":"source/media/files/2014/06/12/cherryblossom.jpg","path":"media/files/2014/06/12/cherryblossom.jpg","modified":0},{"_id":"source/media/files/2014/04/07/white-mountains.jpg","path":"media/files/2014/04/07/white-mountains.jpg","modified":0},{"_id":"source/media/files/2014/03/29/alone.jpg","path":"media/files/2014/03/29/alone.jpg","modified":0},{"_id":"source/media/files/2014/03/17/lost.jpg","path":"media/files/2014/03/17/lost.jpg","modified":0},{"_id":"source/media/files/2014/02/28/yamadera.jpg","path":"media/files/2014/02/28/yamadera.jpg","modified":0},{"_id":"source/media/files/2014/02/20/arashiyama.jpg","path":"media/files/2014/02/20/arashiyama.jpg","modified":0},{"_id":"source/media/files/2014/01/29/HiroshimaPeaceMemorial.jpg","path":"media/files/2014/01/29/HiroshimaPeaceMemorial.jpg","modified":0},{"_id":"source/media/files/2014/01/23/untitled.jpg","path":"media/files/2014/01/23/untitled.jpg","modified":0},{"_id":"source/media/files/2013/12/19/cat.jpg","path":"media/files/2013/12/19/cat.jpg","modified":0},{"_id":"source/media/files/2013/11/05/love-is-being-stupid-together.jpg","path":"media/files/2013/11/05/love-is-being-stupid-together.jpg","modified":0},{"_id":"source/media/files/2013/08/12/kaminarimon.jpg","path":"media/files/2013/08/12/kaminarimon.jpg","modified":0},{"_id":"source/media/files/2013/05/28/sunset.jpg","path":"media/files/2013/05/28/sunset.jpg","modified":0},{"_id":"source/media/files/2013/04/02/sakura.jpg","path":"media/files/2013/04/02/sakura.jpg","modified":0},{"_id":"source/media/files/2013/02/17/pebble.jpg","path":"media/files/2013/02/17/pebble.jpg","modified":0},{"_id":"source/media/files/2012/12/04/girl-in-the-sun.jpg","path":"media/files/2012/12/04/girl-in-the-sun.jpg","modified":0},{"_id":"source/media/files/2012/09/29/stay-still.jpg","path":"media/files/2012/09/29/stay-still.jpg","modified":0},{"_id":"source/media/files/2012/09/10/skytree.jpg","path":"media/files/2012/09/10/skytree.jpg","modified":0},{"_id":"source/media/files/2012/08/07/waterfall-in-house.jpg","path":"media/files/2012/08/07/waterfall-in-house.jpg","modified":0},{"_id":"source/media/files/2012/08/07/lotus.jpg","path":"media/files/2012/08/07/lotus.jpg","modified":0},{"_id":"source/media/files/2012/08/07/farmers.jpg","path":"media/files/2012/08/07/farmers.jpg","modified":0},{"_id":"source/media/files/2012/06/08/rose-6.jpg","path":"media/files/2012/06/08/rose-6.jpg","modified":0},{"_id":"source/media/files/2012/06/08/rose-5.jpg","path":"media/files/2012/06/08/rose-5.jpg","modified":0},{"_id":"source/media/files/2012/06/08/rose-4.jpg","path":"media/files/2012/06/08/rose-4.jpg","modified":0},{"_id":"source/media/files/2012/06/08/rose-3.jpg","path":"media/files/2012/06/08/rose-3.jpg","modified":0},{"_id":"source/media/files/2012/06/08/rose-2.jpg","path":"media/files/2012/06/08/rose-2.jpg","modified":0},{"_id":"source/media/files/2012/06/08/rose-1.jpg","path":"media/files/2012/06/08/rose-1.jpg","modified":0},{"_id":"source/media/files/2012/05/31/river.jpg","path":"media/files/2012/05/31/river.jpg","modified":0},{"_id":"source/media/files/2012/05/16/sakura.jpg","path":"media/files/2012/05/16/sakura.jpg","modified":0},{"_id":"source/media/files/2012/04/16/sakura_02.jpg","path":"media/files/2012/04/16/sakura_02.jpg","modified":0},{"_id":"source/media/files/2012/04/16/sakura_01.jpg","path":"media/files/2012/04/16/sakura_01.jpg","modified":0},{"_id":"source/media/files/2012/04/16/leaf.jpg","path":"media/files/2012/04/16/leaf.jpg","modified":0},{"_id":"source/media/files/2012/01/24/shot-1.jpg","path":"media/files/2012/01/24/shot-1.jpg","modified":0},{"_id":"source/media/files/2011/12/24/SDIM0316.jpg","path":"media/files/2011/12/24/SDIM0316.jpg","modified":0},{"_id":"source/media/files/2011/12/24/SDIM0289.jpg","path":"media/files/2011/12/24/SDIM0289.jpg","modified":0},{"_id":"source/media/files/2011/12/24/SDIM0260.jpg","path":"media/files/2011/12/24/SDIM0260.jpg","modified":0},{"_id":"source/media/files/2011/12/24/SDIM0259.jpg","path":"media/files/2011/12/24/SDIM0259.jpg","modified":0},{"_id":"source/media/files/2011/12/24/SDIM0254.jpg","path":"media/files/2011/12/24/SDIM0254.jpg","modified":0},{"_id":"source/media/files/2011/12/24/SDIM0160.jpg","path":"media/files/2011/12/24/SDIM0160.jpg","modified":0},{"_id":"source/media/files/2011/12/24/SDIM0099.jpg","path":"media/files/2011/12/24/SDIM0099.jpg","modified":0},{"_id":"source/media/files/2011/11/29/SDIM0087.jpg","path":"media/files/2011/11/29/SDIM0087.jpg","modified":0},{"_id":"source/media/files/2011/11/29/SDIM0061.jpg","path":"media/files/2011/11/29/SDIM0061.jpg","modified":0},{"_id":"source/media/files/2011/11/29/SDIM0055.jpg","path":"media/files/2011/11/29/SDIM0055.jpg","modified":0},{"_id":"source/media/files/2011/11/29/SDIM0029.jpg","path":"media/files/2011/11/29/SDIM0029.jpg","modified":0},{"_id":"source/media/files/2011/11/22/with-description.png","path":"media/files/2011/11/22/with-description.png","modified":0},{"_id":"source/media/files/2011/11/22/with-analytics.png","path":"media/files/2011/11/22/with-analytics.png","modified":0},{"_id":"source/media/files/2011/11/22/bug.png","path":"media/files/2011/11/22/bug.png","modified":0},{"_id":"source/media/files/2011/11/22/UIImageView-and-userInteractionEnabled.dot","path":"media/files/2011/11/22/UIImageView-and-userInteractionEnabled.dot","modified":0},{"_id":"source/media/files/2011/10/06/RIPSteveJobs.png","path":"media/files/2011/10/06/RIPSteveJobs.png","modified":0},{"_id":"source/media/files/2011/03/28/compass.jpg","path":"media/files/2011/03/28/compass.jpg","modified":0},{"_id":"source/media/files/2011/03/19/skewer.jpg","path":"media/files/2011/03/19/skewer.jpg","modified":0},{"_id":"source/media/files/2011/03/19/skewer-icecream.jpg","path":"media/files/2011/03/19/skewer-icecream.jpg","modified":0},{"_id":"source/media/files/2011/03/19/nanba.jpg","path":"media/files/2011/03/19/nanba.jpg","modified":0},{"_id":"source/media/files/2011/03/19/lucky-god.jpg","path":"media/files/2011/03/19/lucky-god.jpg","modified":0},{"_id":"source/media/files/2011/03/07/kindle.jpg","path":"media/files/2011/03/07/kindle.jpg","modified":0},{"_id":"source/media/files/2011/02/24/wp2txt.xsl","path":"media/files/2011/02/24/wp2txt.xsl","modified":0},{"_id":"source/media/files/2010/10/29/rain.jpg","path":"media/files/2010/10/29/rain.jpg","modified":0},{"_id":"source/media/files/2010/10/19/track.jpg","path":"media/files/2010/10/19/track.jpg","modified":0},{"_id":"source/media/files/2010/08/17/sunrise.jpg","path":"media/files/2010/08/17/sunrise.jpg","modified":0},{"_id":"source/media/files/2010/06/30/box.jpg","path":"media/files/2010/06/30/box.jpg","modified":0},{"_id":"source/media/files/2010/01/14/disney-sea-15.jpg","path":"media/files/2010/01/14/disney-sea-15.jpg","modified":0},{"_id":"source/media/files/2010/01/14/disney-sea-14.jpg","path":"media/files/2010/01/14/disney-sea-14.jpg","modified":0},{"_id":"source/media/files/2010/01/14/disney-sea-13.jpg","path":"media/files/2010/01/14/disney-sea-13.jpg","modified":0},{"_id":"source/media/files/2010/01/14/disney-sea-12.jpg","path":"media/files/2010/01/14/disney-sea-12.jpg","modified":0},{"_id":"source/media/files/2010/01/14/disney-sea-11.jpg","path":"media/files/2010/01/14/disney-sea-11.jpg","modified":0},{"_id":"source/media/files/2010/01/14/disney-sea-10.jpg","path":"media/files/2010/01/14/disney-sea-10.jpg","modified":0},{"_id":"source/media/files/2010/01/14/disney-sea-09.jpg","path":"media/files/2010/01/14/disney-sea-09.jpg","modified":0},{"_id":"source/media/files/2010/01/14/disney-sea-08.jpg","path":"media/files/2010/01/14/disney-sea-08.jpg","modified":0},{"_id":"source/media/files/2010/01/14/disney-sea-07.jpg","path":"media/files/2010/01/14/disney-sea-07.jpg","modified":0},{"_id":"source/media/files/2010/01/14/disney-sea-06.jpg","path":"media/files/2010/01/14/disney-sea-06.jpg","modified":0},{"_id":"source/media/files/2010/01/14/disney-sea-05.jpg","path":"media/files/2010/01/14/disney-sea-05.jpg","modified":0},{"_id":"source/media/files/2010/01/14/disney-sea-04.jpg","path":"media/files/2010/01/14/disney-sea-04.jpg","modified":0},{"_id":"source/media/files/2010/01/14/disney-sea-03.jpg","path":"media/files/2010/01/14/disney-sea-03.jpg","modified":0},{"_id":"source/media/files/2010/01/14/disney-sea-02.jpg","path":"media/files/2010/01/14/disney-sea-02.jpg","modified":0},{"_id":"source/media/files/2010/01/14/disney-sea-01.jpg","path":"media/files/2010/01/14/disney-sea-01.jpg","modified":0},{"_id":"source/media/files/2009/10/21/washer.jpeg","path":"media/files/2009/10/21/washer.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/tetsugakunomiti-3.jpeg","path":"media/files/2009/10/21/tetsugakunomiti-3.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/tetsugakunomiti-2.jpeg","path":"media/files/2009/10/21/tetsugakunomiti-2.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/tetsugakunomiti-1.jpeg","path":"media/files/2009/10/21/tetsugakunomiti-1.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/sansu.jpeg","path":"media/files/2009/10/21/sansu.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/momiji-3.jpeg","path":"media/files/2009/10/21/momiji-3.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/momiji-2.jpeg","path":"media/files/2009/10/21/momiji-2.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/momiji-1.jpeg","path":"media/files/2009/10/21/momiji-1.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/miti-4.jpeg","path":"media/files/2009/10/21/miti-4.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/miti-3.jpeg","path":"media/files/2009/10/21/miti-3.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/miti-2.jpeg","path":"media/files/2009/10/21/miti-2.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/miti-1.jpeg","path":"media/files/2009/10/21/miti-1.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/kyotoeki-3.jpeg","path":"media/files/2009/10/21/kyotoeki-3.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/kyotoeki-2.jpeg","path":"media/files/2009/10/21/kyotoeki-2.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/kyotoeki-1.jpeg","path":"media/files/2009/10/21/kyotoeki-1.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/kouen-2.jpeg","path":"media/files/2009/10/21/kouen-2.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/kouen-1.jpeg","path":"media/files/2009/10/21/kouen-1.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/hilltop.jpeg","path":"media/files/2009/10/21/hilltop.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/ginkakuji-4.jpeg","path":"media/files/2009/10/21/ginkakuji-4.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/ginkakuji-3.jpeg","path":"media/files/2009/10/21/ginkakuji-3.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/ginkakuji-2.jpeg","path":"media/files/2009/10/21/ginkakuji-2.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/ginkakuji-1.jpeg","path":"media/files/2009/10/21/ginkakuji-1.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/consato.jpeg","path":"media/files/2009/10/21/consato.jpeg","modified":0},{"_id":"source/media/files/2009/10/21/bird.jpeg","path":"media/files/2009/10/21/bird.jpeg","modified":0},{"_id":"source/media/files/2009/10/01/track.jpg","path":"media/files/2009/10/01/track.jpg","modified":0},{"_id":"source/media/files/2009/10/01/toudaiji-10.jpg","path":"media/files/2009/10/01/toudaiji-10.jpg","modified":0},{"_id":"source/media/files/2009/10/01/toudaiji-09.jpg","path":"media/files/2009/10/01/toudaiji-09.jpg","modified":0},{"_id":"source/media/files/2009/10/01/toudaiji-08.jpg","path":"media/files/2009/10/01/toudaiji-08.jpg","modified":0},{"_id":"source/media/files/2009/10/01/toudaiji-07.jpg","path":"media/files/2009/10/01/toudaiji-07.jpg","modified":0},{"_id":"source/media/files/2009/10/01/toudaiji-06.jpg","path":"media/files/2009/10/01/toudaiji-06.jpg","modified":0},{"_id":"source/media/files/2009/10/01/toudaiji-05.jpg","path":"media/files/2009/10/01/toudaiji-05.jpg","modified":0},{"_id":"source/media/files/2009/10/01/toudaiji-04.jpg","path":"media/files/2009/10/01/toudaiji-04.jpg","modified":0},{"_id":"source/media/files/2009/10/01/toudaiji-03.jpg","path":"media/files/2009/10/01/toudaiji-03.jpg","modified":0},{"_id":"source/media/files/2009/10/01/toudaiji-02.jpg","path":"media/files/2009/10/01/toudaiji-02.jpg","modified":0},{"_id":"source/media/files/2009/10/01/toudaiji-01.jpg","path":"media/files/2009/10/01/toudaiji-01.jpg","modified":0},{"_id":"source/media/files/2009/10/01/syokubutsuen-02.jpg","path":"media/files/2009/10/01/syokubutsuen-02.jpg","modified":0},{"_id":"source/media/files/2009/10/01/syokubutsuen-01.jpg","path":"media/files/2009/10/01/syokubutsuen-01.jpg","modified":0},{"_id":"source/media/files/2009/10/01/kokuhoukan.jpg","path":"media/files/2009/10/01/kokuhoukan.jpg","modified":0},{"_id":"source/media/files/2009/10/01/kintetsunara.jpg","path":"media/files/2009/10/01/kintetsunara.jpg","modified":0},{"_id":"source/media/files/2009/10/01/kasugadaisya-04.jpg","path":"media/files/2009/10/01/kasugadaisya-04.jpg","modified":0},{"_id":"source/media/files/2009/10/01/kasugadaisya-03.jpg","path":"media/files/2009/10/01/kasugadaisya-03.jpg","modified":0},{"_id":"source/media/files/2009/10/01/kasugadaisya-02.jpg","path":"media/files/2009/10/01/kasugadaisya-02.jpg","modified":0},{"_id":"source/media/files/2009/10/01/kasugadaisya-01.jpg","path":"media/files/2009/10/01/kasugadaisya-01.jpg","modified":0},{"_id":"source/media/files/2009/10/01/gosoutou.jpg","path":"media/files/2009/10/01/gosoutou.jpg","modified":0},{"_id":"source/media/files/2009/10/01/gosoutou-kokuhoukan.jpg","path":"media/files/2009/10/01/gosoutou-kokuhoukan.jpg","modified":0},{"_id":"source/media/files/2009/10/01/deer-in-store.jpg","path":"media/files/2009/10/01/deer-in-store.jpg","modified":0},{"_id":"source/media/files/2009/10/01/deer-08.jpg","path":"media/files/2009/10/01/deer-08.jpg","modified":0},{"_id":"source/media/files/2009/10/01/deer-07.jpg","path":"media/files/2009/10/01/deer-07.jpg","modified":0},{"_id":"source/media/files/2009/10/01/deer-06.jpg","path":"media/files/2009/10/01/deer-06.jpg","modified":0},{"_id":"source/media/files/2009/10/01/deer-05.jpg","path":"media/files/2009/10/01/deer-05.jpg","modified":0},{"_id":"source/media/files/2009/10/01/deer-04.jpg","path":"media/files/2009/10/01/deer-04.jpg","modified":0},{"_id":"source/media/files/2009/10/01/deer-03.jpg","path":"media/files/2009/10/01/deer-03.jpg","modified":0},{"_id":"source/media/files/2009/10/01/deer-02.jpg","path":"media/files/2009/10/01/deer-02.jpg","modified":0},{"_id":"source/media/files/2009/10/01/deer-01.jpg","path":"media/files/2009/10/01/deer-01.jpg","modified":0},{"_id":"source/media/files/2009/10/01/dear-deer.jpg","path":"media/files/2009/10/01/dear-deer.jpg","modified":0},{"_id":"source/media/files/2009/09/27/yasakajinjya.jpg","path":"media/files/2009/09/27/yasakajinjya.jpg","modified":0},{"_id":"source/media/files/2009/09/27/yasakajinjya-6.jpg","path":"media/files/2009/09/27/yasakajinjya-6.jpg","modified":0},{"_id":"source/media/files/2009/09/27/yasakajinjya-5.jpg","path":"media/files/2009/09/27/yasakajinjya-5.jpg","modified":0},{"_id":"source/media/files/2009/09/27/yasakajinjya-4.jpg","path":"media/files/2009/09/27/yasakajinjya-4.jpg","modified":0},{"_id":"source/media/files/2009/09/27/yasakajinjya-3.jpg","path":"media/files/2009/09/27/yasakajinjya-3.jpg","modified":0},{"_id":"source/media/files/2009/09/27/yasakajinjya-2.jpg","path":"media/files/2009/09/27/yasakajinjya-2.jpg","modified":0},{"_id":"source/media/files/2009/09/27/yasakajinjya-1.jpg","path":"media/files/2009/09/27/yasakajinjya-1.jpg","modified":0},{"_id":"source/media/files/2009/09/27/simizuji-9.jpg","path":"media/files/2009/09/27/simizuji-9.jpg","modified":0},{"_id":"source/media/files/2009/09/27/simizuji-8.jpg","path":"media/files/2009/09/27/simizuji-8.jpg","modified":0},{"_id":"source/media/files/2009/09/27/simizuji-7.jpg","path":"media/files/2009/09/27/simizuji-7.jpg","modified":0},{"_id":"source/media/files/2009/09/27/simizuji-6.jpg","path":"media/files/2009/09/27/simizuji-6.jpg","modified":0},{"_id":"source/media/files/2009/09/27/simizuji-5.jpg","path":"media/files/2009/09/27/simizuji-5.jpg","modified":0},{"_id":"source/media/files/2009/09/27/simizuji-4.jpg","path":"media/files/2009/09/27/simizuji-4.jpg","modified":0},{"_id":"source/media/files/2009/09/27/simizuji-3.jpg","path":"media/files/2009/09/27/simizuji-3.jpg","modified":0},{"_id":"source/media/files/2009/09/27/simizuji-2.jpg","path":"media/files/2009/09/27/simizuji-2.jpg","modified":0},{"_id":"source/media/files/2009/09/27/simizuji-10.jpg","path":"media/files/2009/09/27/simizuji-10.jpg","modified":0},{"_id":"source/media/files/2009/09/27/simizuji-1.jpg","path":"media/files/2009/09/27/simizuji-1.jpg","modified":0},{"_id":"source/media/files/2009/09/27/myouhouin.jpg","path":"media/files/2009/09/27/myouhouin.jpg","modified":0},{"_id":"source/media/files/2009/09/27/kyoto-eki.jpg","path":"media/files/2009/09/27/kyoto-eki.jpg","modified":0},{"_id":"source/media/files/2009/09/27/heianjinko-5.jpg","path":"media/files/2009/09/27/heianjinko-5.jpg","modified":0},{"_id":"source/media/files/2009/09/27/heianjinko-4.jpg","path":"media/files/2009/09/27/heianjinko-4.jpg","modified":0},{"_id":"source/media/files/2009/09/27/heianjinko-3.jpg","path":"media/files/2009/09/27/heianjinko-3.jpg","modified":0},{"_id":"source/media/files/2009/09/27/heianjinko-2.jpg","path":"media/files/2009/09/27/heianjinko-2.jpg","modified":0},{"_id":"source/media/files/2009/09/27/heianjinko-1.jpg","path":"media/files/2009/09/27/heianjinko-1.jpg","modified":0},{"_id":"source/media/files/2009/09/27/ginkakuji.jpg","path":"media/files/2009/09/27/ginkakuji.jpg","modified":0},{"_id":"source/media/files/2009/09/25/kokyo-10.jpg","path":"media/files/2009/09/25/kokyo-10.jpg","modified":0},{"_id":"source/media/files/2009/09/25/kokyo-09.jpg","path":"media/files/2009/09/25/kokyo-09.jpg","modified":0},{"_id":"source/media/files/2009/09/25/kokyo-08.jpg","path":"media/files/2009/09/25/kokyo-08.jpg","modified":0},{"_id":"source/media/files/2009/09/25/kokyo-07.jpg","path":"media/files/2009/09/25/kokyo-07.jpg","modified":0},{"_id":"source/media/files/2009/09/25/kokyo-06.jpg","path":"media/files/2009/09/25/kokyo-06.jpg","modified":0},{"_id":"source/media/files/2009/09/25/kokyo-05.jpg","path":"media/files/2009/09/25/kokyo-05.jpg","modified":0},{"_id":"source/media/files/2009/09/25/kokyo-04.jpg","path":"media/files/2009/09/25/kokyo-04.jpg","modified":0},{"_id":"source/media/files/2009/09/25/kokyo-03.jpg","path":"media/files/2009/09/25/kokyo-03.jpg","modified":0},{"_id":"source/media/files/2009/09/25/kokyo-02.jpg","path":"media/files/2009/09/25/kokyo-02.jpg","modified":0},{"_id":"source/media/files/2009/09/25/kokyo-01.jpg","path":"media/files/2009/09/25/kokyo-01.jpg","modified":0},{"_id":"source/media/files/2009/08/30/time-capsule-size.jpg","path":"media/files/2009/08/30/time-capsule-size.jpg","modified":0},{"_id":"source/media/files/2009/08/09/xilaideng.jpg","path":"media/files/2009/08/09/xilaideng.jpg","modified":0},{"_id":"source/media/files/2009/08/09/tenpura.jpg","path":"media/files/2009/08/09/tenpura.jpg","modified":0},{"_id":"source/media/files/2009/08/09/tanekikouji.jpg","path":"media/files/2009/08/09/tanekikouji.jpg","modified":0},{"_id":"source/media/files/2009/08/09/tanekikouji-1.jpg","path":"media/files/2009/08/09/tanekikouji-1.jpg","modified":0},{"_id":"source/media/files/2009/08/09/susukinomatsuri.jpg","path":"media/files/2009/08/09/susukinomatsuri.jpg","modified":0},{"_id":"source/media/files/2009/08/09/monjyayaki.jpg","path":"media/files/2009/08/09/monjyayaki.jpg","modified":0},{"_id":"source/media/files/2009/08/09/manjiramen.jpg","path":"media/files/2009/08/09/manjiramen.jpg","modified":0},{"_id":"source/media/files/2009/08/09/kawaisounasakana.jpg","path":"media/files/2009/08/09/kawaisounasakana.jpg","modified":0},{"_id":"source/media/files/2009/08/09/kaminopporo.jpg","path":"media/files/2009/08/09/kaminopporo.jpg","modified":0},{"_id":"source/media/files/2009/08/09/jinjya.jpg","path":"media/files/2009/08/09/jinjya.jpg","modified":0},{"_id":"source/media/files/2009/08/09/jingisukan.jpg","path":"media/files/2009/08/09/jingisukan.jpg","modified":0},{"_id":"source/media/files/2009/08/09/hokkaido-daigaku.jpg","path":"media/files/2009/08/09/hokkaido-daigaku.jpg","modified":0},{"_id":"source/media/files/2009/08/09/hiroshimayaki.jpg","path":"media/files/2009/08/09/hiroshimayaki.jpg","modified":0},{"_id":"source/media/files/2009/08/09/dashiwodaite.jpg","path":"media/files/2009/08/09/dashiwodaite.jpg","modified":0},{"_id":"source/media/files/2009/08/09/dankiryu.jpg","path":"media/files/2009/08/09/dankiryu.jpg","modified":0},{"_id":"source/media/files/2009/08/09/chinese.jpg","path":"media/files/2009/08/09/chinese.jpg","modified":0},{"_id":"source/media/files/2009/07/28/03.jpg","path":"media/files/2009/07/28/03.jpg","modified":0},{"_id":"source/media/files/2009/07/28/02.jpg","path":"media/files/2009/07/28/02.jpg","modified":0},{"_id":"source/media/files/2009/07/28/01.jpg","path":"media/files/2009/07/28/01.jpg","modified":0},{"_id":"source/media/files/2009/01/26/street.jpg","path":"media/files/2009/01/26/street.jpg","modified":0},{"_id":"source/media/files/2009/01/26/snow.jpg","path":"media/files/2009/01/26/snow.jpg","modified":0},{"_id":"source/media/files/2009/01/26/santamaria.jpg","path":"media/files/2009/01/26/santamaria.jpg","modified":0},{"_id":"source/media/files/2009/01/26/parda.jpg","path":"media/files/2009/01/26/parda.jpg","modified":0},{"_id":"source/media/files/2009/01/26/mc.jpg","path":"media/files/2009/01/26/mc.jpg","modified":0},{"_id":"source/media/files/2009/01/26/lv.jpg","path":"media/files/2009/01/26/lv.jpg","modified":0},{"_id":"source/media/files/2009/01/26/inside-church.jpg","path":"media/files/2009/01/26/inside-church.jpg","modified":0},{"_id":"source/media/files/2009/01/26/inside-church-1.jpg","path":"media/files/2009/01/26/inside-church-1.jpg","modified":0},{"_id":"source/media/files/2009/01/26/fountain.jpg","path":"media/files/2009/01/26/fountain.jpg","modified":0},{"_id":"source/media/files/2009/01/26/duomo.jpg","path":"media/files/2009/01/26/duomo.jpg","modified":0},{"_id":"source/media/files/2009/01/26/church.jpg","path":"media/files/2009/01/26/church.jpg","modified":0},{"_id":"source/media/files/2009/01/26/castle.jpg","path":"media/files/2009/01/26/castle.jpg","modified":0},{"_id":"source/media/files/2009/01/26/castle-01.jpg","path":"media/files/2009/01/26/castle-01.jpg","modified":0},{"_id":"source/media/files/2009/01/26/bernasoon.jpg","path":"media/files/2009/01/26/bernasoon.jpg","modified":0},{"_id":"source/media/files/2008/12/16/venezia.jpg","path":"media/files/2008/12/16/venezia.jpg","modified":0},{"_id":"source/media/files/2008/12/16/venezia-wedding.jpg","path":"media/files/2008/12/16/venezia-wedding.jpg","modified":0},{"_id":"source/media/files/2008/12/16/venezia-tower.jpg","path":"media/files/2008/12/16/venezia-tower.jpg","modified":0},{"_id":"source/media/files/2008/12/16/venezia-stop-in-rain.jpg","path":"media/files/2008/12/16/venezia-stop-in-rain.jpg","modified":0},{"_id":"source/media/files/2008/12/16/venezia-stop-in-morning.jpg","path":"media/files/2008/12/16/venezia-stop-in-morning.jpg","modified":0},{"_id":"source/media/files/2008/12/16/venezia-river.jpg","path":"media/files/2008/12/16/venezia-river.jpg","modified":0},{"_id":"source/media/files/2008/12/16/venezia-morning.jpg","path":"media/files/2008/12/16/venezia-morning.jpg","modified":0},{"_id":"source/media/files/2008/12/16/venezia-mask.jpg","path":"media/files/2008/12/16/venezia-mask.jpg","modified":0},{"_id":"source/media/files/2008/12/16/venezia-lunch.jpg","path":"media/files/2008/12/16/venezia-lunch.jpg","modified":0},{"_id":"source/media/files/2008/12/16/venezia-in-water.jpg","path":"media/files/2008/12/16/venezia-in-water.jpg","modified":0},{"_id":"source/media/files/2008/12/16/venezia-in-water-04.jpg","path":"media/files/2008/12/16/venezia-in-water-04.jpg","modified":0},{"_id":"source/media/files/2008/12/16/venezia-in-water-03.jpg","path":"media/files/2008/12/16/venezia-in-water-03.jpg","modified":0},{"_id":"source/media/files/2008/12/16/venezia-in-water-02.jpg","path":"media/files/2008/12/16/venezia-in-water-02.jpg","modified":0},{"_id":"source/media/files/2008/12/16/venezia-hotel.jpg","path":"media/files/2008/12/16/venezia-hotel.jpg","modified":0},{"_id":"source/media/files/2008/12/16/venezia-beach.jpg","path":"media/files/2008/12/16/venezia-beach.jpg","modified":0},{"_id":"source/media/files/2008/12/16/tanxiqiao.jpg","path":"media/files/2008/12/16/tanxiqiao.jpg","modified":0},{"_id":"source/media/files/2008/08/18/sunrise-03.jpg","path":"media/files/2008/08/18/sunrise-03.jpg","modified":0},{"_id":"source/media/files/2008/08/18/sunrise-02.jpg","path":"media/files/2008/08/18/sunrise-02.jpg","modified":0},{"_id":"source/media/files/2008/08/18/sunrise-01.jpg","path":"media/files/2008/08/18/sunrise-01.jpg","modified":0},{"_id":"source/media/files/2008/08/18/stick.jpg","path":"media/files/2008/08/18/stick.jpg","modified":0},{"_id":"source/media/files/2008/08/18/me.jpg","path":"media/files/2008/08/18/me.jpg","modified":0},{"_id":"source/media/files/2008/08/18/cloud.jpg","path":"media/files/2008/08/18/cloud.jpg","modified":0},{"_id":"source/media/files/2008/07/22/ship.jpg","path":"media/files/2008/07/22/ship.jpg","modified":0},{"_id":"source/media/files/2008/07/22/port.jpg","path":"media/files/2008/07/22/port.jpg","modified":0},{"_id":"source/media/files/2008/07/22/jellyfish.jpg","path":"media/files/2008/07/22/jellyfish.jpg","modified":0},{"_id":"source/media/files/2008/07/22/eagle.jpg","path":"media/files/2008/07/22/eagle.jpg","modified":0},{"_id":"source/media/files/2008/07/22/beach.jpg","path":"media/files/2008/07/22/beach.jpg","modified":0},{"_id":"source/media/files/2008/07/22/all.jpg","path":"media/files/2008/07/22/all.jpg","modified":0},{"_id":"source/media/files/2008/06/29/mt-tmpl-opt.png","path":"media/files/2008/06/29/mt-tmpl-opt.png","modified":0},{"_id":"source/media/files/2008/06/29/mt-pub-ssi.png","path":"media/files/2008/06/29/mt-pub-ssi.png","modified":0},{"_id":"source/media/files/2008/06/08/okutama-05.jpg","path":"media/files/2008/06/08/okutama-05.jpg","modified":0},{"_id":"source/media/files/2008/06/08/okutama-04.jpg","path":"media/files/2008/06/08/okutama-04.jpg","modified":0},{"_id":"source/media/files/2008/06/08/okutama-03.jpg","path":"media/files/2008/06/08/okutama-03.jpg","modified":0},{"_id":"source/media/files/2008/06/08/okutama-02.jpg","path":"media/files/2008/06/08/okutama-02.jpg","modified":0},{"_id":"source/media/files/2008/06/08/okutama-01.jpg","path":"media/files/2008/06/08/okutama-01.jpg","modified":0},{"_id":"source/media/files/2008/03/31/txp_sofa_screenshot.jpg","path":"media/files/2008/03/31/txp_sofa_screenshot.jpg","modified":0},{"_id":"source/media/files/2008/03/24/dvfy.jpg","path":"media/files/2008/03/24/dvfy.jpg","modified":0},{"_id":"source/media/files/2007/07/26/travel-19.jpg","path":"media/files/2007/07/26/travel-19.jpg","modified":0},{"_id":"source/media/files/2007/07/26/travel-08.jpg","path":"media/files/2007/07/26/travel-08.jpg","modified":0},{"_id":"source/media/files/2007/07/26/travel-03.jpg","path":"media/files/2007/07/26/travel-03.jpg","modified":0},{"_id":"source/media/favicon.ico","path":"media/favicon.ico","modified":0}],"Cache":[{"_id":"themes/maupassant/LICENSE","shasum":"019dc6a9aba02ae3aaabca45f39aecd6e8e7f1d8","modified":1446873914000},{"_id":"themes/maupassant/README.md","shasum":"ea53522455f1b4b7f7237a8f73fafb1f5020cd65","modified":1446873914000},{"_id":"themes/maupassant/languages/en.yml","shasum":"077234127cb56038d59b5b899e4870901b18970e","modified":1446873914000},{"_id":"themes/maupassant/languages/zh-CN.yml","shasum":"c0224621380f3a61d039fe2f31bdecccd651c89e","modified":1446873914000},{"_id":"themes/maupassant/layout/_partial/comments.jade","shasum":"6d26080a2d4f474fc5ef2b3861b865a0b5546695","modified":1446873914000},{"_id":"themes/maupassant/layout/_partial/helpers.jade","shasum":"dd08cd568a2e2af9de5de0a406f15bad6640530b","modified":1446873914000},{"_id":"themes/maupassant/layout/_partial/paginator.jade","shasum":"89390e74abe48b85aaa2c95da6aa991c18559f48","modified":1446873914000},{"_id":"themes/maupassant/layout/_partial/post-nav.jade","shasum":"f5608d4cab87e66fe5230862d1f01580d98748e0","modified":1446873914000},{"_id":"themes/maupassant/layout/_partial/totop.jade","shasum":"759fa6a5d8d27189c706f8e0339c8fe05e5beb96","modified":1446873914000},{"_id":"themes/maupassant/layout/archive.jade","shasum":"386bbf6299096ee84010deebb7a1b6033dc204bf","modified":1446873914000},{"_id":"themes/maupassant/layout/base.jade","shasum":"9cda7d0c7d3cf1700bb14ba5bcab53857d6bb8ae","modified":1446873914000},{"_id":"themes/maupassant/layout/index.jade","shasum":"b8c050b6c105108a87c5680230c99493d4902b3b","modified":1446873914000},{"_id":"themes/maupassant/layout/page.jade","shasum":"aadf9faa1427641dcb335786ef09eecfd0113d23","modified":1446873914000},{"_id":"themes/maupassant/layout/post.jade","shasum":"48e2c11f5311b790cbfc8c97cd89d9748c3b4fbb","modified":1446873914000},{"_id":"themes/maupassant/package.json","shasum":"81fb4e2ac051ecfb9a93f37b28910291b939771a","modified":1446873914000},{"_id":"themes/maupassant/source/css/grids-responsive-min.css","shasum":"703826508193cbe21f2745d3e837256e224eb512","modified":1446873914000},{"_id":"themes/maupassant/source/css/jquery.fancybox.css","shasum":"3e5850dfd0ac87fe34a5c680d50f300536542bae","modified":1446873914000},{"_id":"themes/maupassant/source/css/normalize.css","shasum":"02fe53286d071637534d5aa2c57c76c168c0d521","modified":1446873914000},{"_id":"themes/maupassant/source/fancybox/blank.gif","shasum":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1446873914000},{"_id":"themes/maupassant/source/css/pure-min.css","shasum":"8cf7ea3e9e00e752de63fbc443e9300366327cd9","modified":1446873914000},{"_id":"themes/maupassant/source/fancybox/fancybox_sprite.png","shasum":"17df19f97628e77be09c352bf27425faea248251","modified":1446873914000},{"_id":"themes/maupassant/source/fancybox/fancybox_sprite@2x.png","shasum":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1446873914000},{"_id":"themes/maupassant/source/css/style.scss","shasum":"8dc6625813dcce25a79425e7a5a5ae3147743c47","modified":1446873914000},{"_id":"themes/maupassant/source/fancybox/fancybox_overlay.png","shasum":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1446873914000},{"_id":"themes/maupassant/source/fonts/icomoon.eot","shasum":"6d4fddd9bae85803d2cf483700e9e46b630dfc63","modified":1446873914000},{"_id":"themes/maupassant/source/fonts/icomoon.svg","shasum":"54679013c08696615e3f83e2cf86bf02e17f1deb","modified":1446873914000},{"_id":"themes/maupassant/source/fonts/icomoon.ttf","shasum":"42fbffaef5b1224ce8d8349a5c7c3d7f7ecbb8ae","modified":1446873914000},{"_id":"themes/maupassant/source/fonts/icomoon.woff","shasum":"5f0e31266d6dd7a6d459d583599287e9539f1da8","modified":1446873914000},{"_id":"themes/maupassant/source/js/fancybox.pack.js","shasum":"53360764b429c212f424399384417ccc233bb3be","modified":1446873914000},{"_id":"themes/maupassant/source/js/totop.js","shasum":"f6fdc3d999e8d03aa55d4c5aabfcb1b2bdf0b4c6","modified":1446873914000},{"_id":"themes/maupassant/source/js/jquery.min.js","shasum":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1446873914000},{"_id":"source/_posts/2014-01-20-golang-vs-nginx-test.markdown","shasum":"0fe728640854d819e13383eab50b6ebc49640a97","modified":1446881381000},{"_id":"source/_posts/2014-07-26-stroage_consistency_avaliable_post1.markdown","shasum":"008c33afdb782ac9e4552ba965b8d040f59232ad","modified":1435383395000},{"_id":"source/_posts/2014-08-12-stroage_consistency_avaliable_post2.markdown","shasum":"3e2f141e9b3f2d29805a947058af4519b9b35bf8","modified":1427410842000},{"_id":"source/_posts/2014-09-02-stroage_consistency_replicate.markdown","shasum":"706fa2d1ae3dfe5f33fb8a6fdb858214b7a19226","modified":1427415761000},{"_id":"source/_posts/2014-09-24-goroutine-scheduler.markdown","shasum":"0fda01576bdf6fbaa18199897ce623b88eb9cbc8","modified":1427722133000},{"_id":"source/_posts/2014-10-20-goroutine-contiguous-stack.markdown","shasum":"0b91b44f4b97f674e4fd1ac07c23dc0852cf5f00","modified":1427323644000},{"_id":"source/_posts/2015-06-26-go-runtime-1.4.markdown","shasum":"7234669af0c825c87fe42c7cd0a20dfae88061e9","modified":1435383435000},{"_id":"source/_posts/2015-09-23-youpai_communicate.markdown","shasum":"dc34bbfcae79bef70ac7b8c662d6beea4bca632b","modified":1443703532000},{"_id":"source/_posts/2015-10-09-facebook_photo_caching.markdown","shasum":"e36e07e3c7ee6f401daec5511be43cf8f3c57ffe","modified":1444485840000},{"_id":"source/media/css/style.scss","shasum":"78a68da82385df5282a01b92238803876a66e696","modified":1421763105000},{"_id":"source/media/css/style.css","shasum":"fc617fde55dc532c564f208bb3ad94eaf6c05514","modified":1421763105000},{"_id":"source/_posts/Untitled.md.bak","shasum":"9be0c3819929edc3d0b462e3e44ff7fef43378dd","modified":1427671249000},{"_id":"source/_posts/hello-world.bak","shasum":"774b507901d9b17991ace2a70263b6dd6d11999a","modified":1446871550000},{"_id":"source/media/favicon.ico","shasum":"93464c518e082f98aeef062a31be21502c681c6b","modified":1421763105000},{"_id":"source/media/files/2008/03/24/dvfy.jpg","shasum":"6a5549018e6c082578a387c9f4a5303d62343a14","modified":1421763105000},{"_id":"source/media/files/2007/07/26/travel-19.jpg","shasum":"747d5e4d54514de9ae10a45bf374817479e61ab7","modified":1421763105000},{"_id":"source/media/files/2008/03/31/txp_sofa_screenshot.jpg","shasum":"13725608626e47d16e3017ad926d3f3e09b1d64b","modified":1421763105000},{"_id":"source/media/files/2007/07/26/travel-08.jpg","shasum":"0f6ee159549dcf71bf5792973b8b63e50ede5959","modified":1421763105000},{"_id":"source/media/files/2008/06/29/mt-tmpl-opt.png","shasum":"795f6211c1ee2b6c9e09f7d37b781f2ffce5fd32","modified":1421763105000},{"_id":"source/media/files/2008/06/08/okutama-04.jpg","shasum":"8d78807b6224f9da340c53bc642a81fccad9a6bb","modified":1421763105000},{"_id":"source/media/files/2008/07/22/beach.jpg","shasum":"6c44ea4f6da07582aa23f33a1c87be5bca408556","modified":1421763105000},{"_id":"source/media/files/2008/06/29/mt-pub-ssi.png","shasum":"c5ba6d44215b1e138a36412a1e9f0f55c4d5e4b9","modified":1421763105000},{"_id":"source/media/files/2008/07/22/port.jpg","shasum":"373ed1e06152a306a7e09f080eaa1d90ed56a842","modified":1421763105000},{"_id":"source/media/files/2008/08/18/cloud.jpg","shasum":"4e1a66b8c1485e22c72f97036b8b6c9bdbc91ba8","modified":1421763105000},{"_id":"source/media/files/2008/08/18/me.jpg","shasum":"3235342d74fed65244c209735b63ad64712df9e1","modified":1421763105000},{"_id":"source/media/files/2008/07/22/ship.jpg","shasum":"0fea8e5f2857c6abbf1debcbedbf6e14bd239339","modified":1421763105000},{"_id":"source/media/files/2008/08/18/sunrise-01.jpg","shasum":"69510c2e3d960422d05f632ff21578246d86fdca","modified":1421763105000},{"_id":"source/media/files/2008/08/18/sunrise-02.jpg","shasum":"4dcec153ed4f7095c65da6151c7f83054a92d3be","modified":1421763105000},{"_id":"source/media/files/2008/08/18/sunrise-03.jpg","shasum":"fd36a5059f77b44740b71d65f5f3516e9af27098","modified":1421763105000},{"_id":"source/media/files/2008/08/18/stick.jpg","shasum":"dd7da41d009d654d4f862668fc6fcc2f6de586a2","modified":1421763105000},{"_id":"source/media/files/2008/12/16/tanxiqiao.jpg","shasum":"b86a2fd25673557ba00c9bffd7d104c130d33344","modified":1421763105000},{"_id":"source/media/files/2008/07/22/eagle.jpg","shasum":"eb0853719d5d933331735e559bdf8e0281acdebf","modified":1421763105000},{"_id":"source/media/files/2008/12/16/venezia-hotel.jpg","shasum":"32967a9e4b750714d180b212d93c7a92aa8d00fc","modified":1421763105000},{"_id":"source/media/files/2008/12/16/venezia-in-water-03.jpg","shasum":"3126b52a4e61003991e7c1d1b5796e1c99764eb6","modified":1421763105000},{"_id":"source/media/files/2008/12/16/venezia-morning.jpg","shasum":"f418d3eedfac97e9023643879a5279963949706c","modified":1421763105000},{"_id":"source/media/files/2008/12/16/venezia-river.jpg","shasum":"30576235225ea8c37a9d686641d746188b0dd201","modified":1421763105000},{"_id":"source/media/files/2008/12/16/venezia-stop-in-morning.jpg","shasum":"e78c88b50b22bf8567a369131071de065f3a0122","modified":1421763105000},{"_id":"source/media/files/2008/12/16/venezia-stop-in-rain.jpg","shasum":"87f2a082b3a14149e08c2a832958873c1a2cd8a3","modified":1421763105000},{"_id":"source/media/files/2008/12/16/venezia-wedding.jpg","shasum":"7441f454afd1ecf3b0135b576e9492ea00c43ca4","modified":1421763105000},{"_id":"source/media/files/2009/01/26/castle.jpg","shasum":"3b6381a3d8a94121b731e6070505923df9f64b18","modified":1421763105000},{"_id":"source/media/files/2009/01/26/fountain.jpg","shasum":"1096b355419003bd85247b1f36866be90f87ebca","modified":1421763105000},{"_id":"source/media/files/2009/01/26/castle-01.jpg","shasum":"81de985d9a5786bdcb6316f0ea377e5631452512","modified":1421763105000},{"_id":"source/media/files/2009/01/26/inside-church.jpg","shasum":"487c4c0c6e9b01f3c42ae9edc00570be01705ca1","modified":1421763105000},{"_id":"source/media/files/2009/01/26/church.jpg","shasum":"6196e1aae6f0997fe5a190f11acbf073511f6a48","modified":1421763105000},{"_id":"source/media/files/2009/01/26/snow.jpg","shasum":"45ba72c97986f2331583a15fdde4decaf7f7d2e6","modified":1421763105000},{"_id":"source/media/files/2009/01/26/street.jpg","shasum":"503a829bb9958ec7e65a900851c75786da61c049","modified":1421763105000},{"_id":"source/media/files/2009/01/26/santamaria.jpg","shasum":"0348820752740d1ea521c8fe6a2db7b0288c1cee","modified":1421763105000},{"_id":"source/media/files/2009/07/28/02.jpg","shasum":"5915b1b121d6b112186813d6c818f839acc17d68","modified":1421763105000},{"_id":"source/media/files/2009/07/28/03.jpg","shasum":"5b299601361f6a8c74f046f8252c336005a88f9c","modified":1421763105000},{"_id":"source/media/files/2009/08/09/hiroshimayaki.jpg","shasum":"fb1276772596241861c7a14f32c2144e1b1c888d","modified":1421763105000},{"_id":"source/media/files/2009/08/09/kawaisounasakana.jpg","shasum":"6fcb52ddbea98cdfbe1a316cf636cc0edfe8b89f","modified":1421763105000},{"_id":"source/media/files/2009/08/09/monjyayaki.jpg","shasum":"1e04f970164bca90829530fe9fe4a87bdad54236","modified":1421763105000},{"_id":"source/media/files/2009/09/25/kokyo-04.jpg","shasum":"152d0b5d8ed391cd4121f69c2f054d72101088bc","modified":1421763105000},{"_id":"source/media/files/2009/09/25/kokyo-05.jpg","shasum":"38b30ec994735deffb28cce4f0a89ac8257c3e0d","modified":1421763105000},{"_id":"source/media/files/2009/09/25/kokyo-07.jpg","shasum":"ae3ba3a817b82f5dd0d8281bdc8c4153329240f1","modified":1421763105000},{"_id":"source/media/files/2009/09/25/kokyo-10.jpg","shasum":"de530f49ff4b41ca9e2b3ea5ea6e95b127a0a977","modified":1421763105000},{"_id":"source/media/files/2009/09/27/ginkakuji.jpg","shasum":"6cd915999b2585da1164bb9f1c7da7bb89e6f200","modified":1421763105000},{"_id":"source/media/files/2009/09/27/heianjinko-2.jpg","shasum":"eaab4908a2d4b156262062e39b15d3d10420e98b","modified":1421763105000},{"_id":"source/media/files/2009/09/27/heianjinko-3.jpg","shasum":"718e331fbc2a249a6934ca95790a2b16b39ecf61","modified":1421763105000},{"_id":"source/media/files/2009/09/27/heianjinko-5.jpg","shasum":"77d4b07180b10353653e3bba7da74bb96558eea4","modified":1421763105000},{"_id":"source/media/files/2009/09/27/simizuji-1.jpg","shasum":"b353492265318761856c9b111660fd3959583f81","modified":1421763105000},{"_id":"source/media/files/2009/09/27/simizuji-3.jpg","shasum":"f81ef4b63a0a00dd941a754da2bf567eae4d054a","modified":1421763105000},{"_id":"source/media/files/2009/09/27/simizuji-6.jpg","shasum":"da2344650a38e203e728bb31558d7ec2809bb93d","modified":1421763105000},{"_id":"source/media/files/2009/09/27/yasakajinjya-3.jpg","shasum":"b9f7412fff1ceae5ab4183bf675ca114ba5bea93","modified":1421763105000},{"_id":"source/media/files/2009/09/27/yasakajinjya-5.jpg","shasum":"7db1e2a029a37c0f3332251cf33c858047c79d31","modified":1421763105000},{"_id":"source/media/files/2009/09/27/yasakajinjya-6.jpg","shasum":"135b6b706f84d5def4785b9888b076321cd37b26","modified":1421763105000},{"_id":"source/media/files/2009/10/01/dear-deer.jpg","shasum":"12dc5e5857e538c6b53423429746e69fe9eb0c6f","modified":1421763105000},{"_id":"source/media/files/2009/10/01/deer-in-store.jpg","shasum":"0514f5ee3b6c6c609c5441884c5d6bdede48adc2","modified":1421763105000},{"_id":"source/media/files/2009/10/01/gosoutou.jpg","shasum":"8076f7b611a3c79af4708a580826d4fcd2855f47","modified":1421763105000},{"_id":"source/media/files/2009/10/01/deer-06.jpg","shasum":"54dc044036fbc462d6e600b1d9da3dc347c27157","modified":1421763105000},{"_id":"source/media/files/2009/10/01/syokubutsuen-01.jpg","shasum":"99889f915a80679d7b4962a2adab5c39dd2e9e36","modified":1421763105000},{"_id":"source/media/files/2009/10/01/toudaiji-01.jpg","shasum":"cab167484df91093b4ddc4812f03307fc66817dc","modified":1421763105000},{"_id":"source/media/files/2009/10/01/toudaiji-02.jpg","shasum":"af52f1365a901bd82a484f259d9c07e5c1df460f","modified":1421763105000},{"_id":"source/media/files/2009/10/01/kokuhoukan.jpg","shasum":"664e4abe76645a4cfc098534118030d7350c6165","modified":1421763105000},{"_id":"source/media/files/2009/10/01/toudaiji-10.jpg","shasum":"203c3a9ddc6bf6d52519b61a208a1edb76eb0ebc","modified":1421763105000},{"_id":"source/media/files/2009/10/21/bird.jpeg","shasum":"f6fcdea39cff0f6df1782588e073fae6e2caec41","modified":1421763105000},{"_id":"source/media/files/2009/10/21/ginkakuji-3.jpeg","shasum":"a345a7583311f938151490da39930a887df5243d","modified":1421763105000},{"_id":"source/media/files/2009/10/21/kouen-2.jpeg","shasum":"9a80dec28d7ed69b91043c72be6d423e24344ae8","modified":1421763105000},{"_id":"source/media/files/2009/10/21/kyotoeki-2.jpeg","shasum":"3e32bcf6f5124a42e503d4746a3125a13e84af9d","modified":1421763105000},{"_id":"source/media/files/2009/10/21/kyotoeki-3.jpeg","shasum":"bf40fc91562b74e316154e63f26e674343d58aed","modified":1421763105000},{"_id":"source/media/files/2009/10/21/kouen-1.jpeg","shasum":"aad1cc68acdd00c60230f7f0ec437eb17c200bac","modified":1421763105000},{"_id":"source/media/files/2009/10/21/miti-4.jpeg","shasum":"bfb0256a774c26e471debe33b979482ce5915441","modified":1421763105000},{"_id":"source/media/files/2009/10/21/momiji-3.jpeg","shasum":"2671bd4bf3de94cb20795e8898dd18d598d4f32f","modified":1421763105000},{"_id":"source/media/files/2009/10/21/miti-2.jpeg","shasum":"e5952527abf3571d5142c66511a284cc9af3c0b3","modified":1421763105000},{"_id":"source/media/files/2009/10/21/tetsugakunomiti-1.jpeg","shasum":"912e3a526784c262dad71f8e293b233a9d178a8d","modified":1421763105000},{"_id":"source/media/files/2010/01/14/disney-sea-01.jpg","shasum":"7442c20ac0dc6f5aa606641c1e8fefc599ea8d32","modified":1421763105000},{"_id":"source/media/files/2010/01/14/disney-sea-10.jpg","shasum":"99f94a2e27ad55c67a02e5977deed25ce6c61980","modified":1421763105000},{"_id":"source/media/files/2010/01/14/disney-sea-15.jpg","shasum":"b6dee93fa144733cc518e974bcd40fb26351068e","modified":1421763105000},{"_id":"source/media/files/2010/06/30/box.jpg","shasum":"ba00135b909da4955adb72ee863da8f08b5070fb","modified":1421763105000},{"_id":"source/media/files/2010/08/17/sunrise.jpg","shasum":"cf3df516236f76fef9731234e198d3b23dbfbece","modified":1421763105000},{"_id":"source/media/files/2011/02/24/wp2txt.xsl","shasum":"35a1de98c3d22076eb7433e776c5d626acd300b5","modified":1421763105000},{"_id":"source/media/files/2010/10/29/rain.jpg","shasum":"e9ec458a0d749431bf75366eec41071beecf0273","modified":1421763105000},{"_id":"source/media/files/2011/11/22/UIImageView-and-userInteractionEnabled.dot","shasum":"1f80365fb0072c7b5faecee396a215be9bca0ff1","modified":1421763105000},{"_id":"source/media/files/2011/11/22/with-analytics.png","shasum":"59438dc798c1608e5b2ce101a7badb6dd97eadd9","modified":1421763105000},{"_id":"source/media/files/2011/11/22/with-description.png","shasum":"2a5885547502ae4c22ef489f7a6c89bc03bd9bdb","modified":1421763105000},{"_id":"source/media/files/2011/11/22/bug.png","shasum":"f5df6ecf75ac853ce315af0b71228478ad9787f1","modified":1421763105000},{"_id":"source/media/files/2011/11/29/SDIM0087.jpg","shasum":"97e942382d1ad5501d03fdcf478d4bfca5d6edf2","modified":1421763105000},{"_id":"source/media/files/2012/04/16/leaf.jpg","shasum":"4eab18e4a4cb0dacdcc70d807dcdcfa2ec52132c","modified":1421763105000},{"_id":"source/media/files/2014/07/3.png","shasum":"28beea925d60c4166b330f0b88534f512288e9dc","modified":1427237242000},{"_id":"source/media/files/2014/07/OPaOPb.png","shasum":"89684663a911d957c56656489c231d50c533cef6","modified":1427237119000},{"_id":"source/media/files/2014/07/Primary.png","shasum":"f26fbb9422fce2aca617a41cdd4a494e17048de3","modified":1427237233000},{"_id":"source/media/files/2014/08/follower1.png","shasum":"e4afbc6dbbd6b637222141da377d7b82d0756460","modified":1427237606000},{"_id":"source/media/files/2014/08/followerfailover.png","shasum":"cc0c6e1b0322448424ed91547a257052ceb3c99a","modified":1427237599000},{"_id":"source/media/files/2014/08/master.png","shasum":"f8b45841b8bf8d1dbfe5289dedf4f754d26bf6e2","modified":1427237565000},{"_id":"source/media/files/2014/08/multileader.png","shasum":"a067c1a6c1ddfd0818621d2f10319f154a55f3bb","modified":1427237626000},{"_id":"source/media/files/2014/08/leaderfailover.png","shasum":"bfe73d12d91081b39346d1e7b3c849e3da200605","modified":1427237592000},{"_id":"source/media/files/2014/09/24/goroutinestate.jpg","shasum":"07f60252144c61b2ba4789c9aa167dad6a4487d1","modified":1427322033000},{"_id":"source/media/files/2014/09/24/in-motion.jpg","shasum":"21ab4f451345f0407b17b3486901f708ff0c37f2","modified":1427322012000},{"_id":"source/media/files/2014/09/24/our-cast.jpg","shasum":"f0721b3dbef5428c10afbeb6cf686fcfda089a92","modified":1427321987000},{"_id":"source/media/files/2014/09/24/steal.jpg","shasum":"2e735f25a3358d492aa8df6457801da96329c5db","modified":1427322026000},{"_id":"source/media/files/2014/09/epoch.png","shasum":"1450c815f72dbfa89efecf89151a18fa8e1629c6","modified":1427238468000},{"_id":"source/media/files/2014/09/24/syscall.jpg","shasum":"2e276fca5fda66e4e8c5237212d614b9964bdc97","modified":1427322019000},{"_id":"source/media/files/2014/09/replication-async-300x263.png","shasum":"ad11fa4afb4aac6c6347286063da8e7c7e62c167","modified":1427238399000},{"_id":"source/media/files/2014/09/google-transact09.png","shasum":"3ba8f07e2e92834c44beacbb1a9445440ed6b680","modified":1427238406000},{"_id":"source/media/files/2014/09/replication-both-300x206.png","shasum":"1f4121430d62cd502de63955de64b6edf41db17d","modified":1427238368000},{"_id":"source/media/files/2014/09/system-of-2-300x87.png","shasum":"8fb9b36a7d4912d493b8f36e1c2350ad52efdae1","modified":1427238451000},{"_id":"source/media/files/2014/09/replication-sync-300x274.png","shasum":"e2180cb75fed58b447633e19854d7be095497190","modified":1427238393000},{"_id":"source/media/files/2014/09/system-of-3-300x138.png","shasum":"204e21bd933078550c796a5b90df87778693a635","modified":1427238460000},{"_id":"source/media/files/2015/06/sizeclass.jpg","shasum":"097a622b13586675e6e571a1cf36a28cc348348e","modified":1435308905000},{"_id":"source/media/files/2015/06/span.jpg","shasum":"c9dbee57e10c31ccac5b40fa9bd6421fb01dc9d9","modified":1435308291000},{"_id":"source/media/files/2014/08/statechange.png","shasum":"174b5530f210b9e2ec21cd1e866c12be9b2f46cf","modified":1427237634000},{"_id":"source/media/files/2015/10/11.png","shasum":"724b883980905773eb0a25fd18f2cf7995e837e1","modified":1444470237000},{"_id":"source/media/files/2015/10/10.png","shasum":"0231d25aeb3d57275df8e0f261273a41d1274dc3","modified":1444470227000},{"_id":"source/media/files/2015/10/12.png","shasum":"af09872f581d0f1524839120f6065b03ec79222e","modified":1444470244000},{"_id":"source/media/files/2015/10/2.png","shasum":"f7fd3980b413b3e7bb809597e710d38e3e4702fe","modified":1444470155000},{"_id":"source/media/files/2015/10/13.png","shasum":"14d5e29b554c77083301f07f34abf67600d27066","modified":1444470248000},{"_id":"source/media/files/2015/10/5.png","shasum":"3b4f92f83bfd52503a79e6a6af0f9455cb533118","modified":1444470184000},{"_id":"source/media/files/2015/10/8.png","shasum":"e5073bff165220fe49d482cfff6183e6c596bc7c","modified":1444470204000},{"_id":"source/media/fonts/telex-regular-webfont.eot","shasum":"a494c8afb09b3b8c5139bf9c3b9998616b01652f","modified":1421763105000},{"_id":"source/media/fonts/telex-regular-webfont.woff","shasum":"679bd5e953d7fd46e20cd2b59586ec71af15e733","modified":1421763105000},{"_id":"source/media/fonts/telex-regular.css","shasum":"3f71bb0cd895e159e279d4bb0baa2063b579467f","modified":1421763105000},{"_id":"source/media/js/jquery.tagcloud.js","shasum":"012beeb2895477692ee67f09c694e06863b189dd","modified":1421763105000},{"_id":"source/media/fonts/telex-regular-webfont.ttf","shasum":"b3cda4ac36a67ae3d4b69f24450dfd62d7e91f75","modified":1421763105000},{"_id":"source/media/files/2007/07/26/travel-03.jpg","shasum":"af279b676deab7d9e7773360c460b48e7c0c6f81","modified":1421763105000},{"_id":"source/media/files/2008/06/08/okutama-01.jpg","shasum":"a79613bbca4eb55c12cb1b54a32249d46a91e1aa","modified":1421763105000},{"_id":"source/media/files/2008/06/08/okutama-03.jpg","shasum":"01ce32d5ab459e873ef73be1e9c0d5fa8a6fe577","modified":1421763105000},{"_id":"source/media/fonts/telex-regular-webfont.svg","shasum":"eeab91723e2433eebf23bc70d0d59e1dd47e44b3","modified":1421763105000},{"_id":"source/media/files/2008/06/08/okutama-05.jpg","shasum":"5bcdf45001a436f9865daac7ecf31b1ef12fd5b8","modified":1421763105000},{"_id":"source/media/files/2008/06/08/okutama-02.jpg","shasum":"375806141925af98ea7573256cf093b6d6406dee","modified":1421763105000},{"_id":"source/media/files/2008/07/22/jellyfish.jpg","shasum":"0df64ec0384083c1faefc98f3bb14f0df145700f","modified":1421763105000},{"_id":"source/media/files/2008/12/16/venezia-in-water-02.jpg","shasum":"8dbedf6a010c7d4cdb7225bded06bcadfe6bea5f","modified":1421763105000},{"_id":"source/media/files/2008/12/16/venezia-beach.jpg","shasum":"88adffce33c94f44d408b42f8a636856b7b49ea7","modified":1421763105000},{"_id":"source/media/files/2008/07/22/all.jpg","shasum":"493290203a2b72010c687f4fb3bdb844c4b30e57","modified":1421763105000},{"_id":"source/media/files/2008/12/16/venezia-lunch.jpg","shasum":"22e2d31d0daf3790074a9f97c82ce4db003da99d","modified":1421763105000},{"_id":"source/media/files/2008/12/16/venezia-in-water-04.jpg","shasum":"3512c86a92101d6d65c9a5a08855fde383c47c3b","modified":1421763105000},{"_id":"source/media/files/2008/12/16/venezia-mask.jpg","shasum":"1033ba14fe90320bd877951792daf8c001bc93f0","modified":1421763105000},{"_id":"source/media/files/2008/12/16/venezia.jpg","shasum":"8ce358f5d8fc9781ad15244bd4b35a956fc86980","modified":1421763105000},{"_id":"source/media/files/2009/01/26/bernasoon.jpg","shasum":"78e8d8a0801013fc945edd06890e8ad0161c9003","modified":1421763105000},{"_id":"source/media/files/2008/12/16/venezia-tower.jpg","shasum":"7dd11aabdc68d54db95dd9e7151b25cec54870b1","modified":1421763105000},{"_id":"source/media/files/2009/01/26/duomo.jpg","shasum":"3f7dc2388d7acde6bc66ce9896ce3cd405ef49d0","modified":1421763105000},{"_id":"source/media/files/2009/01/26/inside-church-1.jpg","shasum":"b8948381a6b54222154f75617e3fa456f033aeca","modified":1421763105000},{"_id":"source/media/files/2009/01/26/lv.jpg","shasum":"be51e33134de8c3005cc7e70c913d79ef4be1542","modified":1421763105000},{"_id":"source/media/files/2009/01/26/mc.jpg","shasum":"cb421dbc3026b872f85cb6b9d6d0e35850356c20","modified":1421763105000},{"_id":"source/media/files/2008/12/16/venezia-in-water.jpg","shasum":"a2fe94747fafebcb6d100cc1c80f5a77391b74f5","modified":1421763105000},{"_id":"source/media/files/2009/01/26/parda.jpg","shasum":"457d214b87222735112e52c3d6dc13f1f2ceed9f","modified":1421763105000},{"_id":"source/media/files/2009/07/28/01.jpg","shasum":"aa30534b503eefdf710d7a3296e3c55dfb2fe554","modified":1421763105000},{"_id":"source/media/files/2009/08/09/chinese.jpg","shasum":"5a0ab6d2f038e92e2cd52b0011f758f88c6cd2ce","modified":1421763105000},{"_id":"source/media/files/2009/08/09/dankiryu.jpg","shasum":"b8b27caa934e7ba275b3afe6a6f98852013712c6","modified":1421763105000},{"_id":"source/media/files/2009/08/09/dashiwodaite.jpg","shasum":"e717b27ce790ceb629a4711a9fb9e72dab07e1c0","modified":1421763105000},{"_id":"source/media/files/2009/08/09/hokkaido-daigaku.jpg","shasum":"e226668e9ec3d71ee7d9f406eba29c82a4d32b8c","modified":1421763105000},{"_id":"source/media/files/2009/08/09/jingisukan.jpg","shasum":"346a1ef2c8e8bf77826fb8cbcc14130b6cd956c3","modified":1421763105000},{"_id":"source/media/files/2009/08/09/jinjya.jpg","shasum":"c9a1b41a5578b873658f2e09dc524780cffaebb0","modified":1421763105000},{"_id":"source/media/files/2009/08/09/kaminopporo.jpg","shasum":"3a90985da71a5de50f12d0ead4d172bf720c09f3","modified":1421763105000},{"_id":"source/media/files/2009/08/09/manjiramen.jpg","shasum":"e5bdf0d5fb8d99a6908470678264f59257086afa","modified":1421763105000},{"_id":"source/media/files/2009/08/09/susukinomatsuri.jpg","shasum":"cabd76e5e76513008120f8d5366a04139daf4a75","modified":1421763105000},{"_id":"source/media/files/2009/08/09/tanekikouji-1.jpg","shasum":"3169ea1a14af16c3747b3f00ea999f0b58c3db82","modified":1421763105000},{"_id":"source/media/files/2009/08/09/tenpura.jpg","shasum":"dfffa4a76424e13ad7b77506f9d74f8acdd9cc84","modified":1421763105000},{"_id":"source/media/files/2009/08/09/tanekikouji.jpg","shasum":"5a9bd36008051dea6570b852b6a6ad8c32b50c9c","modified":1421763105000},{"_id":"source/media/files/2009/08/09/xilaideng.jpg","shasum":"96bf90c03b2c91d38974cfea5cdd42d76e777cf6","modified":1421763105000},{"_id":"source/media/files/2009/09/25/kokyo-01.jpg","shasum":"1bafaca7f27ddb78f7e676052e0f469b6cf84606","modified":1421763105000},{"_id":"source/media/files/2009/09/25/kokyo-02.jpg","shasum":"9c871589af88c593aeb88f027cc5e5d263eab3a2","modified":1421763105000},{"_id":"source/media/files/2009/09/25/kokyo-03.jpg","shasum":"f14d7d01aae220198839f75219963813b3646d6c","modified":1421763105000},{"_id":"source/media/files/2009/09/25/kokyo-08.jpg","shasum":"21f57812a0022a8a319952b715f0c75caa503f6f","modified":1421763105000},{"_id":"source/media/files/2009/09/25/kokyo-06.jpg","shasum":"94f481ebae95744a9ddd110006dbd9c69fe764b5","modified":1421763105000},{"_id":"source/media/files/2009/09/25/kokyo-09.jpg","shasum":"c3732ef164dd0b82655df10a559b2ead5abf8a4b","modified":1421763105000},{"_id":"source/media/files/2009/09/27/heianjinko-1.jpg","shasum":"8eeddd2f3c921a725c5c7eb9af0f4c6d1faebc83","modified":1421763105000},{"_id":"source/media/files/2009/08/30/time-capsule-size.jpg","shasum":"a37122b054b9eaa25c3756202010e899e6acc6c1","modified":1421763105000},{"_id":"source/media/files/2009/09/27/heianjinko-4.jpg","shasum":"9463c1e076c22fa407e0c80ce9305211a88ed4bc","modified":1421763105000},{"_id":"source/media/files/2009/09/27/myouhouin.jpg","shasum":"bf202ee91f38d8d9c90f302b11d0496e91e7ac45","modified":1421763105000},{"_id":"source/media/files/2009/09/27/kyoto-eki.jpg","shasum":"798099c0b13cff8862a8ac122a37b9040eb4db84","modified":1421763105000},{"_id":"source/media/files/2009/09/27/simizuji-2.jpg","shasum":"304d2d45783fdebc6e19c47a097bf462f41c5884","modified":1421763105000},{"_id":"source/media/files/2009/09/27/simizuji-4.jpg","shasum":"e92850cce44187e092401fa21f4075f83c7de269","modified":1421763105000},{"_id":"source/media/files/2009/09/27/simizuji-10.jpg","shasum":"1d7fec3cccd8df74dcccfe1497585f838e4021cf","modified":1421763105000},{"_id":"source/media/files/2009/09/27/simizuji-8.jpg","shasum":"bf57b009b03ff04700a0e37a2b8526419853355a","modified":1421763105000},{"_id":"source/media/files/2009/09/27/simizuji-7.jpg","shasum":"507b5b71b81b6d5d9cc2b13856da5589dea6955c","modified":1421763105000},{"_id":"source/media/files/2009/09/27/simizuji-5.jpg","shasum":"0362c7151e15bc8a4935aa1253ebc9504c2e691e","modified":1421763105000},{"_id":"source/media/files/2009/09/27/simizuji-9.jpg","shasum":"f7049232a6f2f8ac13c0f40ad8e64fbb67f8638e","modified":1421763105000},{"_id":"source/media/files/2009/09/27/yasakajinjya-1.jpg","shasum":"74789b70a096a47e837853356e4b2fabca5ad4cc","modified":1421763105000},{"_id":"source/media/files/2009/09/27/yasakajinjya-2.jpg","shasum":"339e5d011af03e93f82d69c632bc895011c0996d","modified":1421763105000},{"_id":"source/media/files/2009/09/27/yasakajinjya-4.jpg","shasum":"b95c5dc37952602923a24dd2714916e58bdf4ee2","modified":1421763105000},{"_id":"source/media/files/2009/09/27/yasakajinjya.jpg","shasum":"1737879ee298d9e0ea749313b6a0a95d540f861b","modified":1421763105000},{"_id":"source/media/files/2009/10/01/deer-01.jpg","shasum":"437749f5aaf860f3f3092e221b27d8b6a915a804","modified":1421763105000},{"_id":"source/media/files/2009/10/01/deer-02.jpg","shasum":"cd1bb3bcfcb2b6df47e5ef5335e71047c737ee85","modified":1421763105000},{"_id":"source/media/files/2009/10/01/deer-03.jpg","shasum":"c7e880b9de63a6e7e6f054d0aef091223ed335f3","modified":1421763105000},{"_id":"source/media/files/2009/10/01/deer-04.jpg","shasum":"79476e7368d605ded5bdb2f1fcaea1f7c67a79fb","modified":1421763105000},{"_id":"source/media/files/2009/10/01/deer-07.jpg","shasum":"a2bd8855adb8ae748e59c6c40b3d91dc99be0bd1","modified":1421763105000},{"_id":"source/media/files/2009/10/01/deer-08.jpg","shasum":"eb70b9735e89eb4017e1f60087a7827adfc969a2","modified":1421763105000},{"_id":"source/media/files/2009/10/01/deer-05.jpg","shasum":"1ced7ac9d4a32a672a7f5c43ad143917a06665e5","modified":1421763105000},{"_id":"source/media/files/2009/10/01/gosoutou-kokuhoukan.jpg","shasum":"a134b6df2bcd3a45db20f9f0e7919cc144005301","modified":1421763105000},{"_id":"source/media/files/2009/10/01/kasugadaisya-01.jpg","shasum":"ae44758d673bbc80d49c658681744b2e053b90e1","modified":1421763105000},{"_id":"source/media/files/2009/10/01/kasugadaisya-03.jpg","shasum":"076f5a6af17f31571baafffa75ba398e80d5cbb5","modified":1421763105000},{"_id":"source/media/files/2009/10/01/kasugadaisya-02.jpg","shasum":"349393333d87ccca485e0466e66042567ad70648","modified":1421763105000},{"_id":"source/media/files/2009/10/01/kasugadaisya-04.jpg","shasum":"a2e912d1bf60055274a7cb581c5fd64dc62e4f9b","modified":1421763105000},{"_id":"source/media/files/2009/10/01/kintetsunara.jpg","shasum":"9c74f995afb6e0ad34e80398cb6fa66a415b6143","modified":1421763105000},{"_id":"source/media/files/2009/10/01/syokubutsuen-02.jpg","shasum":"e47d228bb88600f3af09659bd0a89d356d4bd5b0","modified":1421763105000},{"_id":"source/media/files/2009/10/01/toudaiji-03.jpg","shasum":"68791aa623eb063b8d202482d39caebd63a8d387","modified":1421763105000},{"_id":"source/media/files/2009/10/01/toudaiji-04.jpg","shasum":"dfa4e2efb4c4382e7631d8a0b9bce218df682303","modified":1421763105000},{"_id":"source/media/files/2009/10/01/toudaiji-05.jpg","shasum":"8d924e4318f720b45311be22c60c2ac9f458ac57","modified":1421763105000},{"_id":"source/media/files/2009/10/01/toudaiji-06.jpg","shasum":"63bad61694f405f2f54537e8504f329a7b22e3bb","modified":1421763105000},{"_id":"source/media/files/2009/10/01/toudaiji-07.jpg","shasum":"036479dcd11d523ebeadeab5ebca97df3697679b","modified":1421763105000},{"_id":"source/media/files/2009/10/01/toudaiji-09.jpg","shasum":"601054295b2723d9b4bfc42c63c93247943bcf71","modified":1421763105000},{"_id":"source/media/files/2009/10/21/consato.jpeg","shasum":"de9052d8333e8ef77fd66bbb72ddd2254c37b074","modified":1421763105000},{"_id":"source/media/files/2009/10/21/ginkakuji-1.jpeg","shasum":"a59d4f0e37a43f9732ddfc2176d8dd1d6b7c85e0","modified":1421763105000},{"_id":"source/media/files/2009/10/21/ginkakuji-2.jpeg","shasum":"297d763525271920cb42c6162b48375f2aaa0031","modified":1421763105000},{"_id":"source/media/files/2009/10/01/track.jpg","shasum":"7549df2d0f3ed86612fd00ecc49bfa669ed722c2","modified":1421763105000},{"_id":"source/media/files/2009/10/01/toudaiji-08.jpg","shasum":"f17b72ccf52472fffecb3dde312d3c3e1c6508cf","modified":1421763105000},{"_id":"source/media/files/2009/10/21/hilltop.jpeg","shasum":"3f505aaee816a96480371aa044e78173cf784c80","modified":1421763105000},{"_id":"source/media/files/2009/10/21/kyotoeki-1.jpeg","shasum":"7a51eb4d93e9783c61940cac4c3d7e1da1ae691c","modified":1421763105000},{"_id":"source/media/files/2009/10/21/ginkakuji-4.jpeg","shasum":"0c1683ea9f506ecc9255cc45d7469496b6fbbe1d","modified":1421763105000},{"_id":"source/media/files/2009/10/21/miti-1.jpeg","shasum":"e3b2cc880dcb7e3174f69533b0669567baa3f49d","modified":1421763105000},{"_id":"source/media/files/2009/10/21/miti-3.jpeg","shasum":"733142f5b98109d3dcd13da647f23351fc8a10f5","modified":1421763105000},{"_id":"source/media/files/2009/10/21/momiji-1.jpeg","shasum":"11c40665ecb8ffbcaa7035812fffeb8f9b894017","modified":1421763105000},{"_id":"source/media/files/2009/10/21/tetsugakunomiti-2.jpeg","shasum":"ae7a4a437154afd47025b08fe49fad16dcf8294c","modified":1421763105000},{"_id":"source/media/files/2009/10/21/tetsugakunomiti-3.jpeg","shasum":"5d73ea9f187f97d7d509ff17fefde5d99c755813","modified":1421763105000},{"_id":"source/media/files/2009/10/21/washer.jpeg","shasum":"6d60bf209c11c4e5612c7661c2edd696718ac279","modified":1421763105000},{"_id":"source/media/files/2010/01/14/disney-sea-02.jpg","shasum":"c4f1c57ea5e608db5b796adfa450186135fb4948","modified":1421763105000},{"_id":"source/media/files/2010/01/14/disney-sea-05.jpg","shasum":"55cb2b4c6115ec806c180894e17983016ad5ea18","modified":1421763105000},{"_id":"source/media/files/2010/01/14/disney-sea-03.jpg","shasum":"9d931c79d71655040b49094494868fa99cea7a95","modified":1421763105000},{"_id":"source/media/files/2010/01/14/disney-sea-08.jpg","shasum":"72eadf5c8ab52e0ccf4370011f866b7bd7463b5e","modified":1421763105000},{"_id":"source/media/files/2010/01/14/disney-sea-07.jpg","shasum":"f7a8448321c40a048a9d0d8d41417ca34385036a","modified":1421763105000},{"_id":"source/media/files/2010/01/14/disney-sea-06.jpg","shasum":"aaeb563075600ea43cda8a210eee02e7c786b1b7","modified":1421763105000},{"_id":"source/media/files/2010/01/14/disney-sea-09.jpg","shasum":"3a85780c7ac4bc242aea564736e0f9c27562e275","modified":1421763105000},{"_id":"source/media/files/2010/01/14/disney-sea-12.jpg","shasum":"1c9b83a083fc4f318d4066ae5b1115fc1c5ff466","modified":1421763105000},{"_id":"source/media/files/2010/01/14/disney-sea-14.jpg","shasum":"9742d3fb7ecf25135bed48af7e8c5bd1d4029982","modified":1421763105000},{"_id":"source/media/files/2010/10/19/track.jpg","shasum":"3215ec3bd20f269e1675189a49359a49567c7690","modified":1421763105000},{"_id":"source/media/files/2011/03/07/kindle.jpg","shasum":"bbd5eb697c86bfa0d041da73bded727accc94430","modified":1421763105000},{"_id":"source/media/files/2011/03/19/skewer-icecream.jpg","shasum":"e097bdaf40381fa3762597e5ef3947b61d306539","modified":1421763105000},{"_id":"source/media/files/2009/10/21/sansu.jpeg","shasum":"f6b3c8794fdc1a7daf8b7dcdee9b0e20ec0d850a","modified":1421763105000},{"_id":"source/media/files/2011/11/29/SDIM0055.jpg","shasum":"583c280b5426e7f961da8f68d8cff6e6c2ce0908","modified":1421763105000},{"_id":"source/media/files/2011/11/29/SDIM0061.jpg","shasum":"9fcf71d13d2bd086628d5110e5fc35a0281cd97e","modified":1421763105000},{"_id":"source/media/files/2011/12/24/SDIM0099.jpg","shasum":"3628e3dab00bf552ee1a1a980cd041e893232ff3","modified":1421763105000},{"_id":"source/media/files/2011/12/24/SDIM0254.jpg","shasum":"c283c480c2c92275cfbaa90a987d9ebdcde5e448","modified":1421763105000},{"_id":"source/media/files/2011/11/29/SDIM0029.jpg","shasum":"ca267bda0b4333fc633ccf358d3306ebc7c04fb1","modified":1421763105000},{"_id":"source/media/files/2012/06/08/rose-1.jpg","shasum":"5b6e430dcaef33d9632cf36aeb8cf19ff6cb5ab4","modified":1421763105000},{"_id":"source/media/files/2012/06/08/rose-2.jpg","shasum":"b4e42f1b4ef30a2088397b4d3363503fbecc2011","modified":1421763105000},{"_id":"source/media/files/2012/06/08/rose-4.jpg","shasum":"d7ab6ce54f0e20ab4a02a61be48347c06c5f1fdb","modified":1421763105000},{"_id":"source/media/files/2012/06/08/rose-6.jpg","shasum":"66ae99d1f6f60d4ccaf1d0b904fe08e865b6a626","modified":1421763105000},{"_id":"source/media/files/2013/02/17/pebble.jpg","shasum":"9e849105bad1007e2c9a441450339d370f788d3a","modified":1421763105000},{"_id":"source/media/files/2012/09/29/stay-still.jpg","shasum":"fa32c742dd731334cbbd89b25a2310751f22a71a","modified":1421763105000},{"_id":"source/media/files/2014/07/qurom.png","shasum":"2d482848b3d56b891cc0e6b4deda6f5c958c8344","modified":1427237250000},{"_id":"source/media/files/2014/08/follower2.png","shasum":"7e3e816593d06e671f0c2ade38eb1bbeb7e302ed","modified":1427237613000},{"_id":"source/media/files/2014/07/basicPaxos.png","shasum":"95db69df27fcedc54b90cc8c93485e2ad2001254","modified":1427237258000},{"_id":"source/media/files/2014/08/follower3.png","shasum":"f1a42cef6fa5a7d782b7e7a36774cf56d293b93a","modified":1427237620000},{"_id":"source/media/files/2015/06/init.jpg","shasum":"45167ec68e557cebbe85abcb48b6a31bd55392b9","modified":1435311288000},{"_id":"source/media/files/2015/06/malloc.jpg","shasum":"12e1f0075a011c75627e7811e2dc97423865895d","modified":1435312530000},{"_id":"source/media/files/2015/06/smallandbig.jpg","shasum":"ca61638c8a3e44302fe2951325332c6d3071a154","modified":1435308761000},{"_id":"source/media/files/2015/06/treelevel.jpg","shasum":"53cf96549c433ccf20847c018e85ec4a5d60053e","modified":1435309700000},{"_id":"source/media/files/2015/06/sweep.jpg","shasum":"be0f6531d583b9822b234beb5745117cc328bd95","modified":1435312566000},{"_id":"source/media/files/2015/10/1.png","shasum":"954ced2b0c906de76951d59a4453a1b955af8d2c","modified":1444470076000},{"_id":"source/media/files/2015/10/3.png","shasum":"701704cb105a1574480b5872a25323bb8907aa6a","modified":1444470170000},{"_id":"source/media/files/2015/10/4.png","shasum":"60d37f7e16f88bdd4404aeb905f52338bfd5a5bf","modified":1444470179000},{"_id":"source/media/files/2015/10/6.png","shasum":"49fbac266342eb5b2a4b2ab69f005adf89814fee","modified":1444470191000},{"_id":"source/media/files/2015/10/7.png","shasum":"28e54113284db18d66516611940af7f3b988b7aa","modified":1444470197000},{"_id":"source/media/files/2015/10/9.png","shasum":"3c1596154ce177f49ee9f64b9e6f7cb9525c0dc2","modified":1444470209000},{"_id":"source/media/files/2009/10/21/momiji-2.jpeg","shasum":"68e7b5f3957f0322ee7d05f66c92ccc507eb62b9","modified":1421763105000},{"_id":"source/media/files/2010/01/14/disney-sea-04.jpg","shasum":"643b48629fc16d8b784d8159471ad47b3064b82a","modified":1421763105000},{"_id":"source/media/files/2010/01/14/disney-sea-11.jpg","shasum":"83991d96c457e2453c2ff09771e6e9e7422a09bb","modified":1421763105000},{"_id":"source/media/files/2010/01/14/disney-sea-13.jpg","shasum":"8709361a79037303ebde97e3b59c5e2d341dc9a7","modified":1421763105000},{"_id":"source/media/files/2011/03/19/nanba.jpg","shasum":"3ec368b19c47e3220c923d974bb93a23a32b8b7e","modified":1421763105000},{"_id":"source/media/files/2011/03/19/lucky-god.jpg","shasum":"ff59c8fcddd971cac2c611949221a726d1beb9b9","modified":1421763105000},{"_id":"source/media/files/2011/03/28/compass.jpg","shasum":"adb4fe3bbd5561f7ce1aa7d1a697bc839de1850d","modified":1421763105000},{"_id":"source/media/files/2011/03/19/skewer.jpg","shasum":"ec1a486891c872ca53113ffbedf7bd6e79fbc6a1","modified":1421763105000},{"_id":"source/media/files/2011/10/06/RIPSteveJobs.png","shasum":"0b35706994963b639747e20b047dac269f251cdf","modified":1421763105000},{"_id":"source/media/files/2011/12/24/SDIM0160.jpg","shasum":"d93293caf2bdb8690c5dc2f112254a38209c4790","modified":1421763105000},{"_id":"source/media/files/2011/12/24/SDIM0260.jpg","shasum":"0236999bc8f1f9ecbe81fa7469a12fe41bc4af41","modified":1421763105000},{"_id":"source/media/files/2011/12/24/SDIM0289.jpg","shasum":"14e6ded32e552d78f61484d2a36e8c81ecbd555e","modified":1421763105000},{"_id":"source/media/files/2011/12/24/SDIM0316.jpg","shasum":"1abfca55019a44636148713f868cfd470ea79295","modified":1421763105000},{"_id":"source/media/files/2012/04/16/sakura_01.jpg","shasum":"05b7b286374c18a0c15e264fc9c8d633d7012217","modified":1421763105000},{"_id":"source/media/files/2012/05/31/river.jpg","shasum":"c654767a14a386e7760fb36a1f3d74edfcb2fe9f","modified":1421763105000},{"_id":"source/media/files/2012/06/08/rose-3.jpg","shasum":"a38520fca6e3e90607db81394073c44c6dea8f71","modified":1421763105000},{"_id":"source/media/files/2012/06/08/rose-5.jpg","shasum":"ee4607b23043ea33a1f45af3acca1b603524ff65","modified":1421763105000},{"_id":"source/media/files/2012/01/24/shot-1.jpg","shasum":"ace73fb2799cf4fe7b3518b4955d1d424ece41d6","modified":1421763105000},{"_id":"source/media/files/2012/08/07/waterfall-in-house.jpg","shasum":"1bb3a7718b169093261f593b6c9df0b275b8a20f","modified":1421763105000},{"_id":"source/media/files/2013/05/28/sunset.jpg","shasum":"fb56f335c1e02eb6b810da62350c5cb89c662f9e","modified":1421763105000},{"_id":"source/media/files/2013/11/05/love-is-being-stupid-together.jpg","shasum":"b61c27a419b4b932ca6220248cdb0a892b7ec063","modified":1421763105000},{"_id":"source/media/files/2013/08/12/kaminarimon.jpg","shasum":"a044da0d59f04f09c168518faae309a348d76442","modified":1421763105000},{"_id":"source/media/files/2015/09/facebookcache.jpg","shasum":"c544c8e09776eac34beb6b6810602b86972b3de9","modified":1443066813000},{"_id":"source/media/files/2011/12/24/SDIM0259.jpg","shasum":"e9aafc53c56c53fdc9aac84bc4c3f82d7cbce3f7","modified":1421763105000},{"_id":"source/media/files/2012/09/10/skytree.jpg","shasum":"902e20c8a7ecf5ed38fddfbe898b16d70fb7c906","modified":1421763105000},{"_id":"source/media/files/2013/04/02/sakura.jpg","shasum":"2c74b8f0cc7d06cd71b54b56bbf9232a5075f398","modified":1421763105000},{"_id":"source/media/files/2014/03/29/alone.jpg","shasum":"ef55f544fa2ffa9189e5413bb26a7e943b5d4023","modified":1421763105000},{"_id":"source/media/files/2014/07/02/ibanez.jpg","shasum":"7ebd14f7c552777d4d4ad1b87fabc63b7fb3d64c","modified":1421763105000},{"_id":"source/media/files/2014/07/24/ladder.jpg","shasum":"1205648180ed5d304c2d47b034693729e608f477","modified":1421763105000},{"_id":"source/media/files/2015/09/storagearch.jpg","shasum":"5956720358ef71780ace834ef57f2700473b601b","modified":1443324229000},{"_id":"source/media/files/2012/04/16/sakura_02.jpg","shasum":"f12af956652a91f806bb66211da3342397b2d5f5","modified":1421763105000},{"_id":"source/media/files/2014/04/07/white-mountains.jpg","shasum":"6a22fbac2a5ecec789d5e00f5fe74cf202cc848c","modified":1421763105000},{"_id":"source/media/files/2014/12/18/yamdroktso.jpg","shasum":"0018bd17bf97f238869468c28fba0e9c0ebc413f","modified":1421763105000},{"_id":"source/media/files/2012/08/07/farmers.jpg","shasum":"209b6816136ea8d10e88e695cc66124f084dd4f6","modified":1421763105000},{"_id":"source/media/files/2014/06/12/cherryblossom.jpg","shasum":"bd6d89f271ccef0556f1d8f8508aa87b263b4f69","modified":1421763105000},{"_id":"source/media/files/2014/03/17/lost.jpg","shasum":"92314b3888b5695f3cc0afccd5e6138fa071a3ed","modified":1421763105000},{"_id":"source/media/files/2014/09/05/bridge-to-wonderland.jpg","shasum":"1c74cae14dd4acefd6a3efcfaa49aa223341bbe1","modified":1421763105000},{"_id":"source/media/files/2012/05/16/sakura.jpg","shasum":"95e0fc5a8d0a3c6ad931b0dcf3afa468d30d594d","modified":1421763105000},{"_id":"source/media/files/2012/08/07/lotus.jpg","shasum":"34e3028853c24db8056aa1cfc8006c808353e0a6","modified":1421763105000},{"_id":"source/media/files/2014/01/23/untitled.jpg","shasum":"0d3bf3ce687e1aaf3a99fe3fb60e5635615f4cfb","modified":1421763105000},{"_id":"source/media/files/2014/02/20/arashiyama.jpg","shasum":"7e58ac876ca6710bcea50191c9f9c11a48c84726","modified":1421763105000},{"_id":"source/media/files/2013/12/19/cat.jpg","shasum":"2a145bf34074e4adc495ce82f4b3092d2042e077","modified":1421763105000},{"_id":"source/media/files/2014/01/29/HiroshimaPeaceMemorial.jpg","shasum":"a3121a88d5029b0f31dfb3c78b5018ab723ec02d","modified":1421763105000},{"_id":"source/media/files/2014/02/28/yamadera.jpg","shasum":"bb2e48850868afbb93128d4062815fa335c54941","modified":1421763105000},{"_id":"source/media/files/2012/12/04/girl-in-the-sun.jpg","shasum":"a32b4c8b11a9b7f60719eaa667f89bf50fa62704","modified":1421763105000},{"_id":"source/media/files/2014/12/18/yamdroktso@2x.jpg","shasum":"30d14c00c62ce106caaa2cf9a6785be8d6a4c286","modified":1421763105000},{"_id":"public/js/totop.js","modified":1446881401119,"shasum":"f6fdc3d999e8d03aa55d4c5aabfcb1b2bdf0b4c6"},{"_id":"public/js/jquery.min.js","modified":1446881401133,"shasum":"41b4bfbaa96be6d1440db6e78004ade1c134e276"},{"_id":"public/js/fancybox.pack.js","modified":1446881401135,"shasum":"53360764b429c212f424399384417ccc233bb3be"},{"_id":"public/fonts/icomoon.woff","modified":1446881401143,"shasum":"5f0e31266d6dd7a6d459d583599287e9539f1da8"},{"_id":"public/fonts/icomoon.ttf","modified":1446881401147,"shasum":"42fbffaef5b1224ce8d8349a5c7c3d7f7ecbb8ae"},{"_id":"public/fonts/icomoon.svg","modified":1446881401149,"shasum":"54679013c08696615e3f83e2cf86bf02e17f1deb"},{"_id":"public/fonts/icomoon.eot","modified":1446881401153,"shasum":"6d4fddd9bae85803d2cf483700e9e46b630dfc63"},{"_id":"public/fancybox/fancybox_sprite@2x.png","modified":1446881401154,"shasum":"30c58913f327e28f466a00f4c1ac8001b560aed8"},{"_id":"public/fancybox/fancybox_sprite.png","modified":1446881401158,"shasum":"17df19f97628e77be09c352bf27425faea248251"},{"_id":"public/fancybox/fancybox_overlay.png","modified":1446881401160,"shasum":"b3a4ee645ba494f52840ef8412015ba0f465dbe0"},{"_id":"public/fancybox/blank.gif","modified":1446881401162,"shasum":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a"},{"_id":"public/css/style.css","modified":1446881401182,"shasum":"13504e0bc60ff9ae8b5ecbf395c8f80594603958"},{"_id":"public/css/pure-min.css","modified":1446881401199,"shasum":"8cf7ea3e9e00e752de63fbc443e9300366327cd9"},{"_id":"public/css/normalize.css","modified":1446881401200,"shasum":"02fe53286d071637534d5aa2c57c76c168c0d521"},{"_id":"public/css/jquery.fancybox.css","modified":1446881401202,"shasum":"3e5850dfd0ac87fe34a5c680d50f300536542bae"},{"_id":"public/css/grids-responsive-min.css","modified":1446881401203,"shasum":"703826508193cbe21f2745d3e837256e224eb512"},{"_id":"public/media/fonts/telex-regular-webfont.woff","modified":1446881401205,"shasum":"679bd5e953d7fd46e20cd2b59586ec71af15e733"},{"_id":"public/media/fonts/telex-regular-webfont.ttf","modified":1446881401208,"shasum":"b3cda4ac36a67ae3d4b69f24450dfd62d7e91f75"},{"_id":"public/media/fonts/telex-regular-webfont.svg","modified":1446881401210,"shasum":"eeab91723e2433eebf23bc70d0d59e1dd47e44b3"},{"_id":"public/media/fonts/telex-regular-webfont.eot","modified":1446881401213,"shasum":"a494c8afb09b3b8c5139bf9c3b9998616b01652f"},{"_id":"public/media/files/2015/10/9.png","modified":1446881401215,"shasum":"3c1596154ce177f49ee9f64b9e6f7cb9525c0dc2"},{"_id":"public/media/files/2015/10/8.png","modified":1446881401219,"shasum":"e5073bff165220fe49d482cfff6183e6c596bc7c"},{"_id":"public/media/files/2015/10/7.png","modified":1446881401223,"shasum":"28e54113284db18d66516611940af7f3b988b7aa"},{"_id":"public/media/files/2015/10/6.png","modified":1446881401228,"shasum":"49fbac266342eb5b2a4b2ab69f005adf89814fee"},{"_id":"public/media/files/2015/10/5.png","modified":1446881401236,"shasum":"3b4f92f83bfd52503a79e6a6af0f9455cb533118"},{"_id":"public/media/files/2015/10/4.png","modified":1446881401247,"shasum":"60d37f7e16f88bdd4404aeb905f52338bfd5a5bf"},{"_id":"public/media/files/2015/10/3.png","modified":1446881401253,"shasum":"701704cb105a1574480b5872a25323bb8907aa6a"},{"_id":"public/media/files/2015/10/2.png","modified":1446881401256,"shasum":"f7fd3980b413b3e7bb809597e710d38e3e4702fe"},{"_id":"public/media/files/2015/10/13.png","modified":1446881401259,"shasum":"14d5e29b554c77083301f07f34abf67600d27066"},{"_id":"public/media/files/2015/10/12.png","modified":1446881401263,"shasum":"af09872f581d0f1524839120f6065b03ec79222e"},{"_id":"public/media/files/2015/10/11.png","modified":1446881401266,"shasum":"724b883980905773eb0a25fd18f2cf7995e837e1"},{"_id":"public/media/files/2015/10/10.png","modified":1446881401269,"shasum":"0231d25aeb3d57275df8e0f261273a41d1274dc3"},{"_id":"public/media/files/2015/10/1.png","modified":1446881401273,"shasum":"954ced2b0c906de76951d59a4453a1b955af8d2c"},{"_id":"public/media/files/2015/09/storagearch.jpg","modified":1446881401278,"shasum":"5956720358ef71780ace834ef57f2700473b601b"},{"_id":"public/media/files/2015/09/facebookcache.jpg","modified":1446881401291,"shasum":"c544c8e09776eac34beb6b6810602b86972b3de9"},{"_id":"public/media/files/2015/06/treelevel.jpg","modified":1446881401299,"shasum":"53cf96549c433ccf20847c018e85ec4a5d60053e"},{"_id":"public/media/files/2015/06/sweep.jpg","modified":1446881401303,"shasum":"be0f6531d583b9822b234beb5745117cc328bd95"},{"_id":"public/media/files/2015/06/span.jpg","modified":1446881401305,"shasum":"c9dbee57e10c31ccac5b40fa9bd6421fb01dc9d9"},{"_id":"public/media/files/2015/06/smallandbig.jpg","modified":1446881401315,"shasum":"ca61638c8a3e44302fe2951325332c6d3071a154"},{"_id":"public/media/files/2015/06/sizeclass.jpg","modified":1446881401319,"shasum":"097a622b13586675e6e571a1cf36a28cc348348e"},{"_id":"public/media/files/2015/06/malloc.jpg","modified":1446881401323,"shasum":"12e1f0075a011c75627e7811e2dc97423865895d"},{"_id":"public/media/files/2015/06/init.jpg","modified":1446881401325,"shasum":"45167ec68e557cebbe85abcb48b6a31bd55392b9"},{"_id":"public/media/files/2014/12/18/yamdroktso@2x.jpg","modified":1446881401334,"shasum":"30d14c00c62ce106caaa2cf9a6785be8d6a4c286"},{"_id":"public/media/files/2014/12/18/yamdroktso.jpg","modified":1446881401345,"shasum":"0018bd17bf97f238869468c28fba0e9c0ebc413f"},{"_id":"public/media/files/2014/09/system-of-3-300x138.png","modified":1446881401352,"shasum":"204e21bd933078550c796a5b90df87778693a635"},{"_id":"public/media/files/2014/09/system-of-2-300x87.png","modified":1446881401368,"shasum":"8fb9b36a7d4912d493b8f36e1c2350ad52efdae1"},{"_id":"public/media/files/2014/09/replication-sync-300x274.png","modified":1446881401371,"shasum":"e2180cb75fed58b447633e19854d7be095497190"},{"_id":"public/media/files/2014/09/replication-both-300x206.png","modified":1446881401373,"shasum":"1f4121430d62cd502de63955de64b6edf41db17d"},{"_id":"public/media/files/2014/09/replication-async-300x263.png","modified":1446881401376,"shasum":"ad11fa4afb4aac6c6347286063da8e7c7e62c167"},{"_id":"public/media/files/2014/09/google-transact09.png","modified":1446881401379,"shasum":"3ba8f07e2e92834c44beacbb1a9445440ed6b680"},{"_id":"public/media/files/2014/09/epoch.png","modified":1446881401381,"shasum":"1450c815f72dbfa89efecf89151a18fa8e1629c6"},{"_id":"public/media/files/2014/09/24/syscall.jpg","modified":1446881401384,"shasum":"2e276fca5fda66e4e8c5237212d614b9964bdc97"},{"_id":"public/media/files/2014/09/24/steal.jpg","modified":1446881401387,"shasum":"2e735f25a3358d492aa8df6457801da96329c5db"},{"_id":"public/media/files/2014/09/24/our-cast.jpg","modified":1446881401391,"shasum":"f0721b3dbef5428c10afbeb6cf686fcfda089a92"},{"_id":"public/media/files/2014/09/24/in-motion.jpg","modified":1446881401393,"shasum":"21ab4f451345f0407b17b3486901f708ff0c37f2"},{"_id":"public/media/files/2014/09/24/goroutinestate.jpg","modified":1446881401395,"shasum":"07f60252144c61b2ba4789c9aa167dad6a4487d1"},{"_id":"public/media/files/2014/09/05/bridge-to-wonderland.jpg","modified":1446881401403,"shasum":"1c74cae14dd4acefd6a3efcfaa49aa223341bbe1"},{"_id":"public/media/files/2014/08/statechange.png","modified":1446881401406,"shasum":"174b5530f210b9e2ec21cd1e866c12be9b2f46cf"},{"_id":"public/media/files/2014/08/multileader.png","modified":1446881401409,"shasum":"a067c1a6c1ddfd0818621d2f10319f154a55f3bb"},{"_id":"public/media/files/2014/08/master.png","modified":1446881401415,"shasum":"f8b45841b8bf8d1dbfe5289dedf4f754d26bf6e2"},{"_id":"public/media/files/2014/08/leaderfailover.png","modified":1446881401418,"shasum":"bfe73d12d91081b39346d1e7b3c849e3da200605"},{"_id":"public/media/files/2014/08/followerfailover.png","modified":1446881401420,"shasum":"cc0c6e1b0322448424ed91547a257052ceb3c99a"},{"_id":"public/media/files/2014/08/follower3.png","modified":1446881401436,"shasum":"f1a42cef6fa5a7d782b7e7a36774cf56d293b93a"},{"_id":"public/media/files/2014/08/follower2.png","modified":1446881401438,"shasum":"7e3e816593d06e671f0c2ade38eb1bbeb7e302ed"},{"_id":"public/media/files/2014/08/follower1.png","modified":1446881401439,"shasum":"e4afbc6dbbd6b637222141da377d7b82d0756460"},{"_id":"public/media/files/2014/07/qurom.png","modified":1446881401443,"shasum":"2d482848b3d56b891cc0e6b4deda6f5c958c8344"},{"_id":"public/media/files/2014/07/basicPaxos.png","modified":1446881401451,"shasum":"95db69df27fcedc54b90cc8c93485e2ad2001254"},{"_id":"public/media/files/2014/07/Primary.png","modified":1446881401453,"shasum":"f26fbb9422fce2aca617a41cdd4a494e17048de3"},{"_id":"public/media/files/2014/07/OPaOPb.png","modified":1446881401455,"shasum":"89684663a911d957c56656489c231d50c533cef6"},{"_id":"public/media/files/2014/07/3.png","modified":1446881401457,"shasum":"28beea925d60c4166b330f0b88534f512288e9dc"},{"_id":"public/media/files/2014/07/24/ladder.jpg","modified":1446881401459,"shasum":"1205648180ed5d304c2d47b034693729e608f477"},{"_id":"public/media/files/2014/07/02/ibanez.jpg","modified":1446881401462,"shasum":"7ebd14f7c552777d4d4ad1b87fabc63b7fb3d64c"},{"_id":"public/media/files/2014/06/12/cherryblossom.jpg","modified":1446881401473,"shasum":"bd6d89f271ccef0556f1d8f8508aa87b263b4f69"},{"_id":"public/media/files/2014/04/07/white-mountains.jpg","modified":1446881401480,"shasum":"6a22fbac2a5ecec789d5e00f5fe74cf202cc848c"},{"_id":"public/media/files/2014/03/29/alone.jpg","modified":1446881401498,"shasum":"ef55f544fa2ffa9189e5413bb26a7e943b5d4023"},{"_id":"public/media/files/2014/03/17/lost.jpg","modified":1446881401502,"shasum":"92314b3888b5695f3cc0afccd5e6138fa071a3ed"},{"_id":"public/media/files/2014/02/28/yamadera.jpg","modified":1446881401508,"shasum":"bb2e48850868afbb93128d4062815fa335c54941"},{"_id":"public/media/files/2014/02/20/arashiyama.jpg","modified":1446881401513,"shasum":"7e58ac876ca6710bcea50191c9f9c11a48c84726"},{"_id":"public/media/files/2014/01/29/HiroshimaPeaceMemorial.jpg","modified":1446881401519,"shasum":"a3121a88d5029b0f31dfb3c78b5018ab723ec02d"},{"_id":"public/media/files/2014/01/23/untitled.jpg","modified":1446881401524,"shasum":"0d3bf3ce687e1aaf3a99fe3fb60e5635615f4cfb"},{"_id":"public/media/files/2013/12/19/cat.jpg","modified":1446881401528,"shasum":"2a145bf34074e4adc495ce82f4b3092d2042e077"},{"_id":"public/media/files/2013/11/05/love-is-being-stupid-together.jpg","modified":1446881401536,"shasum":"b61c27a419b4b932ca6220248cdb0a892b7ec063"},{"_id":"public/media/files/2013/08/12/kaminarimon.jpg","modified":1446881401540,"shasum":"a044da0d59f04f09c168518faae309a348d76442"},{"_id":"public/media/files/2013/05/28/sunset.jpg","modified":1446881401545,"shasum":"fb56f335c1e02eb6b810da62350c5cb89c662f9e"},{"_id":"public/media/files/2013/04/02/sakura.jpg","modified":1446881401550,"shasum":"2c74b8f0cc7d06cd71b54b56bbf9232a5075f398"},{"_id":"public/media/files/2013/02/17/pebble.jpg","modified":1446881401562,"shasum":"9e849105bad1007e2c9a441450339d370f788d3a"},{"_id":"public/media/files/2012/12/04/girl-in-the-sun.jpg","modified":1446881401570,"shasum":"a32b4c8b11a9b7f60719eaa667f89bf50fa62704"},{"_id":"public/media/files/2012/09/29/stay-still.jpg","modified":1446881401575,"shasum":"fa32c742dd731334cbbd89b25a2310751f22a71a"},{"_id":"public/media/files/2012/09/10/skytree.jpg","modified":1446881401578,"shasum":"902e20c8a7ecf5ed38fddfbe898b16d70fb7c906"},{"_id":"public/media/files/2012/08/07/waterfall-in-house.jpg","modified":1446881401581,"shasum":"1bb3a7718b169093261f593b6c9df0b275b8a20f"},{"_id":"public/media/files/2012/08/07/lotus.jpg","modified":1446881401587,"shasum":"34e3028853c24db8056aa1cfc8006c808353e0a6"},{"_id":"public/media/files/2012/08/07/farmers.jpg","modified":1446881401592,"shasum":"209b6816136ea8d10e88e695cc66124f084dd4f6"},{"_id":"public/media/files/2012/06/08/rose-6.jpg","modified":1446881401594,"shasum":"66ae99d1f6f60d4ccaf1d0b904fe08e865b6a626"},{"_id":"public/media/files/2012/06/08/rose-5.jpg","modified":1446881401596,"shasum":"ee4607b23043ea33a1f45af3acca1b603524ff65"},{"_id":"public/media/files/2012/06/08/rose-4.jpg","modified":1446881401602,"shasum":"d7ab6ce54f0e20ab4a02a61be48347c06c5f1fdb"},{"_id":"public/media/files/2012/06/08/rose-3.jpg","modified":1446881401605,"shasum":"a38520fca6e3e90607db81394073c44c6dea8f71"},{"_id":"public/media/files/2012/06/08/rose-2.jpg","modified":1446881401608,"shasum":"b4e42f1b4ef30a2088397b4d3363503fbecc2011"},{"_id":"public/media/files/2012/06/08/rose-1.jpg","modified":1446881401614,"shasum":"5b6e430dcaef33d9632cf36aeb8cf19ff6cb5ab4"},{"_id":"public/media/files/2012/05/31/river.jpg","modified":1446881401618,"shasum":"c654767a14a386e7760fb36a1f3d74edfcb2fe9f"},{"_id":"public/media/files/2012/05/16/sakura.jpg","modified":1446881401628,"shasum":"95e0fc5a8d0a3c6ad931b0dcf3afa468d30d594d"},{"_id":"public/media/files/2012/04/16/sakura_02.jpg","modified":1446881401635,"shasum":"f12af956652a91f806bb66211da3342397b2d5f5"},{"_id":"public/media/files/2012/04/16/sakura_01.jpg","modified":1446881401639,"shasum":"05b7b286374c18a0c15e264fc9c8d633d7012217"},{"_id":"public/media/files/2012/04/16/leaf.jpg","modified":1446881401641,"shasum":"4eab18e4a4cb0dacdcc70d807dcdcfa2ec52132c"},{"_id":"public/media/files/2012/01/24/shot-1.jpg","modified":1446881401644,"shasum":"ace73fb2799cf4fe7b3518b4955d1d424ece41d6"},{"_id":"public/media/files/2011/12/24/SDIM0316.jpg","modified":1446881401647,"shasum":"1abfca55019a44636148713f868cfd470ea79295"},{"_id":"public/media/files/2011/12/24/SDIM0289.jpg","modified":1446881401650,"shasum":"14e6ded32e552d78f61484d2a36e8c81ecbd555e"},{"_id":"public/media/files/2011/12/24/SDIM0260.jpg","modified":1446881401655,"shasum":"0236999bc8f1f9ecbe81fa7469a12fe41bc4af41"},{"_id":"public/media/files/2011/12/24/SDIM0259.jpg","modified":1446881401658,"shasum":"e9aafc53c56c53fdc9aac84bc4c3f82d7cbce3f7"},{"_id":"public/media/files/2011/12/24/SDIM0254.jpg","modified":1446881401661,"shasum":"c283c480c2c92275cfbaa90a987d9ebdcde5e448"},{"_id":"public/media/files/2011/12/24/SDIM0160.jpg","modified":1446881401674,"shasum":"d93293caf2bdb8690c5dc2f112254a38209c4790"},{"_id":"public/media/files/2011/12/24/SDIM0099.jpg","modified":1446881401683,"shasum":"3628e3dab00bf552ee1a1a980cd041e893232ff3"},{"_id":"public/media/files/2011/11/29/SDIM0087.jpg","modified":1446881401687,"shasum":"97e942382d1ad5501d03fdcf478d4bfca5d6edf2"},{"_id":"public/media/files/2011/11/29/SDIM0061.jpg","modified":1446881401689,"shasum":"9fcf71d13d2bd086628d5110e5fc35a0281cd97e"},{"_id":"public/media/files/2011/11/29/SDIM0055.jpg","modified":1446881401693,"shasum":"583c280b5426e7f961da8f68d8cff6e6c2ce0908"},{"_id":"public/media/files/2011/11/29/SDIM0029.jpg","modified":1446881401696,"shasum":"ca267bda0b4333fc633ccf358d3306ebc7c04fb1"},{"_id":"public/media/files/2011/11/22/with-description.png","modified":1446881401698,"shasum":"2a5885547502ae4c22ef489f7a6c89bc03bd9bdb"},{"_id":"public/media/files/2011/11/22/with-analytics.png","modified":1446881401700,"shasum":"59438dc798c1608e5b2ce101a7badb6dd97eadd9"},{"_id":"public/media/files/2011/11/22/bug.png","modified":1446881401703,"shasum":"f5df6ecf75ac853ce315af0b71228478ad9787f1"},{"_id":"public/media/files/2011/11/22/UIImageView-and-userInteractionEnabled.dot","modified":1446881401708,"shasum":"1f80365fb0072c7b5faecee396a215be9bca0ff1"},{"_id":"public/media/files/2011/10/06/RIPSteveJobs.png","modified":1446881401724,"shasum":"0b35706994963b639747e20b047dac269f251cdf"},{"_id":"public/media/files/2011/03/28/compass.jpg","modified":1446881401734,"shasum":"adb4fe3bbd5561f7ce1aa7d1a697bc839de1850d"},{"_id":"public/media/files/2011/03/19/skewer.jpg","modified":1446881401738,"shasum":"ec1a486891c872ca53113ffbedf7bd6e79fbc6a1"},{"_id":"public/media/files/2011/03/19/skewer-icecream.jpg","modified":1446881401743,"shasum":"e097bdaf40381fa3762597e5ef3947b61d306539"},{"_id":"public/media/files/2011/03/19/nanba.jpg","modified":1446881401746,"shasum":"3ec368b19c47e3220c923d974bb93a23a32b8b7e"},{"_id":"public/media/files/2011/03/19/lucky-god.jpg","modified":1446881401749,"shasum":"ff59c8fcddd971cac2c611949221a726d1beb9b9"},{"_id":"public/media/files/2011/03/07/kindle.jpg","modified":1446881401752,"shasum":"bbd5eb697c86bfa0d041da73bded727accc94430"},{"_id":"public/media/files/2011/02/24/wp2txt.xsl","modified":1446881401756,"shasum":"35a1de98c3d22076eb7433e776c5d626acd300b5"},{"_id":"public/media/files/2010/10/29/rain.jpg","modified":1446881401758,"shasum":"e9ec458a0d749431bf75366eec41071beecf0273"},{"_id":"public/media/files/2010/10/19/track.jpg","modified":1446881401784,"shasum":"3215ec3bd20f269e1675189a49359a49567c7690"},{"_id":"public/media/files/2010/08/17/sunrise.jpg","modified":1446881401787,"shasum":"cf3df516236f76fef9731234e198d3b23dbfbece"},{"_id":"public/media/files/2010/06/30/box.jpg","modified":1446881401790,"shasum":"ba00135b909da4955adb72ee863da8f08b5070fb"},{"_id":"public/media/files/2010/01/14/disney-sea-15.jpg","modified":1446881401797,"shasum":"b6dee93fa144733cc518e974bcd40fb26351068e"},{"_id":"public/media/files/2010/01/14/disney-sea-14.jpg","modified":1446881401804,"shasum":"9742d3fb7ecf25135bed48af7e8c5bd1d4029982"},{"_id":"public/media/files/2010/01/14/disney-sea-13.jpg","modified":1446881401807,"shasum":"8709361a79037303ebde97e3b59c5e2d341dc9a7"},{"_id":"public/media/files/2010/01/14/disney-sea-12.jpg","modified":1446881401811,"shasum":"1c9b83a083fc4f318d4066ae5b1115fc1c5ff466"},{"_id":"public/media/files/2010/01/14/disney-sea-11.jpg","modified":1446881401815,"shasum":"83991d96c457e2453c2ff09771e6e9e7422a09bb"},{"_id":"public/media/files/2010/01/14/disney-sea-10.jpg","modified":1446881401820,"shasum":"99f94a2e27ad55c67a02e5977deed25ce6c61980"},{"_id":"public/media/files/2010/01/14/disney-sea-09.jpg","modified":1446881401822,"shasum":"3a85780c7ac4bc242aea564736e0f9c27562e275"},{"_id":"public/media/files/2010/01/14/disney-sea-08.jpg","modified":1446881401826,"shasum":"72eadf5c8ab52e0ccf4370011f866b7bd7463b5e"},{"_id":"public/media/files/2010/01/14/disney-sea-07.jpg","modified":1446881401834,"shasum":"f7a8448321c40a048a9d0d8d41417ca34385036a"},{"_id":"public/media/files/2010/01/14/disney-sea-06.jpg","modified":1446881401837,"shasum":"aaeb563075600ea43cda8a210eee02e7c786b1b7"},{"_id":"public/media/files/2010/01/14/disney-sea-05.jpg","modified":1446881401839,"shasum":"55cb2b4c6115ec806c180894e17983016ad5ea18"},{"_id":"public/media/files/2010/01/14/disney-sea-04.jpg","modified":1446881401843,"shasum":"643b48629fc16d8b784d8159471ad47b3064b82a"},{"_id":"public/media/files/2010/01/14/disney-sea-03.jpg","modified":1446881401850,"shasum":"9d931c79d71655040b49094494868fa99cea7a95"},{"_id":"public/media/files/2010/01/14/disney-sea-02.jpg","modified":1446881401853,"shasum":"c4f1c57ea5e608db5b796adfa450186135fb4948"},{"_id":"public/media/files/2010/01/14/disney-sea-01.jpg","modified":1446881401856,"shasum":"7442c20ac0dc6f5aa606641c1e8fefc599ea8d32"},{"_id":"public/media/files/2009/10/21/washer.jpeg","modified":1446881401858,"shasum":"6d60bf209c11c4e5612c7661c2edd696718ac279"},{"_id":"public/media/files/2009/10/21/tetsugakunomiti-3.jpeg","modified":1446881401861,"shasum":"5d73ea9f187f97d7d509ff17fefde5d99c755813"},{"_id":"public/media/files/2009/10/21/tetsugakunomiti-2.jpeg","modified":1446881401863,"shasum":"ae7a4a437154afd47025b08fe49fad16dcf8294c"},{"_id":"public/media/files/2009/10/21/tetsugakunomiti-1.jpeg","modified":1446881401866,"shasum":"912e3a526784c262dad71f8e293b233a9d178a8d"},{"_id":"public/media/files/2009/10/21/sansu.jpeg","modified":1446881401869,"shasum":"f6b3c8794fdc1a7daf8b7dcdee9b0e20ec0d850a"},{"_id":"public/media/files/2009/10/21/momiji-3.jpeg","modified":1446881401871,"shasum":"2671bd4bf3de94cb20795e8898dd18d598d4f32f"},{"_id":"public/media/files/2009/10/21/momiji-2.jpeg","modified":1446881401875,"shasum":"68e7b5f3957f0322ee7d05f66c92ccc507eb62b9"},{"_id":"public/media/files/2009/10/21/momiji-1.jpeg","modified":1446881401878,"shasum":"11c40665ecb8ffbcaa7035812fffeb8f9b894017"},{"_id":"public/media/files/2009/10/21/miti-4.jpeg","modified":1446881401882,"shasum":"bfb0256a774c26e471debe33b979482ce5915441"},{"_id":"public/media/files/2009/10/21/miti-3.jpeg","modified":1446881401888,"shasum":"733142f5b98109d3dcd13da647f23351fc8a10f5"},{"_id":"public/media/files/2009/10/21/miti-2.jpeg","modified":1446881401900,"shasum":"e5952527abf3571d5142c66511a284cc9af3c0b3"},{"_id":"public/media/files/2009/10/21/miti-1.jpeg","modified":1446881401904,"shasum":"e3b2cc880dcb7e3174f69533b0669567baa3f49d"},{"_id":"public/media/files/2009/10/21/kyotoeki-3.jpeg","modified":1446881401906,"shasum":"bf40fc91562b74e316154e63f26e674343d58aed"},{"_id":"public/media/files/2009/10/21/kyotoeki-2.jpeg","modified":1446881401909,"shasum":"3e32bcf6f5124a42e503d4746a3125a13e84af9d"},{"_id":"public/media/files/2009/10/21/kyotoeki-1.jpeg","modified":1446881401912,"shasum":"7a51eb4d93e9783c61940cac4c3d7e1da1ae691c"},{"_id":"public/media/files/2009/10/21/kouen-2.jpeg","modified":1446881401914,"shasum":"9a80dec28d7ed69b91043c72be6d423e24344ae8"},{"_id":"public/media/files/2009/10/21/kouen-1.jpeg","modified":1446881401916,"shasum":"aad1cc68acdd00c60230f7f0ec437eb17c200bac"},{"_id":"public/media/files/2009/10/21/hilltop.jpeg","modified":1446881401918,"shasum":"3f505aaee816a96480371aa044e78173cf784c80"},{"_id":"public/media/files/2009/10/21/ginkakuji-4.jpeg","modified":1446881401921,"shasum":"0c1683ea9f506ecc9255cc45d7469496b6fbbe1d"},{"_id":"public/media/files/2009/10/21/ginkakuji-3.jpeg","modified":1446881401923,"shasum":"a345a7583311f938151490da39930a887df5243d"},{"_id":"public/media/files/2009/10/21/ginkakuji-2.jpeg","modified":1446881401926,"shasum":"297d763525271920cb42c6162b48375f2aaa0031"},{"_id":"public/media/files/2009/10/21/ginkakuji-1.jpeg","modified":1446881401928,"shasum":"a59d4f0e37a43f9732ddfc2176d8dd1d6b7c85e0"},{"_id":"public/media/files/2009/10/21/consato.jpeg","modified":1446881401936,"shasum":"de9052d8333e8ef77fd66bbb72ddd2254c37b074"},{"_id":"public/media/files/2009/10/21/bird.jpeg","modified":1446881401940,"shasum":"f6fcdea39cff0f6df1782588e073fae6e2caec41"},{"_id":"public/media/files/2009/10/01/track.jpg","modified":1446881401946,"shasum":"7549df2d0f3ed86612fd00ecc49bfa669ed722c2"},{"_id":"public/media/files/2009/10/01/toudaiji-10.jpg","modified":1446881401952,"shasum":"203c3a9ddc6bf6d52519b61a208a1edb76eb0ebc"},{"_id":"public/media/files/2009/10/01/toudaiji-09.jpg","modified":1446881401955,"shasum":"601054295b2723d9b4bfc42c63c93247943bcf71"},{"_id":"public/media/files/2009/10/01/toudaiji-08.jpg","modified":1446881401959,"shasum":"f17b72ccf52472fffecb3dde312d3c3e1c6508cf"},{"_id":"public/media/files/2009/10/01/toudaiji-07.jpg","modified":1446881401962,"shasum":"036479dcd11d523ebeadeab5ebca97df3697679b"},{"_id":"public/media/files/2009/10/01/toudaiji-06.jpg","modified":1446881401965,"shasum":"63bad61694f405f2f54537e8504f329a7b22e3bb"},{"_id":"public/media/files/2009/10/01/toudaiji-05.jpg","modified":1446881401967,"shasum":"8d924e4318f720b45311be22c60c2ac9f458ac57"},{"_id":"public/media/files/2009/10/01/toudaiji-04.jpg","modified":1446881401970,"shasum":"dfa4e2efb4c4382e7631d8a0b9bce218df682303"},{"_id":"public/media/files/2009/10/01/toudaiji-03.jpg","modified":1446881401974,"shasum":"68791aa623eb063b8d202482d39caebd63a8d387"},{"_id":"public/media/files/2009/10/01/toudaiji-02.jpg","modified":1446881401977,"shasum":"af52f1365a901bd82a484f259d9c07e5c1df460f"},{"_id":"public/media/files/2009/10/01/toudaiji-01.jpg","modified":1446881401986,"shasum":"cab167484df91093b4ddc4812f03307fc66817dc"},{"_id":"public/media/files/2009/10/01/syokubutsuen-02.jpg","modified":1446881401989,"shasum":"e47d228bb88600f3af09659bd0a89d356d4bd5b0"},{"_id":"public/media/files/2009/10/01/syokubutsuen-01.jpg","modified":1446881401992,"shasum":"99889f915a80679d7b4962a2adab5c39dd2e9e36"},{"_id":"public/media/files/2009/10/01/kokuhoukan.jpg","modified":1446881401997,"shasum":"664e4abe76645a4cfc098534118030d7350c6165"},{"_id":"public/media/files/2009/10/01/kintetsunara.jpg","modified":1446881402002,"shasum":"9c74f995afb6e0ad34e80398cb6fa66a415b6143"},{"_id":"public/media/files/2009/10/01/kasugadaisya-04.jpg","modified":1446881402005,"shasum":"a2e912d1bf60055274a7cb581c5fd64dc62e4f9b"},{"_id":"public/media/files/2009/10/01/kasugadaisya-03.jpg","modified":1446881402010,"shasum":"076f5a6af17f31571baafffa75ba398e80d5cbb5"},{"_id":"public/media/files/2009/10/01/kasugadaisya-02.jpg","modified":1446881402012,"shasum":"349393333d87ccca485e0466e66042567ad70648"},{"_id":"public/media/files/2009/10/01/kasugadaisya-01.jpg","modified":1446881402016,"shasum":"ae44758d673bbc80d49c658681744b2e053b90e1"},{"_id":"public/media/files/2009/10/01/gosoutou.jpg","modified":1446881402018,"shasum":"8076f7b611a3c79af4708a580826d4fcd2855f47"},{"_id":"public/media/files/2009/10/01/gosoutou-kokuhoukan.jpg","modified":1446881402055,"shasum":"a134b6df2bcd3a45db20f9f0e7919cc144005301"},{"_id":"public/media/files/2009/10/01/deer-in-store.jpg","modified":1446881402059,"shasum":"0514f5ee3b6c6c609c5441884c5d6bdede48adc2"},{"_id":"public/media/files/2009/10/01/deer-08.jpg","modified":1446881402060,"shasum":"eb70b9735e89eb4017e1f60087a7827adfc969a2"},{"_id":"public/media/files/2009/10/01/deer-07.jpg","modified":1446881402064,"shasum":"a2bd8855adb8ae748e59c6c40b3d91dc99be0bd1"},{"_id":"public/media/files/2009/10/01/deer-06.jpg","modified":1446881402066,"shasum":"54dc044036fbc462d6e600b1d9da3dc347c27157"},{"_id":"public/media/files/2009/10/01/deer-05.jpg","modified":1446881402067,"shasum":"1ced7ac9d4a32a672a7f5c43ad143917a06665e5"},{"_id":"public/media/files/2009/10/01/deer-04.jpg","modified":1446881402071,"shasum":"79476e7368d605ded5bdb2f1fcaea1f7c67a79fb"},{"_id":"public/media/files/2009/10/01/deer-03.jpg","modified":1446881402073,"shasum":"c7e880b9de63a6e7e6f054d0aef091223ed335f3"},{"_id":"public/media/files/2009/10/01/deer-02.jpg","modified":1446881402075,"shasum":"cd1bb3bcfcb2b6df47e5ef5335e71047c737ee85"},{"_id":"public/media/files/2009/10/01/deer-01.jpg","modified":1446881402076,"shasum":"437749f5aaf860f3f3092e221b27d8b6a915a804"},{"_id":"public/media/files/2009/10/01/dear-deer.jpg","modified":1446881402077,"shasum":"12dc5e5857e538c6b53423429746e69fe9eb0c6f"},{"_id":"public/media/files/2009/09/27/yasakajinjya.jpg","modified":1446881402079,"shasum":"1737879ee298d9e0ea749313b6a0a95d540f861b"},{"_id":"public/media/files/2009/09/27/yasakajinjya-6.jpg","modified":1446881402086,"shasum":"135b6b706f84d5def4785b9888b076321cd37b26"},{"_id":"public/media/files/2009/09/27/yasakajinjya-5.jpg","modified":1446881402088,"shasum":"7db1e2a029a37c0f3332251cf33c858047c79d31"},{"_id":"public/media/files/2009/09/27/yasakajinjya-4.jpg","modified":1446881402091,"shasum":"b95c5dc37952602923a24dd2714916e58bdf4ee2"},{"_id":"public/media/files/2009/09/27/yasakajinjya-3.jpg","modified":1446881402101,"shasum":"b9f7412fff1ceae5ab4183bf675ca114ba5bea93"},{"_id":"public/media/files/2009/09/27/yasakajinjya-2.jpg","modified":1446881402104,"shasum":"339e5d011af03e93f82d69c632bc895011c0996d"},{"_id":"public/media/files/2009/09/27/yasakajinjya-1.jpg","modified":1446881402109,"shasum":"74789b70a096a47e837853356e4b2fabca5ad4cc"},{"_id":"public/media/files/2009/09/27/simizuji-9.jpg","modified":1446881402112,"shasum":"f7049232a6f2f8ac13c0f40ad8e64fbb67f8638e"},{"_id":"public/media/files/2009/09/27/simizuji-8.jpg","modified":1446881402114,"shasum":"bf57b009b03ff04700a0e37a2b8526419853355a"},{"_id":"public/media/files/2009/09/27/simizuji-7.jpg","modified":1446881402117,"shasum":"507b5b71b81b6d5d9cc2b13856da5589dea6955c"},{"_id":"public/media/files/2009/09/27/simizuji-6.jpg","modified":1446881402119,"shasum":"da2344650a38e203e728bb31558d7ec2809bb93d"},{"_id":"public/media/files/2009/09/27/simizuji-5.jpg","modified":1446881402121,"shasum":"0362c7151e15bc8a4935aa1253ebc9504c2e691e"},{"_id":"public/media/files/2009/09/27/simizuji-4.jpg","modified":1446881402123,"shasum":"e92850cce44187e092401fa21f4075f83c7de269"},{"_id":"public/media/files/2009/09/27/simizuji-3.jpg","modified":1446881402125,"shasum":"f81ef4b63a0a00dd941a754da2bf567eae4d054a"},{"_id":"public/media/files/2009/09/27/simizuji-2.jpg","modified":1446881402127,"shasum":"304d2d45783fdebc6e19c47a097bf462f41c5884"},{"_id":"public/media/files/2009/09/27/simizuji-10.jpg","modified":1446881402129,"shasum":"1d7fec3cccd8df74dcccfe1497585f838e4021cf"},{"_id":"public/media/files/2009/09/27/simizuji-1.jpg","modified":1446881402133,"shasum":"b353492265318761856c9b111660fd3959583f81"},{"_id":"public/media/files/2009/09/27/myouhouin.jpg","modified":1446881402135,"shasum":"bf202ee91f38d8d9c90f302b11d0496e91e7ac45"},{"_id":"public/media/files/2009/09/27/kyoto-eki.jpg","modified":1446881402138,"shasum":"798099c0b13cff8862a8ac122a37b9040eb4db84"},{"_id":"public/media/files/2009/09/27/heianjinko-5.jpg","modified":1446881402140,"shasum":"77d4b07180b10353653e3bba7da74bb96558eea4"},{"_id":"public/media/files/2009/09/27/heianjinko-4.jpg","modified":1446881402142,"shasum":"9463c1e076c22fa407e0c80ce9305211a88ed4bc"},{"_id":"public/media/files/2009/09/27/heianjinko-3.jpg","modified":1446881402146,"shasum":"718e331fbc2a249a6934ca95790a2b16b39ecf61"},{"_id":"public/media/files/2009/09/27/heianjinko-2.jpg","modified":1446881402153,"shasum":"eaab4908a2d4b156262062e39b15d3d10420e98b"},{"_id":"public/media/files/2009/09/27/heianjinko-1.jpg","modified":1446881402155,"shasum":"8eeddd2f3c921a725c5c7eb9af0f4c6d1faebc83"},{"_id":"public/media/files/2009/09/27/ginkakuji.jpg","modified":1446881402157,"shasum":"6cd915999b2585da1164bb9f1c7da7bb89e6f200"},{"_id":"public/media/files/2009/09/25/kokyo-10.jpg","modified":1446881402160,"shasum":"de530f49ff4b41ca9e2b3ea5ea6e95b127a0a977"},{"_id":"public/media/files/2009/09/25/kokyo-09.jpg","modified":1446881402166,"shasum":"c3732ef164dd0b82655df10a559b2ead5abf8a4b"},{"_id":"public/media/files/2009/09/25/kokyo-08.jpg","modified":1446881402169,"shasum":"21f57812a0022a8a319952b715f0c75caa503f6f"},{"_id":"public/media/files/2009/09/25/kokyo-07.jpg","modified":1446881402171,"shasum":"ae3ba3a817b82f5dd0d8281bdc8c4153329240f1"},{"_id":"public/media/files/2009/09/25/kokyo-06.jpg","modified":1446881402174,"shasum":"94f481ebae95744a9ddd110006dbd9c69fe764b5"},{"_id":"public/media/files/2009/09/25/kokyo-05.jpg","modified":1446881402177,"shasum":"38b30ec994735deffb28cce4f0a89ac8257c3e0d"},{"_id":"public/media/files/2009/09/25/kokyo-04.jpg","modified":1446881402179,"shasum":"152d0b5d8ed391cd4121f69c2f054d72101088bc"},{"_id":"public/media/files/2009/09/25/kokyo-03.jpg","modified":1446881402184,"shasum":"f14d7d01aae220198839f75219963813b3646d6c"},{"_id":"public/media/files/2009/09/25/kokyo-02.jpg","modified":1446881402188,"shasum":"9c871589af88c593aeb88f027cc5e5d263eab3a2"},{"_id":"public/media/files/2009/09/25/kokyo-01.jpg","modified":1446881402191,"shasum":"1bafaca7f27ddb78f7e676052e0f469b6cf84606"},{"_id":"public/media/files/2009/08/30/time-capsule-size.jpg","modified":1446881402194,"shasum":"a37122b054b9eaa25c3756202010e899e6acc6c1"},{"_id":"public/media/files/2009/08/09/xilaideng.jpg","modified":1446881402196,"shasum":"96bf90c03b2c91d38974cfea5cdd42d76e777cf6"},{"_id":"public/media/files/2009/08/09/tenpura.jpg","modified":1446881402203,"shasum":"dfffa4a76424e13ad7b77506f9d74f8acdd9cc84"},{"_id":"public/media/files/2009/08/09/tanekikouji.jpg","modified":1446881402205,"shasum":"5a9bd36008051dea6570b852b6a6ad8c32b50c9c"},{"_id":"public/media/files/2009/08/09/tanekikouji-1.jpg","modified":1446881402209,"shasum":"3169ea1a14af16c3747b3f00ea999f0b58c3db82"},{"_id":"public/media/files/2009/08/09/susukinomatsuri.jpg","modified":1446881402214,"shasum":"cabd76e5e76513008120f8d5366a04139daf4a75"},{"_id":"public/media/files/2009/08/09/monjyayaki.jpg","modified":1446881402218,"shasum":"1e04f970164bca90829530fe9fe4a87bdad54236"},{"_id":"public/media/files/2009/08/09/manjiramen.jpg","modified":1446881402224,"shasum":"e5bdf0d5fb8d99a6908470678264f59257086afa"},{"_id":"public/media/files/2009/08/09/kawaisounasakana.jpg","modified":1446881402227,"shasum":"6fcb52ddbea98cdfbe1a316cf636cc0edfe8b89f"},{"_id":"public/media/files/2009/08/09/kaminopporo.jpg","modified":1446881402229,"shasum":"3a90985da71a5de50f12d0ead4d172bf720c09f3"},{"_id":"public/media/files/2009/08/09/jinjya.jpg","modified":1446881402233,"shasum":"c9a1b41a5578b873658f2e09dc524780cffaebb0"},{"_id":"public/media/files/2009/08/09/jingisukan.jpg","modified":1446881402238,"shasum":"346a1ef2c8e8bf77826fb8cbcc14130b6cd956c3"},{"_id":"public/media/files/2009/08/09/hokkaido-daigaku.jpg","modified":1446881402240,"shasum":"e226668e9ec3d71ee7d9f406eba29c82a4d32b8c"},{"_id":"public/media/files/2009/08/09/hiroshimayaki.jpg","modified":1446881402244,"shasum":"fb1276772596241861c7a14f32c2144e1b1c888d"},{"_id":"public/media/files/2009/08/09/dashiwodaite.jpg","modified":1446881402251,"shasum":"e717b27ce790ceb629a4711a9fb9e72dab07e1c0"},{"_id":"public/media/files/2009/08/09/dankiryu.jpg","modified":1446881402255,"shasum":"b8b27caa934e7ba275b3afe6a6f98852013712c6"},{"_id":"public/media/files/2009/08/09/chinese.jpg","modified":1446881402259,"shasum":"5a0ab6d2f038e92e2cd52b0011f758f88c6cd2ce"},{"_id":"public/media/files/2009/07/28/03.jpg","modified":1446881402266,"shasum":"5b299601361f6a8c74f046f8252c336005a88f9c"},{"_id":"public/media/files/2009/07/28/02.jpg","modified":1446881402269,"shasum":"5915b1b121d6b112186813d6c818f839acc17d68"},{"_id":"public/media/files/2009/07/28/01.jpg","modified":1446881402272,"shasum":"aa30534b503eefdf710d7a3296e3c55dfb2fe554"},{"_id":"public/media/files/2009/01/26/street.jpg","modified":1446881402274,"shasum":"503a829bb9958ec7e65a900851c75786da61c049"},{"_id":"public/media/files/2009/01/26/snow.jpg","modified":1446881402276,"shasum":"45ba72c97986f2331583a15fdde4decaf7f7d2e6"},{"_id":"public/media/files/2009/01/26/santamaria.jpg","modified":1446881402278,"shasum":"0348820752740d1ea521c8fe6a2db7b0288c1cee"},{"_id":"public/media/files/2009/01/26/parda.jpg","modified":1446881402280,"shasum":"457d214b87222735112e52c3d6dc13f1f2ceed9f"},{"_id":"public/media/files/2009/01/26/mc.jpg","modified":1446881402282,"shasum":"cb421dbc3026b872f85cb6b9d6d0e35850356c20"},{"_id":"public/media/files/2009/01/26/lv.jpg","modified":1446881402284,"shasum":"be51e33134de8c3005cc7e70c913d79ef4be1542"},{"_id":"public/media/files/2009/01/26/inside-church.jpg","modified":1446881402286,"shasum":"487c4c0c6e9b01f3c42ae9edc00570be01705ca1"},{"_id":"public/media/files/2009/01/26/inside-church-1.jpg","modified":1446881402289,"shasum":"b8948381a6b54222154f75617e3fa456f033aeca"},{"_id":"public/media/files/2009/01/26/fountain.jpg","modified":1446881402293,"shasum":"1096b355419003bd85247b1f36866be90f87ebca"},{"_id":"public/media/files/2009/01/26/duomo.jpg","modified":1446881402295,"shasum":"3f7dc2388d7acde6bc66ce9896ce3cd405ef49d0"},{"_id":"public/media/files/2009/01/26/church.jpg","modified":1446881402298,"shasum":"6196e1aae6f0997fe5a190f11acbf073511f6a48"},{"_id":"public/media/files/2009/01/26/castle.jpg","modified":1446881402307,"shasum":"3b6381a3d8a94121b731e6070505923df9f64b18"},{"_id":"public/media/files/2009/01/26/castle-01.jpg","modified":1446881402315,"shasum":"81de985d9a5786bdcb6316f0ea377e5631452512"},{"_id":"public/media/files/2009/01/26/bernasoon.jpg","modified":1446881402320,"shasum":"78e8d8a0801013fc945edd06890e8ad0161c9003"},{"_id":"public/media/files/2008/12/16/venezia.jpg","modified":1446881402324,"shasum":"8ce358f5d8fc9781ad15244bd4b35a956fc86980"},{"_id":"public/media/files/2008/12/16/venezia-wedding.jpg","modified":1446881402328,"shasum":"7441f454afd1ecf3b0135b576e9492ea00c43ca4"},{"_id":"public/media/files/2008/12/16/venezia-tower.jpg","modified":1446881402331,"shasum":"7dd11aabdc68d54db95dd9e7151b25cec54870b1"},{"_id":"public/media/files/2008/12/16/venezia-stop-in-rain.jpg","modified":1446881402333,"shasum":"87f2a082b3a14149e08c2a832958873c1a2cd8a3"},{"_id":"public/media/files/2008/12/16/venezia-stop-in-morning.jpg","modified":1446881402335,"shasum":"e78c88b50b22bf8567a369131071de065f3a0122"},{"_id":"public/media/files/2008/12/16/venezia-river.jpg","modified":1446881402337,"shasum":"30576235225ea8c37a9d686641d746188b0dd201"},{"_id":"public/media/files/2008/12/16/venezia-morning.jpg","modified":1446881402339,"shasum":"f418d3eedfac97e9023643879a5279963949706c"},{"_id":"public/media/files/2008/12/16/venezia-mask.jpg","modified":1446881402343,"shasum":"1033ba14fe90320bd877951792daf8c001bc93f0"},{"_id":"public/media/files/2008/12/16/venezia-lunch.jpg","modified":1446881402346,"shasum":"22e2d31d0daf3790074a9f97c82ce4db003da99d"},{"_id":"public/media/files/2008/12/16/venezia-in-water.jpg","modified":1446881402355,"shasum":"a2fe94747fafebcb6d100cc1c80f5a77391b74f5"},{"_id":"public/media/files/2008/12/16/venezia-in-water-04.jpg","modified":1446881402367,"shasum":"3512c86a92101d6d65c9a5a08855fde383c47c3b"},{"_id":"public/media/files/2008/12/16/venezia-in-water-03.jpg","modified":1446881402370,"shasum":"3126b52a4e61003991e7c1d1b5796e1c99764eb6"},{"_id":"public/media/files/2008/12/16/venezia-in-water-02.jpg","modified":1446881402373,"shasum":"8dbedf6a010c7d4cdb7225bded06bcadfe6bea5f"},{"_id":"public/media/files/2008/12/16/venezia-hotel.jpg","modified":1446881402376,"shasum":"32967a9e4b750714d180b212d93c7a92aa8d00fc"},{"_id":"public/media/files/2008/12/16/venezia-beach.jpg","modified":1446881402380,"shasum":"88adffce33c94f44d408b42f8a636856b7b49ea7"},{"_id":"public/media/files/2008/12/16/tanxiqiao.jpg","modified":1446881402384,"shasum":"b86a2fd25673557ba00c9bffd7d104c130d33344"},{"_id":"public/media/files/2008/08/18/sunrise-03.jpg","modified":1446881402386,"shasum":"fd36a5059f77b44740b71d65f5f3516e9af27098"},{"_id":"public/media/files/2008/08/18/sunrise-02.jpg","modified":1446881402388,"shasum":"4dcec153ed4f7095c65da6151c7f83054a92d3be"},{"_id":"public/media/files/2008/08/18/sunrise-01.jpg","modified":1446881402391,"shasum":"69510c2e3d960422d05f632ff21578246d86fdca"},{"_id":"public/media/files/2008/08/18/stick.jpg","modified":1446881402393,"shasum":"dd7da41d009d654d4f862668fc6fcc2f6de586a2"},{"_id":"public/media/files/2008/08/18/me.jpg","modified":1446881402395,"shasum":"3235342d74fed65244c209735b63ad64712df9e1"},{"_id":"public/media/files/2008/08/18/cloud.jpg","modified":1446881402405,"shasum":"4e1a66b8c1485e22c72f97036b8b6c9bdbc91ba8"},{"_id":"public/media/files/2008/07/22/ship.jpg","modified":1446881402410,"shasum":"0fea8e5f2857c6abbf1debcbedbf6e14bd239339"},{"_id":"public/media/files/2008/07/22/port.jpg","modified":1446881402417,"shasum":"373ed1e06152a306a7e09f080eaa1d90ed56a842"},{"_id":"public/media/files/2008/07/22/jellyfish.jpg","modified":1446881402420,"shasum":"0df64ec0384083c1faefc98f3bb14f0df145700f"},{"_id":"public/media/files/2008/07/22/eagle.jpg","modified":1446881402423,"shasum":"eb0853719d5d933331735e559bdf8e0281acdebf"},{"_id":"public/media/files/2008/07/22/beach.jpg","modified":1446881402426,"shasum":"6c44ea4f6da07582aa23f33a1c87be5bca408556"},{"_id":"public/media/files/2008/07/22/all.jpg","modified":1446881402429,"shasum":"493290203a2b72010c687f4fb3bdb844c4b30e57"},{"_id":"public/media/files/2008/06/29/mt-tmpl-opt.png","modified":1446881402431,"shasum":"795f6211c1ee2b6c9e09f7d37b781f2ffce5fd32"},{"_id":"public/media/files/2008/06/29/mt-pub-ssi.png","modified":1446881402432,"shasum":"c5ba6d44215b1e138a36412a1e9f0f55c4d5e4b9"},{"_id":"public/media/files/2008/06/08/okutama-05.jpg","modified":1446881402435,"shasum":"5bcdf45001a436f9865daac7ecf31b1ef12fd5b8"},{"_id":"public/media/files/2008/06/08/okutama-04.jpg","modified":1446881402439,"shasum":"8d78807b6224f9da340c53bc642a81fccad9a6bb"},{"_id":"public/media/files/2008/06/08/okutama-03.jpg","modified":1446881402441,"shasum":"01ce32d5ab459e873ef73be1e9c0d5fa8a6fe577"},{"_id":"public/media/files/2008/06/08/okutama-02.jpg","modified":1446881402445,"shasum":"375806141925af98ea7573256cf093b6d6406dee"},{"_id":"public/media/files/2008/06/08/okutama-01.jpg","modified":1446881402455,"shasum":"a79613bbca4eb55c12cb1b54a32249d46a91e1aa"},{"_id":"public/media/files/2008/03/31/txp_sofa_screenshot.jpg","modified":1446881402458,"shasum":"13725608626e47d16e3017ad926d3f3e09b1d64b"},{"_id":"public/media/files/2008/03/24/dvfy.jpg","modified":1446881402463,"shasum":"6a5549018e6c082578a387c9f4a5303d62343a14"},{"_id":"public/media/files/2007/07/26/travel-19.jpg","modified":1446881402468,"shasum":"747d5e4d54514de9ae10a45bf374817479e61ab7"},{"_id":"public/media/files/2007/07/26/travel-08.jpg","modified":1446881402470,"shasum":"0f6ee159549dcf71bf5792973b8b63e50ede5959"},{"_id":"public/media/files/2007/07/26/travel-03.jpg","modified":1446881402473,"shasum":"af279b676deab7d9e7773360c460b48e7c0c6f81"},{"_id":"public/media/favicon.ico","modified":1446881402476,"shasum":"93464c518e082f98aeef062a31be21502c681c6b"},{"_id":"public/media/js/jquery.tagcloud.js","modified":1446881402478,"shasum":"012beeb2895477692ee67f09c694e06863b189dd"},{"_id":"public/media/fonts/telex-regular.css","modified":1446881402479,"shasum":"3f71bb0cd895e159e279d4bb0baa2063b579467f"},{"_id":"public/media/css/style.css","modified":1446881402480,"shasum":"fc617fde55dc532c564f208bb3ad94eaf6c05514"},{"_id":"public/2015/11/07/tes.html","modified":1446903621519,"shasum":"2719ec6735f88d52e00ade2972be9f65d5e67224"},{"_id":"public/2015/10/09/facebook_photo_caching.html","modified":1446904455002,"shasum":"4983658f36c5cbb26fb7e367bcf6e1b332b01c95"},{"_id":"public/2015/09/23/youpai_communicate.html","modified":1446904455108,"shasum":"168690bbd95204ac06c40058e6f5ed616eb1c052"},{"_id":"public/2015/06/26/go-runtime-1.4.html","modified":1446904455206,"shasum":"1c5a7a7f5158c3fd0f009538a32ce4cd7b738930"},{"_id":"public/2014/10/20/goroutine-contiguous-stack.html","modified":1446904455320,"shasum":"0f09ebc8242469a69217c6ea0097677721f0be93"},{"_id":"public/2014/09/24/goroutine-scheduler.html","modified":1446904455435,"shasum":"717ab451b12fca0896d854cdadc130c6977e0c85"},{"_id":"public/2014/09/02/stroage_consistency_replicate.html","modified":1446904455539,"shasum":"5020e27e4d1951262d9f1a75f6855da58ef76310"},{"_id":"public/2014/08/12/stroage_consistency_avaliable_post2.html","modified":1446904455622,"shasum":"2760f135cf2f5c36ece9132be218c2c122fa4155"},{"_id":"public/2014/07/26/stroage_consistency_avaliable_post1.html","modified":1446904455719,"shasum":"6c2f437b1045d54d7020b7945a9ac124f1ef0a57"},{"_id":"public/2014/01/20/golang-vs-nginx-test.html","modified":1446904455821,"shasum":"15da8bf4cd6ab620303df9ccdbc3358f42513773"},{"_id":"public/archives/index.html","modified":1446904455904,"shasum":"c0140c1875ba1f35e56a852c83d162f2bb4e37a4"},{"_id":"public/archives/2014/index.html","modified":1446904455988,"shasum":"3426214fcc6e54c09b1263d3f2c316483a7843f5"},{"_id":"public/archives/2014/01/index.html","modified":1446904456071,"shasum":"5294c398a22dfa2b0f9149392de2dcf491a6ab9a"},{"_id":"public/archives/2014/07/index.html","modified":1446904456166,"shasum":"0fea507fe9c292f1db6bdc997c5b6a47f0a04827"},{"_id":"public/archives/2014/08/index.html","modified":1446904456235,"shasum":"2c2ee459024d5ef1b4fc3c4e4ea5047677ecadea"},{"_id":"public/archives/2014/09/index.html","modified":1446904456323,"shasum":"db38df2f000d2044afed6a16e5fb885dc354b4e5"},{"_id":"public/archives/2014/10/index.html","modified":1446904456406,"shasum":"d5ef9621ffa3d3a5464a5ede38f46f63b9ea699b"},{"_id":"public/archives/2015/index.html","modified":1446904456490,"shasum":"470f73775537ffdb28e916277e37060ffdd01f60"},{"_id":"public/archives/2015/06/index.html","modified":1446904456561,"shasum":"8ebda09537eae1d8a0ccef25292c5a9469d82e3e"},{"_id":"public/archives/2015/09/index.html","modified":1446904456642,"shasum":"1b43afd28d9a64e9a285244c6ebe1936c69c5768"},{"_id":"public/archives/2015/10/index.html","modified":1446904456730,"shasum":"ea68d4f1d4b2f32933d8d4d8cccae7b26913b6bd"},{"_id":"public/archives/2015/11/index.html","modified":1446903623815,"shasum":"111c5c4eaa11acd1f038fe44592857dff9b8a766"},{"_id":"public/index.html","modified":1446904457889,"shasum":"b1da164af4b90b1400b49d946ba978ab1218ce99"},{"_id":"public/tags/golang/index.html","modified":1446904456835,"shasum":"70cd744fe3c4f7cf178490929f34eb096e8d1b14"},{"_id":"public/tags/nginx/index.html","modified":1446904456934,"shasum":"ee54537caf5c4d84fd1dc90bfa403a24c4dac1ec"},{"_id":"public/tags/performance/index.html","modified":1446904457035,"shasum":"daae8ded4419b2449a881b17014b67866b0dc602"},{"_id":"public/tags/云存储/index.html","modified":1446904457137,"shasum":"6c9febc8c2a1a318a822dab1e81c9ad39555c253"},{"_id":"public/tags/CDN/index.html","modified":1446904457248,"shasum":"2718b919817d91da2f0418f622a8499a940d70c3"},{"_id":"public/tags/交流/index.html","modified":1446904457313,"shasum":"e68fc5677fc399d759a2ccfc40462763ff0b44e4"},{"_id":"public/tags/云存储、cdn/index.html","modified":1446904457410,"shasum":"85d66829d41eb30ae3ccb6d73d7a052055d39243"},{"_id":"public/tags/runtime/index.html","modified":1446904457475,"shasum":"65dd4ad5a8f72c602b6282d12ead61fe1dd93505"},{"_id":"public/tags/goroutine/index.html","modified":1446904457556,"shasum":"6435f09251cae16d91e41f252581fb4ba6226f7b"},{"_id":"public/tags/storage/index.html","modified":1446904457632,"shasum":"73cff60ee68e7d137d56a977887414f4a1c4f6dc"},{"_id":"public/tags/consistency/index.html","modified":1446904457722,"shasum":"9817d080201232e05591a1a08282623ad8b634ab"},{"_id":"public/tags/avaliable/index.html","modified":1446904457786,"shasum":"2257172e333c4702a037ff391a389948bddd22a8"},{"_id":"source/about/index.md","shasum":"6b376268b51082fa5841a163c5ad2a039ca40908","modified":1446879638000},{"_id":"public/about/index.html","modified":1446904454831,"shasum":"c5a3527962759ab794cd39591c625b9380f7b917"}],"Category":[],"Data":[],"Page":[{"_content":"(function($) {\n\n  $.fn.tagcloud = function(options) {\n    var opts = $.extend({}, $.fn.tagcloud.defaults, options);\n    tagWeights = this.map(function(){\n      return $(this).attr(\"rel\");\n    });\n    tagWeights = jQuery.makeArray(tagWeights).sort(compareWeights);\n    lowest = tagWeights[0];\n    highest = tagWeights.pop();\n    range = highest - lowest;\n    if(range === 0) {range = 1;}\n    // Sizes\n    if (opts.size) {\n      fontIncr = (opts.size.end - opts.size.start)/range;\n    }\n    // Colors\n    if (opts.color) {\n      colorIncr = colorIncrement (opts.color, range);\n    }\n    return this.each(function() {\n      weighting = $(this).attr(\"rel\") - lowest;\n      if (opts.size) {\n        $(this).css({\"font-size\": opts.size.start + (weighting * fontIncr) + opts.size.unit});\n      }\n      if (opts.color) {\n        $(this).css({\"color\": tagColor(opts.color, colorIncr, weighting)});\n      }\n    });\n  };\n\n  $.fn.tagcloud.defaults = {\n    size: {start: 14, end: 18, unit: \"pt\"}\n  };\n\n  // Converts hex to an RGB array\n  function toRGB (code) {\n    if (code.length == 4) {\n      code = jQuery.map(/\\w+/.exec(code), function(el) {return el + el; }).join(\"\");\n    }\n    hex = /(\\w{2})(\\w{2})(\\w{2})/.exec(code);\n    return [parseInt(hex[1], 16), parseInt(hex[2], 16), parseInt(hex[3], 16)];\n  }\n\n  // Converts an RGB array to hex\n  function toHex (ary) {\n    return \"#\" + jQuery.map(ary, function(i) {\n      hex =  i.toString(16);\n      hex = (hex.length == 1) ? \"0\" + hex : hex;\n      return hex;\n    }).join(\"\");\n  }\n\n  function colorIncrement (color, range) {\n    return jQuery.map(toRGB(color.end), function(n, i) {\n      return (n - toRGB(color.start)[i])/range;\n    });\n  }\n\n  function tagColor (color, increment, weighting) {\n    rgb = jQuery.map(toRGB(color.start), function(n, i) {\n      ref = Math.round(n + (increment[i] * weighting));\n      if (ref > 255) {\n        ref = 255;\n      } else {\n        if (ref < 0) {\n          ref = 0;\n        }\n      }\n      return ref;\n    });\n    return toHex(rgb);\n  }\n\n  function compareWeights(a, b)\n  {\n    return a - b;\n  }\n\n})(jQuery);\n","source":"media/js/jquery.tagcloud.js","raw":"(function($) {\n\n  $.fn.tagcloud = function(options) {\n    var opts = $.extend({}, $.fn.tagcloud.defaults, options);\n    tagWeights = this.map(function(){\n      return $(this).attr(\"rel\");\n    });\n    tagWeights = jQuery.makeArray(tagWeights).sort(compareWeights);\n    lowest = tagWeights[0];\n    highest = tagWeights.pop();\n    range = highest - lowest;\n    if(range === 0) {range = 1;}\n    // Sizes\n    if (opts.size) {\n      fontIncr = (opts.size.end - opts.size.start)/range;\n    }\n    // Colors\n    if (opts.color) {\n      colorIncr = colorIncrement (opts.color, range);\n    }\n    return this.each(function() {\n      weighting = $(this).attr(\"rel\") - lowest;\n      if (opts.size) {\n        $(this).css({\"font-size\": opts.size.start + (weighting * fontIncr) + opts.size.unit});\n      }\n      if (opts.color) {\n        $(this).css({\"color\": tagColor(opts.color, colorIncr, weighting)});\n      }\n    });\n  };\n\n  $.fn.tagcloud.defaults = {\n    size: {start: 14, end: 18, unit: \"pt\"}\n  };\n\n  // Converts hex to an RGB array\n  function toRGB (code) {\n    if (code.length == 4) {\n      code = jQuery.map(/\\w+/.exec(code), function(el) {return el + el; }).join(\"\");\n    }\n    hex = /(\\w{2})(\\w{2})(\\w{2})/.exec(code);\n    return [parseInt(hex[1], 16), parseInt(hex[2], 16), parseInt(hex[3], 16)];\n  }\n\n  // Converts an RGB array to hex\n  function toHex (ary) {\n    return \"#\" + jQuery.map(ary, function(i) {\n      hex =  i.toString(16);\n      hex = (hex.length == 1) ? \"0\" + hex : hex;\n      return hex;\n    }).join(\"\");\n  }\n\n  function colorIncrement (color, range) {\n    return jQuery.map(toRGB(color.end), function(n, i) {\n      return (n - toRGB(color.start)[i])/range;\n    });\n  }\n\n  function tagColor (color, increment, weighting) {\n    rgb = jQuery.map(toRGB(color.start), function(n, i) {\n      ref = Math.round(n + (increment[i] * weighting));\n      if (ref > 255) {\n        ref = 255;\n      } else {\n        if (ref < 0) {\n          ref = 0;\n        }\n      }\n      return ref;\n    });\n    return toHex(rgb);\n  }\n\n  function compareWeights(a, b)\n  {\n    return a - b;\n  }\n\n})(jQuery);\n","date":"2015-11-07T06:11:08.000Z","updated":"2015-01-20T14:11:45.000Z","path":"media/js/jquery.tagcloud.js","layout":"false","title":"","comments":1,"_id":"cigortt8h0007kfy7kqzo4nh0"},{"_content":"/* Generated by Font Squirrel (http://www.fontsquirrel.com) on March 17, 2014 */\n\n\n\n@font-face {\n    font-family: 'telexregular';\n    src: url('telex-regular-webfont.eot');\n    src: url('telex-regular-webfont.eot?#iefix') format('embedded-opentype'),\n         url('telex-regular-webfont.woff') format('woff'),\n         url('telex-regular-webfont.ttf') format('truetype'),\n         url('telex-regular-webfont.svg#telexregular') format('svg');\n    font-weight: normal;\n    font-style: normal;\n\n}","source":"media/fonts/telex-regular.css","raw":"/* Generated by Font Squirrel (http://www.fontsquirrel.com) on March 17, 2014 */\n\n\n\n@font-face {\n    font-family: 'telexregular';\n    src: url('telex-regular-webfont.eot');\n    src: url('telex-regular-webfont.eot?#iefix') format('embedded-opentype'),\n         url('telex-regular-webfont.woff') format('woff'),\n         url('telex-regular-webfont.ttf') format('truetype'),\n         url('telex-regular-webfont.svg#telexregular') format('svg');\n    font-weight: normal;\n    font-style: normal;\n\n}","date":"2015-11-07T06:11:08.000Z","updated":"2015-01-20T14:11:45.000Z","path":"media/fonts/telex-regular.css","layout":"false","title":"","comments":1,"_id":"cigortt8k0008kfy7u9e0mglu"},{"_content":"/*------------------------------------\n * sext-vi\n *\n * @author linghua.zhang@me.com\n * @link http://lhzhang.com/\n * @update 2014-03-17\n *\n * |/ | (~ (~\n * |\\.|._)._)\n * --------------------------------- */\n\n@import url(/media/fonts/telex-regular.css);\n@import url(http://fonts.googleapis.com/css?family=Ubuntu:300);\n\n$font-name: \"telexregular\";\n$mono-font-name: \"Ubuntu\";\n$page-width: 800;\n$font: $font-name, \"Hiragino Sans GB\", \"Microsoft YaHei\", sans-serif;\n$monospace: $mono-font-name, monospace;\n$label-font: $font-name, \"Hiragino Sans GB\", \"Microsoft YaHei\", sans-serif;\n$background-color: #f9f9f9;\n\n@mixin page($width: $page-width) { width: #{$width}px; margin: 0 auto; }\n\n* { margin: 0; padding: 0; }\nbody { font-size: 14px; font-family: #{$font}; color: #555; background-color: $background-color; }\nol, ul { list-style-position: inside; }\nli { padding: .2em 0; }\nhr { width: 4em; border: none; border-top: 1px dashed #d0d0d0; border-bottom: 1px dashed #f9f9f9; }\n.center { text-align: center; }\n::selection { color: #fff; background-color: #333; }\na { text-decoration: none; color: #bb2222; &:hover { color: #dd1144; }; }\nblockquote { background-color: #f8f8f8; padding: 0 1em; border: 1px dashed #eee !important; border-color: #e0e0e0 !important; font-size: 13px; line-height: 1.6; display: block; overflow: auto; }\npre code { @extend blockquote; }\ncode { background-color: #f8f8f8; padding: 2px 5px; border: 1px dashed #e0e0e0; font-family: #{$monospace}; }\nh1, h2 { font-size: 1.4em; }\ntime { font-family: #{$monospace}; }\n\nheader {\n  @include page;\n  text-align: justify;\n  margin: 3em auto;\n  &:after { content: ''; width: 100%; display: inline-block; }\n\n  #header {\n    display: inline-block;\n    position: relative;\n    top: 1em;\n    h1 {\n      font-family: #{$label-font};\n      a { @extend a;\n        color: #a9a9a9; display: block;\n      }\n    }\n  }\n\n  nav {\n    display: inline-block;\n    position: relative;\n    top: 1em;\n    span { margin: .5em; }\n    a {\n      font-family: #{$label-font}; color: #ccc;\n    }\n  }\n}\n\n#content {\n  @include page;\n  line-height: 2em;\n  .main-listing { margin-bottom: 2em; }\n}\n\nul.listing {\n  margin-top: 1em;\n  li {\n    list-style-type:none;\n    padding: 0;\n    &.listing-item { a { padding: .2em 0 .2em 2em; } time { color: #999; } &:hover { background-color: #f9f9f9; } }\n    &.listing-seperator { font-family: #{$label-font}; &:before { content: \"⭠ \"; color: #ccc; } }\n  }\n}\n\narticle {\n  margin: 2em 0;\n  p { padding: .7em 0; }\n  .title {\n    font-family: #{$label-font};\n    a { @extend a; color: #777; }\n  }\n\n  .meta {\n    display: block;\n    overflow: auto;\n    margin-top: -.5em;\n    font-size: .9em;\n    .tags a { @extend a; color: #999999; padding: .25em; }\n  }\n\n  .post { img { max-width: #{$page-width}px; display: block; margin: .5em auto; } }\n\n  .comment { margin: 3em 0; }\n  .divider {\n    margin: 2em 0;\n\n    i { margin: 0 2em; color: #e9e9e9; }\n\n    a { @extend a;\n      font-family: #{$label-font};\n      margin: 0 2em;\n      i { color: #999999; margin: 0; &:hover { color: #bb2222 !important; } }\n    }\n  }\n  .divider {\n    position: relative;\n    font-size: 1em;\n    z-index: 1;\n    overflow: hidden;\n    text-align: center;\n  }\n  .divider:before, .divider:after {\n    position: absolute;\n    top: 51%;\n    overflow: hidden;\n    width: 49%;\n    height: 2px;\n    content: '\\a0';\n    background-color: #f0f0f0;\n  }\n  .divider:before { margin-left: -50%; text-align: right; }\n  .divider:after { margin-left: 1%; }\n}\n\nfooter {\n  @include page;\n  display: block;\n  color: #909090;\n  font-family: #{$label-font};\n  font-size: .9em;\n  text-align: center;\n  margin: 1em auto;\n  a { color: #cccccc; &:hover { color: #999999; }; }\n\n}\n\n#tag_cloud { margin-bottom: 1em; }\n\n// gist\n.gist { font-size: 13px; line-height:1.6; }\n\n// like\n.like-wrapper {\n  color: #999;\n  .like-button, .unlike-button { font-family: \"FontAwesome\"; padding-right: 5px; }\n  .like-button:hover { color: #dd1144; }\n  .like-button.liked:hover { color: #999; }\n}\n","source":"media/css/style.scss","raw":"/*------------------------------------\n * sext-vi\n *\n * @author linghua.zhang@me.com\n * @link http://lhzhang.com/\n * @update 2014-03-17\n *\n * |/ | (~ (~\n * |\\.|._)._)\n * --------------------------------- */\n\n@import url(/media/fonts/telex-regular.css);\n@import url(http://fonts.googleapis.com/css?family=Ubuntu:300);\n\n$font-name: \"telexregular\";\n$mono-font-name: \"Ubuntu\";\n$page-width: 800;\n$font: $font-name, \"Hiragino Sans GB\", \"Microsoft YaHei\", sans-serif;\n$monospace: $mono-font-name, monospace;\n$label-font: $font-name, \"Hiragino Sans GB\", \"Microsoft YaHei\", sans-serif;\n$background-color: #f9f9f9;\n\n@mixin page($width: $page-width) { width: #{$width}px; margin: 0 auto; }\n\n* { margin: 0; padding: 0; }\nbody { font-size: 14px; font-family: #{$font}; color: #555; background-color: $background-color; }\nol, ul { list-style-position: inside; }\nli { padding: .2em 0; }\nhr { width: 4em; border: none; border-top: 1px dashed #d0d0d0; border-bottom: 1px dashed #f9f9f9; }\n.center { text-align: center; }\n::selection { color: #fff; background-color: #333; }\na { text-decoration: none; color: #bb2222; &:hover { color: #dd1144; }; }\nblockquote { background-color: #f8f8f8; padding: 0 1em; border: 1px dashed #eee !important; border-color: #e0e0e0 !important; font-size: 13px; line-height: 1.6; display: block; overflow: auto; }\npre code { @extend blockquote; }\ncode { background-color: #f8f8f8; padding: 2px 5px; border: 1px dashed #e0e0e0; font-family: #{$monospace}; }\nh1, h2 { font-size: 1.4em; }\ntime { font-family: #{$monospace}; }\n\nheader {\n  @include page;\n  text-align: justify;\n  margin: 3em auto;\n  &:after { content: ''; width: 100%; display: inline-block; }\n\n  #header {\n    display: inline-block;\n    position: relative;\n    top: 1em;\n    h1 {\n      font-family: #{$label-font};\n      a { @extend a;\n        color: #a9a9a9; display: block;\n      }\n    }\n  }\n\n  nav {\n    display: inline-block;\n    position: relative;\n    top: 1em;\n    span { margin: .5em; }\n    a {\n      font-family: #{$label-font}; color: #ccc;\n    }\n  }\n}\n\n#content {\n  @include page;\n  line-height: 2em;\n  .main-listing { margin-bottom: 2em; }\n}\n\nul.listing {\n  margin-top: 1em;\n  li {\n    list-style-type:none;\n    padding: 0;\n    &.listing-item { a { padding: .2em 0 .2em 2em; } time { color: #999; } &:hover { background-color: #f9f9f9; } }\n    &.listing-seperator { font-family: #{$label-font}; &:before { content: \"⭠ \"; color: #ccc; } }\n  }\n}\n\narticle {\n  margin: 2em 0;\n  p { padding: .7em 0; }\n  .title {\n    font-family: #{$label-font};\n    a { @extend a; color: #777; }\n  }\n\n  .meta {\n    display: block;\n    overflow: auto;\n    margin-top: -.5em;\n    font-size: .9em;\n    .tags a { @extend a; color: #999999; padding: .25em; }\n  }\n\n  .post { img { max-width: #{$page-width}px; display: block; margin: .5em auto; } }\n\n  .comment { margin: 3em 0; }\n  .divider {\n    margin: 2em 0;\n\n    i { margin: 0 2em; color: #e9e9e9; }\n\n    a { @extend a;\n      font-family: #{$label-font};\n      margin: 0 2em;\n      i { color: #999999; margin: 0; &:hover { color: #bb2222 !important; } }\n    }\n  }\n  .divider {\n    position: relative;\n    font-size: 1em;\n    z-index: 1;\n    overflow: hidden;\n    text-align: center;\n  }\n  .divider:before, .divider:after {\n    position: absolute;\n    top: 51%;\n    overflow: hidden;\n    width: 49%;\n    height: 2px;\n    content: '\\a0';\n    background-color: #f0f0f0;\n  }\n  .divider:before { margin-left: -50%; text-align: right; }\n  .divider:after { margin-left: 1%; }\n}\n\nfooter {\n  @include page;\n  display: block;\n  color: #909090;\n  font-family: #{$label-font};\n  font-size: .9em;\n  text-align: center;\n  margin: 1em auto;\n  a { color: #cccccc; &:hover { color: #999999; }; }\n\n}\n\n#tag_cloud { margin-bottom: 1em; }\n\n// gist\n.gist { font-size: 13px; line-height:1.6; }\n\n// like\n.like-wrapper {\n  color: #999;\n  .like-button, .unlike-button { font-family: \"FontAwesome\"; padding-right: 5px; }\n  .like-button:hover { color: #dd1144; }\n  .like-button.liked:hover { color: #999; }\n}\n","date":"2015-11-07T06:11:08.000Z","updated":"2015-01-20T14:11:45.000Z","path":"media/css/style.css","layout":"false","title":"","comments":1,"_id":"cigortt9p0009kfy7g4mtunme"},{"_content":"@charset \"UTF-8\";\n/*------------------------------------\n * sext-vi\n *\n * @author linghua.zhang@me.com\n * @link http://lhzhang.com/\n * @update 2014-03-17\n *\n * |/ | (~ (~\n * |\\.|._)._)\n * --------------------------------- */\n@import url(/media/fonts/telex-regular.css);\n@import url(http://fonts.googleapis.com/css?family=Ubuntu:300);\n* {\n  margin: 0;\n  padding: 0; }\n\nbody {\n  font-size: 14px;\n  font-family: \"telexregular\", \"Hiragino Sans GB\", \"Microsoft YaHei\", sans-serif;\n  color: #555;\n  background-color: #f9f9f9; }\n\nol, ul {\n  list-style-position: inside; }\n\nli {\n  padding: .2em 0; }\n\nhr {\n  width: 4em;\n  border: none;\n  border-top: 1px dashed #d0d0d0;\n  border-bottom: 1px dashed #f9f9f9; }\n\n.center {\n  text-align: center; }\n\n::selection {\n  color: #fff;\n  background-color: #333; }\n\na, header #header h1 a, article .title a, article .meta .tags a, article .divider a {\n  text-decoration: none;\n  color: #bb2222; }\n  a:hover, header #header h1 a:hover, article .title a:hover, article .meta .tags a:hover, article .divider a:hover {\n    color: #dd1144; }\n\nblockquote, pre code {\n  background-color: #f8f8f8;\n  padding: 0 1em;\n  border: 1px dashed #eee !important;\n  border-color: #e0e0e0 !important;\n  font-size: 13px;\n  line-height: 1.6;\n  display: block;\n  overflow: auto; }\n\ncode {\n  background-color: #f8f8f8;\n  padding: 2px 5px;\n  border: 1px dashed #e0e0e0;\n  font-family: \"Ubuntu\", monospace; }\n\nh1, h2 {\n  font-size: 1.4em; }\n\ntime {\n  font-family: \"Ubuntu\", monospace; }\n\nheader {\n  width: 800px;\n  margin: 0 auto;\n  text-align: justify;\n  margin: 3em auto; }\n  header:after {\n    content: '';\n    width: 100%;\n    display: inline-block; }\n  header #header {\n    display: inline-block;\n    position: relative;\n    top: 1em; }\n    header #header h1 {\n      font-family: \"telexregular\", \"Hiragino Sans GB\", \"Microsoft YaHei\", sans-serif; }\n      header #header h1 a {\n        color: #a9a9a9;\n        display: block; }\n  header nav {\n    display: inline-block;\n    position: relative;\n    top: 1em; }\n    header nav span {\n      margin: .5em; }\n    header nav a, header nav #header h1 a, header #header h1 nav a, header nav article .title a, article .title header nav a, header nav article .meta .tags a, article .meta .tags header nav a, header nav article .divider a, article .divider header nav a {\n      font-family: \"telexregular\", \"Hiragino Sans GB\", \"Microsoft YaHei\", sans-serif;\n      color: #ccc; }\n\n#content {\n  width: 800px;\n  margin: 0 auto;\n  line-height: 2em; }\n  #content .main-listing {\n    margin-bottom: 2em; }\n\nul.listing {\n  margin-top: 1em; }\n  ul.listing li {\n    list-style-type: none;\n    padding: 0; }\n    ul.listing li.listing-item a, ul.listing li.listing-item header #header h1 a, header #header h1 ul.listing li.listing-item a {\n      padding: .2em 0 .2em 2em; }\n    ul.listing li.listing-item time {\n      color: #999; }\n    ul.listing li.listing-item:hover {\n      background-color: #f9f9f9; }\n    ul.listing li.listing-seperator {\n      font-family: \"telexregular\", \"Hiragino Sans GB\", \"Microsoft YaHei\", sans-serif; }\n      ul.listing li.listing-seperator:before {\n        content: \"⭠ \";\n        color: #ccc; }\n\narticle {\n  margin: 2em 0; }\n  article p {\n    padding: .7em 0; }\n  article .title {\n    font-family: \"telexregular\", \"Hiragino Sans GB\", \"Microsoft YaHei\", sans-serif; }\n    article .title header #header h1 a, header #header h1 article .title a, article .title a, article .title .meta .tags a, article .meta .tags .title a {\n      color: #777; }\n  article .meta {\n    display: block;\n    overflow: auto;\n    margin-top: -.5em;\n    font-size: .9em; }\n    article .meta .tags header #header h1 a, header #header h1 article .meta .tags a, article .meta .tags a {\n      color: #999999;\n      padding: .25em; }\n  article .post img {\n    max-width: 800px;\n    display: block;\n    margin: .5em auto; }\n  article .comment {\n    margin: 3em 0; }\n  article .divider {\n    margin: 2em 0; }\n    article .divider i {\n      margin: 0 2em;\n      color: #e9e9e9; }\n    article .divider header #header h1 a, header #header h1 article .divider a, article .divider .meta .tags a, article .meta .tags .divider a, article .divider a {\n      font-family: \"telexregular\", \"Hiragino Sans GB\", \"Microsoft YaHei\", sans-serif;\n      margin: 0 2em; }\n      article .divider header #header h1 a i, header #header h1 article .divider a i, article .divider .meta .tags a i, article .meta .tags .divider a i, article .divider a i {\n        color: #999999;\n        margin: 0; }\n        article .divider header #header h1 a i:hover, header #header h1 article .divider a i:hover, article .divider a i:hover {\n          color: #bb2222 !important; }\n  article .divider {\n    position: relative;\n    font-size: 1em;\n    z-index: 1;\n    overflow: hidden;\n    text-align: center; }\n  article .divider:before, article .divider:after {\n    position: absolute;\n    top: 51%;\n    overflow: hidden;\n    width: 49%;\n    height: 2px;\n    content: '\\a0';\n    background-color: #f0f0f0; }\n  article .divider:before {\n    margin-left: -50%;\n    text-align: right; }\n  article .divider:after {\n    margin-left: 1%; }\n\nfooter {\n  width: 800px;\n  margin: 0 auto;\n  display: block;\n  color: #909090;\n  font-family: \"telexregular\", \"Hiragino Sans GB\", \"Microsoft YaHei\", sans-serif;\n  font-size: .9em;\n  text-align: center;\n  margin: 1em auto; }\n  footer a, footer header #header h1 a, header #header h1 footer a, footer article .title a, article .title footer a, footer article .meta .tags a, article .meta .tags footer a, footer article .divider a, article .divider footer a {\n    color: #cccccc; }\n    footer a:hover, footer header #header h1 a:hover, header #header h1 footer a:hover, footer article .meta .tags a:hover, article .meta .tags footer a:hover {\n      color: #999999; }\n\n#tag_cloud {\n  margin-bottom: 1em; }\n\n.gist {\n  font-size: 13px;\n  line-height: 1.6; }\n\n.like-wrapper {\n  color: #999; }\n  .like-wrapper .like-button, .like-wrapper .unlike-button {\n    font-family: \"FontAwesome\";\n    padding-right: 5px; }\n  .like-wrapper .like-button:hover {\n    color: #dd1144; }\n  .like-wrapper .like-button.liked:hover {\n    color: #999; }\n","source":"media/css/style.css","raw":"@charset \"UTF-8\";\n/*------------------------------------\n * sext-vi\n *\n * @author linghua.zhang@me.com\n * @link http://lhzhang.com/\n * @update 2014-03-17\n *\n * |/ | (~ (~\n * |\\.|._)._)\n * --------------------------------- */\n@import url(/media/fonts/telex-regular.css);\n@import url(http://fonts.googleapis.com/css?family=Ubuntu:300);\n* {\n  margin: 0;\n  padding: 0; }\n\nbody {\n  font-size: 14px;\n  font-family: \"telexregular\", \"Hiragino Sans GB\", \"Microsoft YaHei\", sans-serif;\n  color: #555;\n  background-color: #f9f9f9; }\n\nol, ul {\n  list-style-position: inside; }\n\nli {\n  padding: .2em 0; }\n\nhr {\n  width: 4em;\n  border: none;\n  border-top: 1px dashed #d0d0d0;\n  border-bottom: 1px dashed #f9f9f9; }\n\n.center {\n  text-align: center; }\n\n::selection {\n  color: #fff;\n  background-color: #333; }\n\na, header #header h1 a, article .title a, article .meta .tags a, article .divider a {\n  text-decoration: none;\n  color: #bb2222; }\n  a:hover, header #header h1 a:hover, article .title a:hover, article .meta .tags a:hover, article .divider a:hover {\n    color: #dd1144; }\n\nblockquote, pre code {\n  background-color: #f8f8f8;\n  padding: 0 1em;\n  border: 1px dashed #eee !important;\n  border-color: #e0e0e0 !important;\n  font-size: 13px;\n  line-height: 1.6;\n  display: block;\n  overflow: auto; }\n\ncode {\n  background-color: #f8f8f8;\n  padding: 2px 5px;\n  border: 1px dashed #e0e0e0;\n  font-family: \"Ubuntu\", monospace; }\n\nh1, h2 {\n  font-size: 1.4em; }\n\ntime {\n  font-family: \"Ubuntu\", monospace; }\n\nheader {\n  width: 800px;\n  margin: 0 auto;\n  text-align: justify;\n  margin: 3em auto; }\n  header:after {\n    content: '';\n    width: 100%;\n    display: inline-block; }\n  header #header {\n    display: inline-block;\n    position: relative;\n    top: 1em; }\n    header #header h1 {\n      font-family: \"telexregular\", \"Hiragino Sans GB\", \"Microsoft YaHei\", sans-serif; }\n      header #header h1 a {\n        color: #a9a9a9;\n        display: block; }\n  header nav {\n    display: inline-block;\n    position: relative;\n    top: 1em; }\n    header nav span {\n      margin: .5em; }\n    header nav a, header nav #header h1 a, header #header h1 nav a, header nav article .title a, article .title header nav a, header nav article .meta .tags a, article .meta .tags header nav a, header nav article .divider a, article .divider header nav a {\n      font-family: \"telexregular\", \"Hiragino Sans GB\", \"Microsoft YaHei\", sans-serif;\n      color: #ccc; }\n\n#content {\n  width: 800px;\n  margin: 0 auto;\n  line-height: 2em; }\n  #content .main-listing {\n    margin-bottom: 2em; }\n\nul.listing {\n  margin-top: 1em; }\n  ul.listing li {\n    list-style-type: none;\n    padding: 0; }\n    ul.listing li.listing-item a, ul.listing li.listing-item header #header h1 a, header #header h1 ul.listing li.listing-item a {\n      padding: .2em 0 .2em 2em; }\n    ul.listing li.listing-item time {\n      color: #999; }\n    ul.listing li.listing-item:hover {\n      background-color: #f9f9f9; }\n    ul.listing li.listing-seperator {\n      font-family: \"telexregular\", \"Hiragino Sans GB\", \"Microsoft YaHei\", sans-serif; }\n      ul.listing li.listing-seperator:before {\n        content: \"⭠ \";\n        color: #ccc; }\n\narticle {\n  margin: 2em 0; }\n  article p {\n    padding: .7em 0; }\n  article .title {\n    font-family: \"telexregular\", \"Hiragino Sans GB\", \"Microsoft YaHei\", sans-serif; }\n    article .title header #header h1 a, header #header h1 article .title a, article .title a, article .title .meta .tags a, article .meta .tags .title a {\n      color: #777; }\n  article .meta {\n    display: block;\n    overflow: auto;\n    margin-top: -.5em;\n    font-size: .9em; }\n    article .meta .tags header #header h1 a, header #header h1 article .meta .tags a, article .meta .tags a {\n      color: #999999;\n      padding: .25em; }\n  article .post img {\n    max-width: 800px;\n    display: block;\n    margin: .5em auto; }\n  article .comment {\n    margin: 3em 0; }\n  article .divider {\n    margin: 2em 0; }\n    article .divider i {\n      margin: 0 2em;\n      color: #e9e9e9; }\n    article .divider header #header h1 a, header #header h1 article .divider a, article .divider .meta .tags a, article .meta .tags .divider a, article .divider a {\n      font-family: \"telexregular\", \"Hiragino Sans GB\", \"Microsoft YaHei\", sans-serif;\n      margin: 0 2em; }\n      article .divider header #header h1 a i, header #header h1 article .divider a i, article .divider .meta .tags a i, article .meta .tags .divider a i, article .divider a i {\n        color: #999999;\n        margin: 0; }\n        article .divider header #header h1 a i:hover, header #header h1 article .divider a i:hover, article .divider a i:hover {\n          color: #bb2222 !important; }\n  article .divider {\n    position: relative;\n    font-size: 1em;\n    z-index: 1;\n    overflow: hidden;\n    text-align: center; }\n  article .divider:before, article .divider:after {\n    position: absolute;\n    top: 51%;\n    overflow: hidden;\n    width: 49%;\n    height: 2px;\n    content: '\\a0';\n    background-color: #f0f0f0; }\n  article .divider:before {\n    margin-left: -50%;\n    text-align: right; }\n  article .divider:after {\n    margin-left: 1%; }\n\nfooter {\n  width: 800px;\n  margin: 0 auto;\n  display: block;\n  color: #909090;\n  font-family: \"telexregular\", \"Hiragino Sans GB\", \"Microsoft YaHei\", sans-serif;\n  font-size: .9em;\n  text-align: center;\n  margin: 1em auto; }\n  footer a, footer header #header h1 a, header #header h1 footer a, footer article .title a, article .title footer a, footer article .meta .tags a, article .meta .tags footer a, footer article .divider a, article .divider footer a {\n    color: #cccccc; }\n    footer a:hover, footer header #header h1 a:hover, header #header h1 footer a:hover, footer article .meta .tags a:hover, article .meta .tags footer a:hover {\n      color: #999999; }\n\n#tag_cloud {\n  margin-bottom: 1em; }\n\n.gist {\n  font-size: 13px;\n  line-height: 1.6; }\n\n.like-wrapper {\n  color: #999; }\n  .like-wrapper .like-button, .like-wrapper .unlike-button {\n    font-family: \"FontAwesome\";\n    padding-right: 5px; }\n  .like-wrapper .like-button:hover {\n    color: #dd1144; }\n  .like-wrapper .like-button.liked:hover {\n    color: #999; }\n","date":"2015-11-07T06:11:08.000Z","updated":"2015-01-20T14:11:45.000Z","path":"media/css/style.css","layout":"false","title":"","comments":1,"_id":"cigortt9q000akfy7ipxxwt27"},{"title":"about","layout":"page","_content":"\nschool\n\n> 华中科技大学信息存储及应用实验室\n\ncorp\n\n> 目前于网易存储技术团队担任对象存储oss（NOS）开发\n\nfind me\n\n> weibo: http://weibo.com/u/1527291873\n\n> gmail: work.jlsun@gmail.com\n\n> github: https://github.com/work-jlsun\n\n\n","source":"about/index.md","raw":"---\ntitle: about\nlayout: page\n---\n\nschool\n\n> 华中科技大学信息存储及应用实验室\n\ncorp\n\n> 目前于网易存储技术团队担任对象存储oss（NOS）开发\n\nfind me\n\n> weibo: http://weibo.com/u/1527291873\n\n> gmail: work.jlsun@gmail.com\n\n> github: https://github.com/work-jlsun\n\n\n","date":"2015-11-07T07:00:38.000Z","updated":"2015-11-07T07:00:38.000Z","path":"about/index.html","comments":1,"_id":"cigotd38z000073y7jbxglyv3"}],"Post":[{"title":"Nginx And Go Http 并发性能","date":"2014-01-20T07:27:57.000Z","layout":"post","_content":"\n\n\n## 1 测试硬件\n\n```\nIntel(R) Xeon(R) CPU           X3440  @ 2.53GHz\ncpu cache size\t: 8192 KB\nDRAM：8G\n```\n\n## 2 测试软件\n\n```\n2.6.32-5-amd64 #1 SMP\nnginx：ngx_openresty-1.4.3.4 \ngo :go version go1.3.3 linux/amd64\nab：ApacheBench, Version 2.3 \n```\n\n## 3 测试配置\n---------\n\n### 3.1  一些内核配置\n\n```\n/proc/sys/fs/file-max                    3145728\n/proc/sys/fs/nr_open                     1048576\n/proc/sys/net/core/netdev_max_backlog    1000\n/proc/sys/net/core/rmem_max              131071\n/proc/sys/net/core/wmem_max              131071\n/proc/sys/net/core/somaxconn             128\n/proc/sys/net/ipv4/ip_forward            0\n/proc/sys/net/ipv4/ip_local_port_range   8192\t65535\n/proc/sys/net/ipv4/tcp_fin_timeout       60\n/proc/sys/net/ipv4/tcp_keepalive_time    7200\n/proc/sys/net/ipv4/tcp_max_syn_backlog   2048\n/proc/sys/net/ipv4/tcp_max_tw_buckets    1048576\n/proc/sys/net/ipv4/tcp_no_metrics_save   0\n/proc/sys/net/ipv4/tcp_syn_retries       5\n/proc/sys/net/ipv4/tcp_synack_retries    5\n/proc/sys/net/ipv4/tcp_tw_recycle        0\n/proc/sys/net/ipv4/tcp_tw_reuse          0\n/proc/sys/vm/min_free_kbytes             11489\n/proc/sys/vm/overcommit_memory           0\n```\n\n### 3.2 Nginx\n\n```\nworker_processes  8;\nevents {\n    worker_connections  2046;\n    use epoll;\n}\nhttp {\n    include       mime.types;\n    default_type  application/octet-stream;\n    location / {\n            root   html;\n            index  index.html index.htm;\n    }\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html {\n            root   html;\n    }\n    location /ab-test {\n        \tproxy_http_version 1.1;\n        \tproxy_set_header Connection \"\"; \n        \tcontent_by_lua 'ngx.print(\"aaa---here omit other char a, total 512--- aaaaaa\")';    \n    }\n}\n```\n\n### 3.3 golang 测试代码\n\n[go512server.go][3]\n\n\n## 4 测试方法\n\n### 4.1 测试工具及命令\n\n使用 ab测试不同并发场景下nginx和golang http 服务的性能，测试数据大小512Byte。\n\n测试命令示例：ab -n 1000000 -c 5000  -k  \"http://127.0.0.1:8081/512b\"\n\n(ps: 所有测试结果，都是3次之后取平均值)\n\n\n### 4.2  测试结果\n\n\n* 短连接场景\n\n并发请求量 | 100 | 200 | 500 | 1000 | 2000 | 5000\n----|-----|----|----|------|----\nnginx(tps) | 12741.62   | 12598.08     |  11917.15   |  12016.63 | 11640.36   |   6047.29\ngo（tps）  |  11310.32   | 11208.87     | 10731.40    |  10757.3  | 10750.26   |     10869.80\n\nps： 端连接情况下 并发5000 情况下， nginx情况不知道是为什么（nginx进程cpu利用看起来不是很均衡）\n\n\n* 长连接场景\n\n并发请求量 | 100 | 200 | 500 | 1000 | 2000 | 5000\n----|-----|----|----|------|----\nnginx（tps）    |     61249.81   |     60672.71  |   59548.39  |   55287.55    |           58375.65    |     60662.44\ngo（tps）    |    55257.64       |     53288.23 |   49006.64  |  46362.55  |             48042.18      |    47855.02\n\n\n* golang + nginx （golang as proxy）\n\n并发请求量 | 100 | 200 | 500 | 1000 | 2000 | 5000\n----|-----|----|----|------|----\ngo（tps）  |   31535.37   | 29081.96  |  30250.24  |   28921.48  |  26631.12  |    25333.64\n\n1: [golang  proxy 代码][3]\n\n2: golang作为proxy的时候性能基本为非proxy的一半左右，这个是可以理解的，一个请求的响应时间就是nginx + go两层的响应时间。\n\n3: 使用golang自带的httpclient连接后端的nginx\n\n\n* nginx + golang （nginx as proxy）\n\n并发请求量 | 100 | 200 | 500 | 1000 | 2000 | 5000\n----|-----|----|----|------|----\nTPS | 43336.19    | 41722.05   | 37984.94  |  34033.42  |  29489.74  |         25693.03\n\n\n* golang proxy简单稳定性测试\n\n```\n5000 并发测试 30 分钟，tps 达到24751，基本没有因为随着时间的增长而对性能造成很大的影响，资源使用也比较稳定\n\n1000并发测试 1小时 ， tsp达到 27651.10，基本没有因为随着时间的增长而对性能造成很大的影响，资源使用比较稳定\n```\n\n## 5 测试结论\n\n\n基于以上4中场景，上百组测试下，得出一下简单结论\n\n* golang表现还是较为出色，相比于标杆Nginx性能差20%左右\n* golang在高并发压力测试下稳定性还是不错，可以接受的\n  \n当前我们基于简单测试环境下的测试验证golang，现实环境远远比测试环境复杂，后续我们会在NosMedia 开发测试上线过程中不断总结经验。\n\n## 6 参考资料\n\n* [golang test][1] \n* [nginx-lua vs golang][2]\n\n\n[1]: https://gist.github.com/hgfischer/7965620\n[2]: http://blog.lifeibo.com/blog/2013/01/28/ngx-lua-and-go.html\n[3]: https://github.com/work-jlsun/golang/blob/develop/go512server.go\n[4]: https://github.com/work-jlsun/golang/blob/develop/goproxytest.go\n\n\n## 7 坑\n1. goang http 长连接问题\n2.  Connection reset  by peer (104)\n","source":"_posts/2014-01-20-golang-vs-nginx-test.markdown","raw":"---\ntitle: 'Nginx And Go Http 并发性能'\ndate: 2014-01-20 15:27:57\nlayout: post\ntags:\n    - golang\n    - nginx\n    - performance\n---\n\n\n\n## 1 测试硬件\n\n```\nIntel(R) Xeon(R) CPU           X3440  @ 2.53GHz\ncpu cache size\t: 8192 KB\nDRAM：8G\n```\n\n## 2 测试软件\n\n```\n2.6.32-5-amd64 #1 SMP\nnginx：ngx_openresty-1.4.3.4 \ngo :go version go1.3.3 linux/amd64\nab：ApacheBench, Version 2.3 \n```\n\n## 3 测试配置\n---------\n\n### 3.1  一些内核配置\n\n```\n/proc/sys/fs/file-max                    3145728\n/proc/sys/fs/nr_open                     1048576\n/proc/sys/net/core/netdev_max_backlog    1000\n/proc/sys/net/core/rmem_max              131071\n/proc/sys/net/core/wmem_max              131071\n/proc/sys/net/core/somaxconn             128\n/proc/sys/net/ipv4/ip_forward            0\n/proc/sys/net/ipv4/ip_local_port_range   8192\t65535\n/proc/sys/net/ipv4/tcp_fin_timeout       60\n/proc/sys/net/ipv4/tcp_keepalive_time    7200\n/proc/sys/net/ipv4/tcp_max_syn_backlog   2048\n/proc/sys/net/ipv4/tcp_max_tw_buckets    1048576\n/proc/sys/net/ipv4/tcp_no_metrics_save   0\n/proc/sys/net/ipv4/tcp_syn_retries       5\n/proc/sys/net/ipv4/tcp_synack_retries    5\n/proc/sys/net/ipv4/tcp_tw_recycle        0\n/proc/sys/net/ipv4/tcp_tw_reuse          0\n/proc/sys/vm/min_free_kbytes             11489\n/proc/sys/vm/overcommit_memory           0\n```\n\n### 3.2 Nginx\n\n```\nworker_processes  8;\nevents {\n    worker_connections  2046;\n    use epoll;\n}\nhttp {\n    include       mime.types;\n    default_type  application/octet-stream;\n    location / {\n            root   html;\n            index  index.html index.htm;\n    }\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html {\n            root   html;\n    }\n    location /ab-test {\n        \tproxy_http_version 1.1;\n        \tproxy_set_header Connection \"\"; \n        \tcontent_by_lua 'ngx.print(\"aaa---here omit other char a, total 512--- aaaaaa\")';    \n    }\n}\n```\n\n### 3.3 golang 测试代码\n\n[go512server.go][3]\n\n\n## 4 测试方法\n\n### 4.1 测试工具及命令\n\n使用 ab测试不同并发场景下nginx和golang http 服务的性能，测试数据大小512Byte。\n\n测试命令示例：ab -n 1000000 -c 5000  -k  \"http://127.0.0.1:8081/512b\"\n\n(ps: 所有测试结果，都是3次之后取平均值)\n\n\n### 4.2  测试结果\n\n\n* 短连接场景\n\n并发请求量 | 100 | 200 | 500 | 1000 | 2000 | 5000\n----|-----|----|----|------|----\nnginx(tps) | 12741.62   | 12598.08     |  11917.15   |  12016.63 | 11640.36   |   6047.29\ngo（tps）  |  11310.32   | 11208.87     | 10731.40    |  10757.3  | 10750.26   |     10869.80\n\nps： 端连接情况下 并发5000 情况下， nginx情况不知道是为什么（nginx进程cpu利用看起来不是很均衡）\n\n\n* 长连接场景\n\n并发请求量 | 100 | 200 | 500 | 1000 | 2000 | 5000\n----|-----|----|----|------|----\nnginx（tps）    |     61249.81   |     60672.71  |   59548.39  |   55287.55    |           58375.65    |     60662.44\ngo（tps）    |    55257.64       |     53288.23 |   49006.64  |  46362.55  |             48042.18      |    47855.02\n\n\n* golang + nginx （golang as proxy）\n\n并发请求量 | 100 | 200 | 500 | 1000 | 2000 | 5000\n----|-----|----|----|------|----\ngo（tps）  |   31535.37   | 29081.96  |  30250.24  |   28921.48  |  26631.12  |    25333.64\n\n1: [golang  proxy 代码][3]\n\n2: golang作为proxy的时候性能基本为非proxy的一半左右，这个是可以理解的，一个请求的响应时间就是nginx + go两层的响应时间。\n\n3: 使用golang自带的httpclient连接后端的nginx\n\n\n* nginx + golang （nginx as proxy）\n\n并发请求量 | 100 | 200 | 500 | 1000 | 2000 | 5000\n----|-----|----|----|------|----\nTPS | 43336.19    | 41722.05   | 37984.94  |  34033.42  |  29489.74  |         25693.03\n\n\n* golang proxy简单稳定性测试\n\n```\n5000 并发测试 30 分钟，tps 达到24751，基本没有因为随着时间的增长而对性能造成很大的影响，资源使用也比较稳定\n\n1000并发测试 1小时 ， tsp达到 27651.10，基本没有因为随着时间的增长而对性能造成很大的影响，资源使用比较稳定\n```\n\n## 5 测试结论\n\n\n基于以上4中场景，上百组测试下，得出一下简单结论\n\n* golang表现还是较为出色，相比于标杆Nginx性能差20%左右\n* golang在高并发压力测试下稳定性还是不错，可以接受的\n  \n当前我们基于简单测试环境下的测试验证golang，现实环境远远比测试环境复杂，后续我们会在NosMedia 开发测试上线过程中不断总结经验。\n\n## 6 参考资料\n\n* [golang test][1] \n* [nginx-lua vs golang][2]\n\n\n[1]: https://gist.github.com/hgfischer/7965620\n[2]: http://blog.lifeibo.com/blog/2013/01/28/ngx-lua-and-go.html\n[3]: https://github.com/work-jlsun/golang/blob/develop/go512server.go\n[4]: https://github.com/work-jlsun/golang/blob/develop/goproxytest.go\n\n\n## 7 坑\n1. goang http 长连接问题\n2.  Connection reset  by peer (104)\n","slug":"golang-vs-nginx-test","published":1,"updated":"2015-11-07T07:29:41.000Z","comments":1,"photos":[],"link":"","_id":"cigortt870000kfy7posfs523"},{"title":"Facebook图片服务堆栈浅析","layout":"post","_content":"\n此文 [An Analysis of Facebook Photo Caching](http://www.cs.cornell.edu/~qhuang/papers/sosp_fbanalysis.pdf)是facebook发表在OSDI 2010年的文章Finding a needle in Haystack: facebook’s photo storage的后续。 分析facabook图片系统的整个堆栈, 通过trace 1, 000, 000张不同的照片上77,000,000次访问。 总结了访问的traffic pattern, cache access pattern, geolocation of clients and servers等规律, 并探索了照片内容和它被访问模式之间的关系。 以下为仔细阅读此文的一些公司内部分享总结文档。\n\n\n目录\n\n1. 整体框架\n2. 采样方法(略)\n3. Workload分析\n4.  替换算法优化提升点\n5. 不同地理上的负载特性分析\n6. Eage cache 协作缓存理论上的优势\n7. 社交网络分析(略)\n8. 大总结\n9. NOS借鉴点\n\n\n### 1 整体架构\n![storagearch.jpg](/media/files/2015/09/facebookcache.jpg)\n\n* browser浏览器缓存\n* Edge缓存\n\t* 相当于CDN，包括自建的Facebook Edge节点以及第三方的Akamai节点\n\t* brower中的fetch path指定了请求指向哪个具体的Edge cache节点\n\t* in-memory hash table存索引 + flash 存储数据 \n\t* 使用FIFO淘汰算法\n* Origin缓存：\n\t* 在Edge节点MISS情况下请求发送到Origin层次\n\t* Origin缓存在整个图片系统中是逻辑上统一的一个层次\n\t* Edge节点到Origin的映射是通过Unique photo Id  进行hash 结合hash map的方式进行映射的\n\t* 同样采用in-memory hash table存索引 + flash 存储数据 \n\t* 使用FIFO淘汰算法\n* Haystack\n\t* origin缓存和Haystack缓存是紧密结合的，origin在Miss通常在Haystack里面直接找到\n\t* 在对应haystack集群“负载过高”/“系统故障”/“其他因素”等情况下，origin会从local replica获取，如果还是不行，从remote data center获取（灾备节点）\n\t* 使用campact blob存储方式（日志式存储），索引放内存，使得一次图片获取操作只有一次磁盘IO\n* 缓存层次的关键动机\n\t* 使用Edge cached的主要目的：节省Edge和Origin Data center 之间的带宽（费用啊费用，成本很关键）\n\t* 其他缓存的主要目的：减小底层Haystack存储的压力，主要是I/O。\n\nURL的组成：unique photo identifier +  display dimensions of image  + fetch path；\n\nfetch path: which specifies where a request that misses at each layer of cache should be directed next:指定在miss情况下这个请求是指向Akamai CDN还是Facebook自身的Edge节点\n\n### 2 Methodology[略]\n\n### 3 WorkLoad分析\n\n#### 3.1 分析使用的workload\n* 使用70TB数据\n* client-initiated requests\n* 1 month\n* 77M 请求\n* 13.2M user\n* 1.3M unique photo\n\n#### 3.2 基本统计\n![1.png](/media/files/2015/10/1.png)\n\n\n分析结果：\n\n* 请求分布：65.5% brower serve、20% Edge cache serve、4.6% Origin cache serve、 9.9% Haystack serve\n* 流量分布：Edge cache 241.6GB、Origin cache 63.4GB、187.2GB从Haystack获取(resize前达到456.5GB)(ps:http缓存是整个体系的，千万别小看用户浏览器的缓存行为)。\n* 命中率分布：Inside Brower 65.5%、Edge cache 58.0%、Origin cache 31.8% \n* 出口ip：Edge cache 24个，origin cache 4个。（24个自建CDN大节点，4个Origincache/HayStack集群）\n* 图片大小分布，小于256K,缩略前47%对象小于32K，缩略后80%对象小于32K。\n\n![2.png](/media/media/files/2015/10/2.png)\n       \n#### 3.3 Popularity Distribution\n分析在不同缓存层次下对象的Popularity\n\n![3.png](/media/files/2015/10/3.png)\n\n* 上图X轴为请求数的排名，Y轴为请求，体现的就是对应排名的请求的重复数目\n* 可以看到对象的Popularity 呈现zipf分布，缓存层次越往下，zipf的a(阿尔法)因子不断变小，体现出来就是分布更加稳定，热点越来越不明显。\n\n#### 3.4 Hit Ratio\n分析Popularity 和Hit Ratio的联系。\n![4.png](/media/files/2015/10/4.png)\n\n* 图a：在一周内请求在不同缓存层次的被服务的比例\n* 图b：A-G不同Popularity 的photo在不同缓存层次被服务的比例（Popularity 排名越往后被后面的缓存层次服务的比例不断变大）\n* 图c\n\t* A-G不同Popularity 的photo在各个缓存层次中的命中率。\n\t* A B两类的对象在Edge层次和Origin的层次的命中率远远高于Brower \n\t* E G则相反，在Browser层次的命中率远远高于其他两个层次\n\t* 在Brower层次B比C的命中率更加低\n\n### 4 替换算法的优化提升点\n#### 4.1 浏览器缓存\n\n![5.png](/media/files/2015/10/5.png)\n\n* 使用一个月中的25%的trace对缓存进行预热，使用剩余的75%的trace进行测试\n* Client activity group越大，即用户越活跃，其命中率越高（很明显:用户越活跃访问同一内容的概率越高）\n* 考虑浏览器在“容量无限”/“容量无限+resize本地化”情况下的理论命中率最大值\n\t* 对于不是很活跃的用户命中率提高的幅度非常小，只是提升2.6%到了41.8%\n\n#### 4.2 Edge Cache\n\n##### 4.2.1 理论命中率最大值\n\n![6.png](/media/files/2015/10/6.png)\n\n* 在“容量无限”/(“容量无限+Resize本地化”)情况下，各个Edge节点命中率提升情况测试理论命中率最大值\n\n##### 4.2.2 不同替换算法影响\n\n![7.png](/media/files/2015/10/7.png)\n\n* FIFO：先进先出\n* LRU：Least Recently Used，最近最少使用\n* LFU：Least Frequently Used，最不经常使用\n* S4LRU：4级LRU缓存，0-3级，（在cache Miss情况下，插入0级队列的头部，在cache Hit的情况下，上移到上一级的头部：即2级上移到3级别头部，3级只能上移到3级别）\n* Clairvoyant：千里眼算法，最佳替换算法（理论最佳算法）\n* Infinite：（缓存无限大）\n   \nps:(LRU和LFU的区别。LFU算法是根据在一段时间里数据项被使用的次数选择出最少使用的数据项，即根据使用次数的差异来决定。而LRU是根据使用时间的差异来决定的。)\n\n* 选取 San Jose Edge cache进行测试\n* San Jose实际命中率在59.2%，算法FIFO\n* 模拟不同算法情况下的对象命中率\n\t* FIFO：59.2%\n\t* LFU：+2%\n\t* LRU：+3.6%\n\t* S4LRU：+8.5%（减少20%的下行流量）\n\t* Clairvoyant:77.3%(当前缓存空间情况下的理论最大值，与当前实际差18.1%（44.4%的下行流量）)\n\t* Infinite:84.3%\n* 模拟不同算法情况下的byte命中率\n\t* S4LRU：+5.3%（在Edge和Origin之间减少10%的带宽）\n\t* ps：当前edge cache的主要作用不是traffic sheltering，而是为了节省带宽\n\n##### 4.2.3 增加缓存容量的影响\n* 在缓存容量加倍情况下\n\t* 对象命中率\n\t\t* FIFO：+5.8%\n\t\t* LFU：+5.6%\n\t\t* LRU：+5.7%\n\t\t* S4LRU：+4.3%\n\t* byte命中率\n\t\t* FIFO：+4.8%\n\t\t* LFU：+6.4%\n\t\t* LRU：+4.8%\n\t\t* S4LRU：+4.2%\n\n##### 4.2.4 使用不同缓存算法达到当前实际命中需要的缓存空间\n* FIFO：   1     *   X\n* LFU：    0.8  *   X\n* LRU：   0.7   *   X\n* S4LRU：0.3  *   X\n结论：\n* 对静态内容的缓存提供可两大可行的建议\n\t* 在边缘节点投入精力进行算法调优可以大幅度减少后端的带宽\n\t* 可以使用更小的缓存空间达到当前的命中率\n\n#### 4.3 Origin Cache\n\n同样使用edge cache的测试方法进行测试。\n\n![8.png](/media/files/2015/10/8.png)\n\n##### 4.3.1 替换算法影响\n* FIFO：\n* LFU： +9.8%\n* LRU：+4.7%\n* S4LRU：+13.9% （节省Backend Disk-IO operation 20.7%）\n\n##### 4.3.2 增加一倍缓存容量的影响\n* FIFO：+9.5%\n* S4LRU：+8.5%(命中率达到54.4%，相比于当前缓存大小和FIFO情况下减少31.9%的后端IO)\n### 5 Geographic  Traffic Distribution\n\n#### 5.1 Client To Eage Cache Traffic\n\n![9.png](/media/files/2015/10/9.png)\n\n* 每个城市的请求都被9个不同区域的Edge cache 节点服务(显然跨越东西海岸延时肯定要变大)\n* 大部分的请求都路由到离自己近的Edge cache节点\n* [例外],Atlanta的大部分请求的大头是由D.C.的Eage cache节点服务\n   \n原因：请求的路由策略是结合latency、Edge Cache容量、Edge Cache当前的负载、ISP厂商价格等诸多的因素计算的而综合计算得出的最佳的DNS策略。(FaceBook DNS)\n\n#### 5.2 Edge Cache to Origin Cache Traffic\n  Edge cache 到Orgin cache底层存储的路由访问特征。\n\n![10.png](/media/files/2015/10/10.png)\n\n* 4个origin cache 节点（4个数据中心接收数据的upload服务）（orgin cache和haystack是部署在一个数据中心的）\n* Edge Cache到Origin Cache的路由策略是基于photo id进行hash的\n* 和Edge cache不同Origin Cache只有全局唯一的一个，和地理分布无关，只跟photo内容相关\n* 9个 Edge节点回源到4个origin cache的比例基本都是一样的。\n\n\n#### 5.3 Cross-Region Traffic at Backend\n\nOrgin cache 到Haystack底层存储的路由访问特征。\n\n![11.png](/media/files/2015/10/11.png)\n\n\n* Origin cache Miss的请求大部分请求98%左右都是直接路由到同一本区域（数据中心）内\n\t* 小部分请求会路由到别的区域（数据中心内）\n\t* 前端变化的路由策略导致不可避免的会出现一些误差\n\t* 本地节点故障offline或者overload的时候会路由到remote节点\n\nOrgin cache 到Haystack底层存储的访问时间分布\n\n![12.png](/media/files/2015/10/12.png)\n\n\n* 大部分请求在50ms左右时间内完成\n* 100ms和3s出现2个拐点\n\t* 100ms为路由出错情况下跨不同区域机房的延时\n\t* 3s对应与在本地副本失败情况下max-timeout(3s)之后到remote节点获取的延时\n\n### 6. Eage cache 协作缓存理论上的优势\n\n![13.png](/media/files/2015/10/13.png)\n\n* 把所有的Edge缓存当做一个独立的缓存，进行协作缓存\n* 协作统一缓存之后各个算法的命中率提升\n\t* FIFO：+17%\n\t* S4LRU：+16.6%（相比于当前独立缓存的FIFO算法方案，S4LRU方案命中率提升21.9%，减少42%的回源量）\n* (ps：个人觉得，把Edge缓存之间建立高速专用网络这种方案才靠谱)\n\n### 7 社交网络分析\n\nphoto的流行特性，跟图片的age即social-networking metrics 有很大的关联性。\n\n\n大结论\n\n* Facebook 图片traffic的整体的分布：65.5% browser cache, 20.0% Edge Cache, 4.6% Origin Cache, and 9.9% Backend storage\n* 由于负载均衡等策略的影响有相当一部分的请求route到距离较远edge节点\n* 模拟测试表明在edge和Origin缓存层次使用S4LRU淘汰算法可能非常有益\n* photo的流行特性，跟图片的age及social-networking metrics 有很大的关联性\n\nNOS(Netease Object Storage)借鉴点\n\n* origin cache，减小 disk IO：对于易信这样的独立服务，对小对象跟产品一起合作做一层Origin Cache缓存，能够大量减小 disk IO，减少对NOS及SDFS层次压力，减少不少成本。\n* 多级缓存：缓存替换算法，使用类似S4LRU算法，或者说类似于多层次IO进一步提高命中率，减小后端I/O压力\n* 数据说话，精细化运维并指导开发，最大程度减低成本。\n\n\n\n\n\n\n","source":"_posts/2015-10-09-facebook_photo_caching.markdown","raw":"---\ntitle: 'Facebook图片服务堆栈浅析'\nlayout: post\ntags:\n    - 云存储\n    - CDN\n---\n\n此文 [An Analysis of Facebook Photo Caching](http://www.cs.cornell.edu/~qhuang/papers/sosp_fbanalysis.pdf)是facebook发表在OSDI 2010年的文章Finding a needle in Haystack: facebook’s photo storage的后续。 分析facabook图片系统的整个堆栈, 通过trace 1, 000, 000张不同的照片上77,000,000次访问。 总结了访问的traffic pattern, cache access pattern, geolocation of clients and servers等规律, 并探索了照片内容和它被访问模式之间的关系。 以下为仔细阅读此文的一些公司内部分享总结文档。\n\n\n目录\n\n1. 整体框架\n2. 采样方法(略)\n3. Workload分析\n4.  替换算法优化提升点\n5. 不同地理上的负载特性分析\n6. Eage cache 协作缓存理论上的优势\n7. 社交网络分析(略)\n8. 大总结\n9. NOS借鉴点\n\n\n### 1 整体架构\n![storagearch.jpg](/media/files/2015/09/facebookcache.jpg)\n\n* browser浏览器缓存\n* Edge缓存\n\t* 相当于CDN，包括自建的Facebook Edge节点以及第三方的Akamai节点\n\t* brower中的fetch path指定了请求指向哪个具体的Edge cache节点\n\t* in-memory hash table存索引 + flash 存储数据 \n\t* 使用FIFO淘汰算法\n* Origin缓存：\n\t* 在Edge节点MISS情况下请求发送到Origin层次\n\t* Origin缓存在整个图片系统中是逻辑上统一的一个层次\n\t* Edge节点到Origin的映射是通过Unique photo Id  进行hash 结合hash map的方式进行映射的\n\t* 同样采用in-memory hash table存索引 + flash 存储数据 \n\t* 使用FIFO淘汰算法\n* Haystack\n\t* origin缓存和Haystack缓存是紧密结合的，origin在Miss通常在Haystack里面直接找到\n\t* 在对应haystack集群“负载过高”/“系统故障”/“其他因素”等情况下，origin会从local replica获取，如果还是不行，从remote data center获取（灾备节点）\n\t* 使用campact blob存储方式（日志式存储），索引放内存，使得一次图片获取操作只有一次磁盘IO\n* 缓存层次的关键动机\n\t* 使用Edge cached的主要目的：节省Edge和Origin Data center 之间的带宽（费用啊费用，成本很关键）\n\t* 其他缓存的主要目的：减小底层Haystack存储的压力，主要是I/O。\n\nURL的组成：unique photo identifier +  display dimensions of image  + fetch path；\n\nfetch path: which specifies where a request that misses at each layer of cache should be directed next:指定在miss情况下这个请求是指向Akamai CDN还是Facebook自身的Edge节点\n\n### 2 Methodology[略]\n\n### 3 WorkLoad分析\n\n#### 3.1 分析使用的workload\n* 使用70TB数据\n* client-initiated requests\n* 1 month\n* 77M 请求\n* 13.2M user\n* 1.3M unique photo\n\n#### 3.2 基本统计\n![1.png](/media/files/2015/10/1.png)\n\n\n分析结果：\n\n* 请求分布：65.5% brower serve、20% Edge cache serve、4.6% Origin cache serve、 9.9% Haystack serve\n* 流量分布：Edge cache 241.6GB、Origin cache 63.4GB、187.2GB从Haystack获取(resize前达到456.5GB)(ps:http缓存是整个体系的，千万别小看用户浏览器的缓存行为)。\n* 命中率分布：Inside Brower 65.5%、Edge cache 58.0%、Origin cache 31.8% \n* 出口ip：Edge cache 24个，origin cache 4个。（24个自建CDN大节点，4个Origincache/HayStack集群）\n* 图片大小分布，小于256K,缩略前47%对象小于32K，缩略后80%对象小于32K。\n\n![2.png](/media/media/files/2015/10/2.png)\n       \n#### 3.3 Popularity Distribution\n分析在不同缓存层次下对象的Popularity\n\n![3.png](/media/files/2015/10/3.png)\n\n* 上图X轴为请求数的排名，Y轴为请求，体现的就是对应排名的请求的重复数目\n* 可以看到对象的Popularity 呈现zipf分布，缓存层次越往下，zipf的a(阿尔法)因子不断变小，体现出来就是分布更加稳定，热点越来越不明显。\n\n#### 3.4 Hit Ratio\n分析Popularity 和Hit Ratio的联系。\n![4.png](/media/files/2015/10/4.png)\n\n* 图a：在一周内请求在不同缓存层次的被服务的比例\n* 图b：A-G不同Popularity 的photo在不同缓存层次被服务的比例（Popularity 排名越往后被后面的缓存层次服务的比例不断变大）\n* 图c\n\t* A-G不同Popularity 的photo在各个缓存层次中的命中率。\n\t* A B两类的对象在Edge层次和Origin的层次的命中率远远高于Brower \n\t* E G则相反，在Browser层次的命中率远远高于其他两个层次\n\t* 在Brower层次B比C的命中率更加低\n\n### 4 替换算法的优化提升点\n#### 4.1 浏览器缓存\n\n![5.png](/media/files/2015/10/5.png)\n\n* 使用一个月中的25%的trace对缓存进行预热，使用剩余的75%的trace进行测试\n* Client activity group越大，即用户越活跃，其命中率越高（很明显:用户越活跃访问同一内容的概率越高）\n* 考虑浏览器在“容量无限”/“容量无限+resize本地化”情况下的理论命中率最大值\n\t* 对于不是很活跃的用户命中率提高的幅度非常小，只是提升2.6%到了41.8%\n\n#### 4.2 Edge Cache\n\n##### 4.2.1 理论命中率最大值\n\n![6.png](/media/files/2015/10/6.png)\n\n* 在“容量无限”/(“容量无限+Resize本地化”)情况下，各个Edge节点命中率提升情况测试理论命中率最大值\n\n##### 4.2.2 不同替换算法影响\n\n![7.png](/media/files/2015/10/7.png)\n\n* FIFO：先进先出\n* LRU：Least Recently Used，最近最少使用\n* LFU：Least Frequently Used，最不经常使用\n* S4LRU：4级LRU缓存，0-3级，（在cache Miss情况下，插入0级队列的头部，在cache Hit的情况下，上移到上一级的头部：即2级上移到3级别头部，3级只能上移到3级别）\n* Clairvoyant：千里眼算法，最佳替换算法（理论最佳算法）\n* Infinite：（缓存无限大）\n   \nps:(LRU和LFU的区别。LFU算法是根据在一段时间里数据项被使用的次数选择出最少使用的数据项，即根据使用次数的差异来决定。而LRU是根据使用时间的差异来决定的。)\n\n* 选取 San Jose Edge cache进行测试\n* San Jose实际命中率在59.2%，算法FIFO\n* 模拟不同算法情况下的对象命中率\n\t* FIFO：59.2%\n\t* LFU：+2%\n\t* LRU：+3.6%\n\t* S4LRU：+8.5%（减少20%的下行流量）\n\t* Clairvoyant:77.3%(当前缓存空间情况下的理论最大值，与当前实际差18.1%（44.4%的下行流量）)\n\t* Infinite:84.3%\n* 模拟不同算法情况下的byte命中率\n\t* S4LRU：+5.3%（在Edge和Origin之间减少10%的带宽）\n\t* ps：当前edge cache的主要作用不是traffic sheltering，而是为了节省带宽\n\n##### 4.2.3 增加缓存容量的影响\n* 在缓存容量加倍情况下\n\t* 对象命中率\n\t\t* FIFO：+5.8%\n\t\t* LFU：+5.6%\n\t\t* LRU：+5.7%\n\t\t* S4LRU：+4.3%\n\t* byte命中率\n\t\t* FIFO：+4.8%\n\t\t* LFU：+6.4%\n\t\t* LRU：+4.8%\n\t\t* S4LRU：+4.2%\n\n##### 4.2.4 使用不同缓存算法达到当前实际命中需要的缓存空间\n* FIFO：   1     *   X\n* LFU：    0.8  *   X\n* LRU：   0.7   *   X\n* S4LRU：0.3  *   X\n结论：\n* 对静态内容的缓存提供可两大可行的建议\n\t* 在边缘节点投入精力进行算法调优可以大幅度减少后端的带宽\n\t* 可以使用更小的缓存空间达到当前的命中率\n\n#### 4.3 Origin Cache\n\n同样使用edge cache的测试方法进行测试。\n\n![8.png](/media/files/2015/10/8.png)\n\n##### 4.3.1 替换算法影响\n* FIFO：\n* LFU： +9.8%\n* LRU：+4.7%\n* S4LRU：+13.9% （节省Backend Disk-IO operation 20.7%）\n\n##### 4.3.2 增加一倍缓存容量的影响\n* FIFO：+9.5%\n* S4LRU：+8.5%(命中率达到54.4%，相比于当前缓存大小和FIFO情况下减少31.9%的后端IO)\n### 5 Geographic  Traffic Distribution\n\n#### 5.1 Client To Eage Cache Traffic\n\n![9.png](/media/files/2015/10/9.png)\n\n* 每个城市的请求都被9个不同区域的Edge cache 节点服务(显然跨越东西海岸延时肯定要变大)\n* 大部分的请求都路由到离自己近的Edge cache节点\n* [例外],Atlanta的大部分请求的大头是由D.C.的Eage cache节点服务\n   \n原因：请求的路由策略是结合latency、Edge Cache容量、Edge Cache当前的负载、ISP厂商价格等诸多的因素计算的而综合计算得出的最佳的DNS策略。(FaceBook DNS)\n\n#### 5.2 Edge Cache to Origin Cache Traffic\n  Edge cache 到Orgin cache底层存储的路由访问特征。\n\n![10.png](/media/files/2015/10/10.png)\n\n* 4个origin cache 节点（4个数据中心接收数据的upload服务）（orgin cache和haystack是部署在一个数据中心的）\n* Edge Cache到Origin Cache的路由策略是基于photo id进行hash的\n* 和Edge cache不同Origin Cache只有全局唯一的一个，和地理分布无关，只跟photo内容相关\n* 9个 Edge节点回源到4个origin cache的比例基本都是一样的。\n\n\n#### 5.3 Cross-Region Traffic at Backend\n\nOrgin cache 到Haystack底层存储的路由访问特征。\n\n![11.png](/media/files/2015/10/11.png)\n\n\n* Origin cache Miss的请求大部分请求98%左右都是直接路由到同一本区域（数据中心）内\n\t* 小部分请求会路由到别的区域（数据中心内）\n\t* 前端变化的路由策略导致不可避免的会出现一些误差\n\t* 本地节点故障offline或者overload的时候会路由到remote节点\n\nOrgin cache 到Haystack底层存储的访问时间分布\n\n![12.png](/media/files/2015/10/12.png)\n\n\n* 大部分请求在50ms左右时间内完成\n* 100ms和3s出现2个拐点\n\t* 100ms为路由出错情况下跨不同区域机房的延时\n\t* 3s对应与在本地副本失败情况下max-timeout(3s)之后到remote节点获取的延时\n\n### 6. Eage cache 协作缓存理论上的优势\n\n![13.png](/media/files/2015/10/13.png)\n\n* 把所有的Edge缓存当做一个独立的缓存，进行协作缓存\n* 协作统一缓存之后各个算法的命中率提升\n\t* FIFO：+17%\n\t* S4LRU：+16.6%（相比于当前独立缓存的FIFO算法方案，S4LRU方案命中率提升21.9%，减少42%的回源量）\n* (ps：个人觉得，把Edge缓存之间建立高速专用网络这种方案才靠谱)\n\n### 7 社交网络分析\n\nphoto的流行特性，跟图片的age即social-networking metrics 有很大的关联性。\n\n\n大结论\n\n* Facebook 图片traffic的整体的分布：65.5% browser cache, 20.0% Edge Cache, 4.6% Origin Cache, and 9.9% Backend storage\n* 由于负载均衡等策略的影响有相当一部分的请求route到距离较远edge节点\n* 模拟测试表明在edge和Origin缓存层次使用S4LRU淘汰算法可能非常有益\n* photo的流行特性，跟图片的age及social-networking metrics 有很大的关联性\n\nNOS(Netease Object Storage)借鉴点\n\n* origin cache，减小 disk IO：对于易信这样的独立服务，对小对象跟产品一起合作做一层Origin Cache缓存，能够大量减小 disk IO，减少对NOS及SDFS层次压力，减少不少成本。\n* 多级缓存：缓存替换算法，使用类似S4LRU算法，或者说类似于多层次IO进一步提高命中率，减小后端I/O压力\n* 数据说话，精细化运维并指导开发，最大程度减低成本。\n\n\n\n\n\n\n","slug":"facebook_photo_caching","published":1,"date":"2015-10-08T16:00:00.000Z","updated":"2015-10-10T14:04:00.000Z","comments":1,"photos":[],"link":"","_id":"cigortt9t000ckfy7xnslj86s"},{"title":"又拍云交流","layout":"post","_content":"\n\n团队邀请[又拍云](https://www.upyun.com/index.html)团队CTO（黄慧攀）& COO（沈志华） 来公司进行一次交流。主要涉及到CDN、云存储、以及未来的工作方向相关的讨论，以下简单纪录一些point。\n\n### 1 存储\n\n* 1.1 数据\n\n又拍云的数据量大概在5个P的样子，跟我们网易云存储差不多在一个规模，我们大概逻辑存储量在1.5P左右的样子，物理存储量在3.5P的样子。ps:CDN是其核心业务，存储并不是。\n\n\n* 1.2 技术\n\n\t* 存储引擎\n\n\t又拍云从2010开始到现在底层存储系统经过几次的技术选型，刚开始是[mongofs](http://www.phpclasses.org/package/6086-PHP-Store-and-get-data-in-MongoDB-GridFS-like-files.html),到[ceph](http://ceph.com/)，以及当前自己的分布式文件系统。当前mongofs基本已经淘汰，而ceph和其自研的存储系统都可能还都在使用，当前是主要启用自研的文件系统，ceph中不再新增加新数据。\n\n\n\t* EC\n \n\t又拍当前没有用EC，基本采用三备份方式，在启用灾备份情况下就是6倍分，这主要跟他们自己产品的卖点跟成本上考虑有关。因为存储成本相对于流量成本相对比较小，而且本身接业务的时候会主要偏向于接流量型的业务(😄，主要还是靠CDN赚钱)，而并不是大量冷数据，访问很少的业务。\n\n\t* 存储架构\n \n\t一般对象存储会选择将对象元信息和对象数据分别保存到不同的系统里面，对象元信息可以选择存放在关系型数据库，而对象数据存储于底层其他kv存储引擎。而##又拍##选择了将对象元信息和对象一起存放(meta文件)，都存放在存储引擎的方式，距离如下图所示。\n![storagearch.jpg](/media/files/2015/09/storagearch.jpg)\n\t\n\t这两种方式各有各的优缺点\n\t统一存储：统一设计考虑数据和元数据高可用，高可靠，分布式策略,相比数据元数据大的小往往是比较小的，所以相比而言元数据会得到非常好的分布特性，元数据不易成为瓶颈。但是在应用灵活性方面相对会比较差。\n\n\t分离存储：需要分别考虑设计元数据系统和数据系统，（ps：统一使用同一套存储引擎，元数据数据还是还是分离的方法还是另说）,并且一般来说元数据是比较集中的，所以机遇元数据的操作和方案设计相对会比较简单，比如统计、标记删除过期清理等等。\n\n\n* 1.3 其他\n\n\t对##机房存储容量## 的问题进行了一次讨论，我们云存储团队在2014年由于机房容量的问题做过一次存储机器的整体搬迁（ps：需要做同城物理运输），存储系统架构对副本数据的设计是比较集中的，所以搬迁过程还算比较简单，以副本为单位进行搬迁。但是对运维来说还是存在不少的挑战，因为设计的时候并没有考虑到机房搬迁等运维事务。而跟又拍云交流过程中，我们也找到了另外一种思路。\n\n\t一般来说，就云存储而言，应该会基于当前的存储容量和增长趋势做容量规划，比如机房够用1～2年，那么在机房资源紧张的情况下，启用新的机房，针对几个量比较大的产品，要求使换一个桶，使用另外一个新机房的存储空间（bucket）。\n\n\n### 2 CDN\n\nCDN当前是又拍业务的核心、计算图片处理，视频处理非核心；应该说与七牛云存储是不同的发展方向，七牛是围绕数据做计算，但是，CDN是外包的，其核心是存储＋数据计算。\n\n* 2.1 points\n\n以下为一些关键技术点。\n\n1. 130多个边缘区域，10多个二级区域，3级就是又拍的存储机房，服务器大概几千台。\n2. 二级节点主要选择的是一些多线(BGP)的资源，因为会牵涉到不同线的用户源站。\n3. 评估选点三级区域节点选择相对比较简单，利用听云等服务商测量服务器ping值等信息，进而选择相对优秀的节点。\n4. IP库选择，目前使用的是IPIP的付费IP库。\n5. 可用带宽大概子1T。\n6. 三级节点到二级节点的路由是自动选择的。\n7. 上传加速依赖于同一套系统，通过边缘节点进行流式代理。\n8. 在CDN当前这种日趋白热化的竞争状态下，又拍表示还是有一定利润空间的。\n9. CDN的防攻击，针对CDN的攻击，攻击几个点是比较容易的，但是攻击整个CDN事比较困难的，(ps:CloudFlare CDN技术支持AnyCast协议可以使得CDN攻击流量分散到所有节点而大大提升抗攻击能力）\n10. 又拍CDN团队大概是10多号人，存储团队5个人，阿里云CDN团队30多号人.\n\n-\n\n* 2.2 整体架构\n\n其实又拍云的架构跟Facebook的图片系统还是比较类似的，中心机房之前其实有一套[ATS](http://trafficserver.apache.org/)通用缓存，一来是作为缩略图的缓存，另外由于支持多机房策略，Origin会从别的storage机房获取数据，缓存有利于较少跨机房的数据复制(Ps:由于前两层CDN缓存，一般到请求到origin的请求量相对较少，命中率相对比较低了，20%左右)\n\n![storagearch.jpg](/media/files/2015/09/facebookcache.jpg)\n\n\n* 2.3 其他\n\nCDN技术应该算现在都已经算是比较完善的技术了。所以又拍的技术团队也在思考能不能基于CDN做更多的差异化和创新。正在酝酿基于CDN做分布式区域服务，比如抢红包业务，可以将红包服务分散到各个区域的边缘节点。当然这个服务需要一定的技术支持，当前就这种服务形态也在做技术选型。一种是当前比较火的docker技术，另一种是希望构建语言级别的虚拟机（类似于SAE？）。个人认为docker容器技术对于人家的应用场景相对还是比较重，所以选择后者可能更佳合适。","source":"_posts/2015-09-23-youpai_communicate.markdown","raw":"---\ntitle: '又拍云交流'\nlayout: post\ntags:\n    - 交流\n    - 云存储、cdn\n---\n\n\n团队邀请[又拍云](https://www.upyun.com/index.html)团队CTO（黄慧攀）& COO（沈志华） 来公司进行一次交流。主要涉及到CDN、云存储、以及未来的工作方向相关的讨论，以下简单纪录一些point。\n\n### 1 存储\n\n* 1.1 数据\n\n又拍云的数据量大概在5个P的样子，跟我们网易云存储差不多在一个规模，我们大概逻辑存储量在1.5P左右的样子，物理存储量在3.5P的样子。ps:CDN是其核心业务，存储并不是。\n\n\n* 1.2 技术\n\n\t* 存储引擎\n\n\t又拍云从2010开始到现在底层存储系统经过几次的技术选型，刚开始是[mongofs](http://www.phpclasses.org/package/6086-PHP-Store-and-get-data-in-MongoDB-GridFS-like-files.html),到[ceph](http://ceph.com/)，以及当前自己的分布式文件系统。当前mongofs基本已经淘汰，而ceph和其自研的存储系统都可能还都在使用，当前是主要启用自研的文件系统，ceph中不再新增加新数据。\n\n\n\t* EC\n \n\t又拍当前没有用EC，基本采用三备份方式，在启用灾备份情况下就是6倍分，这主要跟他们自己产品的卖点跟成本上考虑有关。因为存储成本相对于流量成本相对比较小，而且本身接业务的时候会主要偏向于接流量型的业务(😄，主要还是靠CDN赚钱)，而并不是大量冷数据，访问很少的业务。\n\n\t* 存储架构\n \n\t一般对象存储会选择将对象元信息和对象数据分别保存到不同的系统里面，对象元信息可以选择存放在关系型数据库，而对象数据存储于底层其他kv存储引擎。而##又拍##选择了将对象元信息和对象一起存放(meta文件)，都存放在存储引擎的方式，距离如下图所示。\n![storagearch.jpg](/media/files/2015/09/storagearch.jpg)\n\t\n\t这两种方式各有各的优缺点\n\t统一存储：统一设计考虑数据和元数据高可用，高可靠，分布式策略,相比数据元数据大的小往往是比较小的，所以相比而言元数据会得到非常好的分布特性，元数据不易成为瓶颈。但是在应用灵活性方面相对会比较差。\n\n\t分离存储：需要分别考虑设计元数据系统和数据系统，（ps：统一使用同一套存储引擎，元数据数据还是还是分离的方法还是另说）,并且一般来说元数据是比较集中的，所以机遇元数据的操作和方案设计相对会比较简单，比如统计、标记删除过期清理等等。\n\n\n* 1.3 其他\n\n\t对##机房存储容量## 的问题进行了一次讨论，我们云存储团队在2014年由于机房容量的问题做过一次存储机器的整体搬迁（ps：需要做同城物理运输），存储系统架构对副本数据的设计是比较集中的，所以搬迁过程还算比较简单，以副本为单位进行搬迁。但是对运维来说还是存在不少的挑战，因为设计的时候并没有考虑到机房搬迁等运维事务。而跟又拍云交流过程中，我们也找到了另外一种思路。\n\n\t一般来说，就云存储而言，应该会基于当前的存储容量和增长趋势做容量规划，比如机房够用1～2年，那么在机房资源紧张的情况下，启用新的机房，针对几个量比较大的产品，要求使换一个桶，使用另外一个新机房的存储空间（bucket）。\n\n\n### 2 CDN\n\nCDN当前是又拍业务的核心、计算图片处理，视频处理非核心；应该说与七牛云存储是不同的发展方向，七牛是围绕数据做计算，但是，CDN是外包的，其核心是存储＋数据计算。\n\n* 2.1 points\n\n以下为一些关键技术点。\n\n1. 130多个边缘区域，10多个二级区域，3级就是又拍的存储机房，服务器大概几千台。\n2. 二级节点主要选择的是一些多线(BGP)的资源，因为会牵涉到不同线的用户源站。\n3. 评估选点三级区域节点选择相对比较简单，利用听云等服务商测量服务器ping值等信息，进而选择相对优秀的节点。\n4. IP库选择，目前使用的是IPIP的付费IP库。\n5. 可用带宽大概子1T。\n6. 三级节点到二级节点的路由是自动选择的。\n7. 上传加速依赖于同一套系统，通过边缘节点进行流式代理。\n8. 在CDN当前这种日趋白热化的竞争状态下，又拍表示还是有一定利润空间的。\n9. CDN的防攻击，针对CDN的攻击，攻击几个点是比较容易的，但是攻击整个CDN事比较困难的，(ps:CloudFlare CDN技术支持AnyCast协议可以使得CDN攻击流量分散到所有节点而大大提升抗攻击能力）\n10. 又拍CDN团队大概是10多号人，存储团队5个人，阿里云CDN团队30多号人.\n\n-\n\n* 2.2 整体架构\n\n其实又拍云的架构跟Facebook的图片系统还是比较类似的，中心机房之前其实有一套[ATS](http://trafficserver.apache.org/)通用缓存，一来是作为缩略图的缓存，另外由于支持多机房策略，Origin会从别的storage机房获取数据，缓存有利于较少跨机房的数据复制(Ps:由于前两层CDN缓存，一般到请求到origin的请求量相对较少，命中率相对比较低了，20%左右)\n\n![storagearch.jpg](/media/files/2015/09/facebookcache.jpg)\n\n\n* 2.3 其他\n\nCDN技术应该算现在都已经算是比较完善的技术了。所以又拍的技术团队也在思考能不能基于CDN做更多的差异化和创新。正在酝酿基于CDN做分布式区域服务，比如抢红包业务，可以将红包服务分散到各个区域的边缘节点。当然这个服务需要一定的技术支持，当前就这种服务形态也在做技术选型。一种是当前比较火的docker技术，另一种是希望构建语言级别的虚拟机（类似于SAE？）。个人认为docker容器技术对于人家的应用场景相对还是比较重，所以选择后者可能更佳合适。","slug":"youpai_communicate","published":1,"date":"2015-09-22T16:00:00.000Z","updated":"2015-10-01T12:45:32.000Z","comments":1,"photos":[],"link":"","_id":"cigortt9v000hkfy7hx6kv4ab"},{"title":"go runtime 1.4 之 内存分配器","layout":"post","_content":"\n\n\n机缘巧合，参加了gopher china 2015，见识到各位大牛在go领域的实践经验，会上《go 学习笔记》作者“雨痕”在会上分享了go语言runtime的核心实现，应该是很难得的高质量的关于go runtime的分享了，并且ppt做得非常有品，应该是花了非常大的精力的，感谢@雨痕大侠的分享。本文就@雨痕的分享结合自己的理解谈谈go runtime 1.4。\n\n\ngo runtime 的三大核心组件为：\n \n* Memory Allocator 内存分配器\n* Garbage Collector 垃圾回收器\n* Goroutine Scheduler 协程调度器\n \n \n抛开语法go语言语法层面的设计，程序写完之后放到服务器上跑，基本就是这三大核心组建接替完成接下来所有的工作。\n \n\n## 1 Go 1.4 Runtime 内存分配器\n\ngo语言内存分配并没有使用linux系统原生的内存分配器，而是基于Google 自家的tcmalloc 结合自身的gc系统的设计重新实现了一遍，以实现更加高效的内存分配，回收等，实现内存的自主管理、缓存复用以及无锁分配。\n\n操作系统的内存管理的基本单元都是以页（4K、8K），go 内存分配的基础是基于页的span（块），即多个地址连续的页组合成一个span，如下图所示：\n\n\n![span.jpg](/media/files/2015/06/span.jpg)\n\n\n页、Span是用来管理大块的内存的，不适合给对象分配内存的，所以在给对象分配内存的时候需要对大块内存进行切分。在go语言中对象以32K为边界把内存分为大小两种\n\n![smallandbig.jpg](/media/files/2015/06/smallandbig.jpg)\n\n\n* 大对象：对于大块内存的申请和回收都不可以称之为碎片，没必要对其做特别的优化，另外一个原因是，对于大多数程序而言，大对象非常少，并且生命周期长，一般来说生命周期跟程序的生命周期是一样的，一直在复用的，直接放在heap托管即可。\n\n* 小对象：几十字节，几百字节，这种分配很容易导致内存碎片话，因为从1字节到32K有非常多的大小规格，所以直接按照其所需要的字节进行分配容易导致内存的复用率会非常差。\n\n所以核心来说，内存分配器主要都是对小对象的分配进行优化。多数内存分配起都会选择按照8字节对齐分成n种等级的方式进行内存分配，因为处理器，指针，内存地址，还有结构题的对齐都是按照8，16对齐或者作为基本处理单元的。\n\n小对象按照8字节对齐之后，32K大小的数据其实分成非常有限的几种。(ps：小于1K的按照8字节对齐，而大于1K的跳跃会比较大一点)\n\n![sizeclass.jpg](/media/files/2015/06/sizeclass.jpg)\n\n\n上面描述的内容是描述如何把内存(原始材料)根据不同的需求和大小划分为不同的大小等级，以下三级结构描述了如何对这些划分的材料进行合理的管理。三级结构示意图如下\n\n\n![treelevel.jpg](/media/files/2015/06/treelevel.jpg)\n\n* heap\n\nheap层次做两件事情，在程序内存不够的时候向OS申请内存，并管理空闲的span,全局只有一个heap，所以这层锁事很重的。\n\n* central\n\n小对象不会直接向堆heap申请内存，而是计算小对象的大小所属的size class，然后从对应的central上批量获取小对象，如果对应central已经分配完，会从heap获取span，把对应span内存全部切分成相同大小的size class，然后进行分配使用，并且管理未全部回收的span。从central层次获取也是要加锁了，但是相对heap而言，锁的粒度被分散到不同的central等级了。\n\n* cache\n\n从central获取一批已经切分好的大小相等的对象链表，与运行时线程绑定，所有从cache上获取对象是无锁分配。（在并发编程中，最重要的事锁，锁的粒度控制得不好，会导致性能急剧下降）\n\n\n上面介绍了内存的维度切分和基本的管理模式，但是这一套逻辑和算法(包括后续说的GC)，依赖连续地址，所以内存分配器在初始化的时候需要预留比较大的地址空间，如下所示：\n\n![init.jpg](/media/files/2015/06/init.jpg)\n\n\n在初学go语言的时候，写一个hello world，发现这个程序的vm占用有130多G，实际上现代操作系统任何一个进程看到的实际上都是虚拟地址(VA)，虚拟地址只有通过MMU的映射才能够分配实际的内存。在64位系统上面，是可以非常奢侈的使用PB级别的虚拟地址。\n\ngo的内存分配器在初始化的时候预留了一个很大的虚拟地址空间，以后所有的内存的分配使用的地址都使用这段地址，因为垃圾回收器和内存分配器依赖于连续地址。这样可以把这段地址用数组的方式进行管理（在寻址上最快的）\n\n* arena\n\n所有的对象都在arena地址上进行分配，go 1.4最大能够分配的是128G，所有服务器物理内存超过128G时是没有意义的，可以启多个程序实例的方式。\n\n* bitmap\n\n为arena区域每个地址分配4bit的管理位，用户垃圾回收\n\n\t当前内存是否在做垃圾回收\t\n\t是否是可达对象\n\t当前分配是否为“数组”\n\t当前分配是否为“指针”\n\t\n* spans\n\n用于具体的一个object反查对应所属的span块\n\n> 分析一次内存分配\n\n![malloc.jpg](/media/files/2015/06/malloc.jpg)\n\n分析完以上内容之后，看一次完整的内存分配流程。\n对于大对象，直接从堆（heap）上分配，对于小对象，比如 go程序中执行  myObject :＝ &MyObject{},那么首先会从本线程对应的cache区域查找是否\n存在对应size class的内存，如果存在则获取成功，失败，则查找central上是否存在对应size class的内存，如果存在，则获取一批缓存到自身的cache链表中，以备下次使用，如果不存在则需要直接向heap申请内存，heap中有空余的span可以分配，则直接分配给central，然后切分给cache，然后使用，如果heap也没有空余可使用的内存空间，则需要向OS申请内存，1MB起步，64K为最小的递增申请粒度。\n\n\n> 分析内存回收过程\n\n![sweep.jpg](/media/files/2015/06/sweep.jpg)\n\n通过GC扫描之后(后续章节会介绍)标记为不可达的对象是可以回收的，如果是可回收的，并且是大对象，则直接归还给heap(堆)，如果是小对象，则查找小对象对应的span，对应span的ref为0，即对应span上所有对象都在使用，则归还给 heap，如果还有空余对象可以使用，则可以继续放入central继续使用。\n\n\n\n\n\n\n\n\n","source":"_posts/2015-06-26-go-runtime-1.4.markdown","raw":"---\ntitle: 'go runtime 1.4 之 内存分配器'\nlayout: post\ntags:\n    - golang\n    - runtime\n---\n\n\n\n机缘巧合，参加了gopher china 2015，见识到各位大牛在go领域的实践经验，会上《go 学习笔记》作者“雨痕”在会上分享了go语言runtime的核心实现，应该是很难得的高质量的关于go runtime的分享了，并且ppt做得非常有品，应该是花了非常大的精力的，感谢@雨痕大侠的分享。本文就@雨痕的分享结合自己的理解谈谈go runtime 1.4。\n\n\ngo runtime 的三大核心组件为：\n \n* Memory Allocator 内存分配器\n* Garbage Collector 垃圾回收器\n* Goroutine Scheduler 协程调度器\n \n \n抛开语法go语言语法层面的设计，程序写完之后放到服务器上跑，基本就是这三大核心组建接替完成接下来所有的工作。\n \n\n## 1 Go 1.4 Runtime 内存分配器\n\ngo语言内存分配并没有使用linux系统原生的内存分配器，而是基于Google 自家的tcmalloc 结合自身的gc系统的设计重新实现了一遍，以实现更加高效的内存分配，回收等，实现内存的自主管理、缓存复用以及无锁分配。\n\n操作系统的内存管理的基本单元都是以页（4K、8K），go 内存分配的基础是基于页的span（块），即多个地址连续的页组合成一个span，如下图所示：\n\n\n![span.jpg](/media/files/2015/06/span.jpg)\n\n\n页、Span是用来管理大块的内存的，不适合给对象分配内存的，所以在给对象分配内存的时候需要对大块内存进行切分。在go语言中对象以32K为边界把内存分为大小两种\n\n![smallandbig.jpg](/media/files/2015/06/smallandbig.jpg)\n\n\n* 大对象：对于大块内存的申请和回收都不可以称之为碎片，没必要对其做特别的优化，另外一个原因是，对于大多数程序而言，大对象非常少，并且生命周期长，一般来说生命周期跟程序的生命周期是一样的，一直在复用的，直接放在heap托管即可。\n\n* 小对象：几十字节，几百字节，这种分配很容易导致内存碎片话，因为从1字节到32K有非常多的大小规格，所以直接按照其所需要的字节进行分配容易导致内存的复用率会非常差。\n\n所以核心来说，内存分配器主要都是对小对象的分配进行优化。多数内存分配起都会选择按照8字节对齐分成n种等级的方式进行内存分配，因为处理器，指针，内存地址，还有结构题的对齐都是按照8，16对齐或者作为基本处理单元的。\n\n小对象按照8字节对齐之后，32K大小的数据其实分成非常有限的几种。(ps：小于1K的按照8字节对齐，而大于1K的跳跃会比较大一点)\n\n![sizeclass.jpg](/media/files/2015/06/sizeclass.jpg)\n\n\n上面描述的内容是描述如何把内存(原始材料)根据不同的需求和大小划分为不同的大小等级，以下三级结构描述了如何对这些划分的材料进行合理的管理。三级结构示意图如下\n\n\n![treelevel.jpg](/media/files/2015/06/treelevel.jpg)\n\n* heap\n\nheap层次做两件事情，在程序内存不够的时候向OS申请内存，并管理空闲的span,全局只有一个heap，所以这层锁事很重的。\n\n* central\n\n小对象不会直接向堆heap申请内存，而是计算小对象的大小所属的size class，然后从对应的central上批量获取小对象，如果对应central已经分配完，会从heap获取span，把对应span内存全部切分成相同大小的size class，然后进行分配使用，并且管理未全部回收的span。从central层次获取也是要加锁了，但是相对heap而言，锁的粒度被分散到不同的central等级了。\n\n* cache\n\n从central获取一批已经切分好的大小相等的对象链表，与运行时线程绑定，所有从cache上获取对象是无锁分配。（在并发编程中，最重要的事锁，锁的粒度控制得不好，会导致性能急剧下降）\n\n\n上面介绍了内存的维度切分和基本的管理模式，但是这一套逻辑和算法(包括后续说的GC)，依赖连续地址，所以内存分配器在初始化的时候需要预留比较大的地址空间，如下所示：\n\n![init.jpg](/media/files/2015/06/init.jpg)\n\n\n在初学go语言的时候，写一个hello world，发现这个程序的vm占用有130多G，实际上现代操作系统任何一个进程看到的实际上都是虚拟地址(VA)，虚拟地址只有通过MMU的映射才能够分配实际的内存。在64位系统上面，是可以非常奢侈的使用PB级别的虚拟地址。\n\ngo的内存分配器在初始化的时候预留了一个很大的虚拟地址空间，以后所有的内存的分配使用的地址都使用这段地址，因为垃圾回收器和内存分配器依赖于连续地址。这样可以把这段地址用数组的方式进行管理（在寻址上最快的）\n\n* arena\n\n所有的对象都在arena地址上进行分配，go 1.4最大能够分配的是128G，所有服务器物理内存超过128G时是没有意义的，可以启多个程序实例的方式。\n\n* bitmap\n\n为arena区域每个地址分配4bit的管理位，用户垃圾回收\n\n\t当前内存是否在做垃圾回收\t\n\t是否是可达对象\n\t当前分配是否为“数组”\n\t当前分配是否为“指针”\n\t\n* spans\n\n用于具体的一个object反查对应所属的span块\n\n> 分析一次内存分配\n\n![malloc.jpg](/media/files/2015/06/malloc.jpg)\n\n分析完以上内容之后，看一次完整的内存分配流程。\n对于大对象，直接从堆（heap）上分配，对于小对象，比如 go程序中执行  myObject :＝ &MyObject{},那么首先会从本线程对应的cache区域查找是否\n存在对应size class的内存，如果存在则获取成功，失败，则查找central上是否存在对应size class的内存，如果存在，则获取一批缓存到自身的cache链表中，以备下次使用，如果不存在则需要直接向heap申请内存，heap中有空余的span可以分配，则直接分配给central，然后切分给cache，然后使用，如果heap也没有空余可使用的内存空间，则需要向OS申请内存，1MB起步，64K为最小的递增申请粒度。\n\n\n> 分析内存回收过程\n\n![sweep.jpg](/media/files/2015/06/sweep.jpg)\n\n通过GC扫描之后(后续章节会介绍)标记为不可达的对象是可以回收的，如果是可回收的，并且是大对象，则直接归还给heap(堆)，如果是小对象，则查找小对象对应的span，对应span的ref为0，即对应span上所有对象都在使用，则归还给 heap，如果还有空余对象可以使用，则可以继续放入central继续使用。\n\n\n\n\n\n\n\n\n","slug":"go-runtime-1.4","published":1,"date":"2015-06-25T16:00:00.000Z","updated":"2015-06-27T05:37:15.000Z","comments":1,"photos":[],"link":"","_id":"cigortt9w000mkfy7k3w7mqum"},{"title":"goroutine contiguous stack","layout":"post","_content":"\n在我们学习关于golang goroutine的文章时，或多或少很多类似的断言：相比于linux pthread，在 golang中我们可以很轻松的创建100k＋的goroutine，而不用担心其带来的开销，其中一个原因是goroutine初始stack非常小，在当前release的1.3 版本中，一个goroutine初试创建只需要4K 的stack，而linux pthead 则需要2M或者更多的stack空间，那到底是不是这样的？\n\n如下在一个进程中创建100个线程，主进程和线程sleep的方式简单测试下pthread 线程初试创建占用的内存资源。\n\n    void* func(void* arg){ \n        while (true){ \n             usleep(1000000); \n        } \n    }\n    int main(int argc, char* argv[]) {\n        pthread_t tid; \n        int n = 10000; \n        while (n != 0) { \n             if (pthread_create(&tid, NULL, func, &n) != 0 ) {\n                 printf(\"create fail\"); \n             } \n             n = n - 1; \n        } \n        while (1) { usleep(10000000); } \n     } \n    \n\n通过ps看到的资源使用情况如下\n\n    USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND \n    12768    21828  4.9  1.0 81976776 83988 pts/3  Sl+  08:08   0:01 ./a.out\n    \n\n可以看到创建10000个线程之后，该程序实际占用的内存（RSS）为83988KB，算下来每个线程所占用的内存空间才8K左右，远远不是很多文章中所说的1M或者时2M or more。\n\n## pthread 线程堆栈\n\n通过strace分析phtread_create 得到如下结果\n\n    mmap(NULL, 8392704, \n        PROT_READ|PROT_WRITE,MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, \n        -1, 0) = 0x7fa274760000\n    brk(0)                                  = 0x1fd2000\n    brk(0x1ff3000)                          = 0x1ff3000\n    mprotect(0x7fa274760000, 4096, PROT_NONE) = 0\n    clone(child_stack=0x7fa274f5ffd0,\n         flags=CLONE_VM|CLONE_FS|CLONE_FILES|CLONE_SIGHAND|\n               CLONE_THREAD|CLONE_SYSVSEM|CLONE_SETTLS|\n               CLONE_PARENT_SETTID|CLONE_CHILD_CLEARTID,  \n         parent_tidptr=0x7fa274f609d0, tls=0x7fa274f60700,    \n         child_tidptr=0x7fa274f609d0) = 31316\n    \n\n我们可以看到，在调用pthread_create的时候，首先使用mmap分配了8392704 Byte（8196kB）堆栈空间，但是在创建线程的时候，如果不指定堆栈大小，理应使用系统定义的默认最大空间， 通过ulimit -s 可以看到值为8192kB\n\n    hzsunjianliang@inspur1:~/github/golang$ ulimit -s\n    8192\n    \n\nmmap多映射了4k（一页），可以看到在mmap之后又调用mprotect将堆栈尾部空间的权限设置为PROT_NONE即不可读写和执行，所以基本可以判断多mmap的1页内存空间主要是用于内存溢出情况下的检测。最后将mmap返回的堆栈作为clone的一个参数创建一个线程。 通过mmap分配的内存并不会直接映射为实际使用的物理内存空间，只有当实际使用的时候，在发现当前虚拟地址空间没有分配实际无力内存的情况下，会触发操作系统缺页中断从而再分配实际物理内存。\n\n## golang  contiguous stack 实现\n\ngoroutine作为golang的独立调度单元，每个goroutine能够独立运行的重要元素为其独立的栈空间，golang 1.2的实现类似于pthread，分配固定大小的空间，由于是固定的所以即不能太大也不能太小。而Go1.3 引入了 contiguous stack，可以在goroutine初试创建时分配非常小的栈空间（1.3为4k，后续1.5roadmap中说到会减到2k），在使用过程中自动进行增长和收缩。这使得我们可以在golang中创建很多很多的goroutine而不用担心内存耗尽。这激发我们编写各种各样的并发模型而不用太担心其可能对内存照成很大的开销。\n\n## 实现原理\n\ngolang在每次执行函数调用的时候，首先，其runtime会检测当前的栈空间是否足够使用，如果不够使用，会触发类似“缺页中断”，Go 的runtime会保存此事函数的上下文环境，然后malloc一块内存，将旧堆栈的内存copy到新的堆栈，并做一些合理的调整。当函数返回的时候，函数会在新的堆栈中继续运行，仿佛整个过程啥事都没发生过。所以理论上来说goroutine可以使用“无限大的堆栈空间”\n\n## 实现细节\n\nGo的运行库中，每个goroutine对应一个结构体G（类似于linux操作系统的中进程控制块），此结构中保存有stackbase 和stackguard用于定义其使用的栈信息，每次函数调用时候都会检测当前函数需要使用的栈空间是否够用，如果不够用就进行扩张。\n\n接下来我们分析golang的汇编代码进行分析\n\n    package main \n    import  \"fmt\"  \n    func main(){ \n        a := 1 \n        strb := \"hello \" \n        a = a + 1 \n        strb += \"world\" \n        fmt.Print(a, strb) \n        main()  \n    }\n    \n\ngo tool 6g -S continuousStack.go | head -8\n\n    \"\".main t=1 size=352 value=0 args=0 locals=0xb8\n    000000 00000 (continuousStack.go:5) TEXT         \"\".main+0(SB),$184-0\n    000000 00000 (continuousStack.go:5) MOVQ    (TLS),CX\n    0x0009 00009 (continuousStack.go:5) LEAQ    -56(SP),AX\n    0x000e 00014 (continuousStack.go:5) CMPQ    AX,(CX)\n    0x0011 00017 (continuousStack.go:5) JHI ,26\n    0x0013 00019 (continuousStack.go:5) CALL,runtime.morestack00_noctxt(SB)\n    0x0018 00024 (continuousStack.go:5) JMP ,0\n    0x001a 00026 (continuousStack.go:5) SUBQ    $184,SP\n    \n\n从上面可以看到，在进入main函数之后，首先从TLS中取得第一个字段，也就是g－>stackguard字段，然后将当前SP值减去函数预计将要使用的局部堆栈空间56byte，如果得到的值小于stackguard则表示当前栈空间不够使用，需要调用runtime.morestack分配更大的堆栈空间。\n\nmore：\\[连续栈\\]\\[1\\]\n\n## 参考资料\n\n1.  https://github.com/tiancaiamao/go-internals/blob/master/ebook/03.5.md\n2.  https://docs.google.com/document/d/1wAaf1rYoM4S4gtnPh0zOlGzWtrZFQ5suE8qr2sD8uWQ/pub\n3.  http://stackoverflow.com/questions/6270945/linux-stack-sizes\n4.  http://www.unix.com/unix-for-dummies-questions-and-answers/174134-kernel-stack-vs-user-mode-stack.html\n","source":"_posts/2014-10-20-goroutine-contiguous-stack.markdown","raw":"---\ntitle: 'goroutine contiguous stack'\nlayout: post\ntags:\n    - golang\n    - performance\n---\n\n在我们学习关于golang goroutine的文章时，或多或少很多类似的断言：相比于linux pthread，在 golang中我们可以很轻松的创建100k＋的goroutine，而不用担心其带来的开销，其中一个原因是goroutine初始stack非常小，在当前release的1.3 版本中，一个goroutine初试创建只需要4K 的stack，而linux pthead 则需要2M或者更多的stack空间，那到底是不是这样的？\n\n如下在一个进程中创建100个线程，主进程和线程sleep的方式简单测试下pthread 线程初试创建占用的内存资源。\n\n    void* func(void* arg){ \n        while (true){ \n             usleep(1000000); \n        } \n    }\n    int main(int argc, char* argv[]) {\n        pthread_t tid; \n        int n = 10000; \n        while (n != 0) { \n             if (pthread_create(&tid, NULL, func, &n) != 0 ) {\n                 printf(\"create fail\"); \n             } \n             n = n - 1; \n        } \n        while (1) { usleep(10000000); } \n     } \n    \n\n通过ps看到的资源使用情况如下\n\n    USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND \n    12768    21828  4.9  1.0 81976776 83988 pts/3  Sl+  08:08   0:01 ./a.out\n    \n\n可以看到创建10000个线程之后，该程序实际占用的内存（RSS）为83988KB，算下来每个线程所占用的内存空间才8K左右，远远不是很多文章中所说的1M或者时2M or more。\n\n## pthread 线程堆栈\n\n通过strace分析phtread_create 得到如下结果\n\n    mmap(NULL, 8392704, \n        PROT_READ|PROT_WRITE,MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, \n        -1, 0) = 0x7fa274760000\n    brk(0)                                  = 0x1fd2000\n    brk(0x1ff3000)                          = 0x1ff3000\n    mprotect(0x7fa274760000, 4096, PROT_NONE) = 0\n    clone(child_stack=0x7fa274f5ffd0,\n         flags=CLONE_VM|CLONE_FS|CLONE_FILES|CLONE_SIGHAND|\n               CLONE_THREAD|CLONE_SYSVSEM|CLONE_SETTLS|\n               CLONE_PARENT_SETTID|CLONE_CHILD_CLEARTID,  \n         parent_tidptr=0x7fa274f609d0, tls=0x7fa274f60700,    \n         child_tidptr=0x7fa274f609d0) = 31316\n    \n\n我们可以看到，在调用pthread_create的时候，首先使用mmap分配了8392704 Byte（8196kB）堆栈空间，但是在创建线程的时候，如果不指定堆栈大小，理应使用系统定义的默认最大空间， 通过ulimit -s 可以看到值为8192kB\n\n    hzsunjianliang@inspur1:~/github/golang$ ulimit -s\n    8192\n    \n\nmmap多映射了4k（一页），可以看到在mmap之后又调用mprotect将堆栈尾部空间的权限设置为PROT_NONE即不可读写和执行，所以基本可以判断多mmap的1页内存空间主要是用于内存溢出情况下的检测。最后将mmap返回的堆栈作为clone的一个参数创建一个线程。 通过mmap分配的内存并不会直接映射为实际使用的物理内存空间，只有当实际使用的时候，在发现当前虚拟地址空间没有分配实际无力内存的情况下，会触发操作系统缺页中断从而再分配实际物理内存。\n\n## golang  contiguous stack 实现\n\ngoroutine作为golang的独立调度单元，每个goroutine能够独立运行的重要元素为其独立的栈空间，golang 1.2的实现类似于pthread，分配固定大小的空间，由于是固定的所以即不能太大也不能太小。而Go1.3 引入了 contiguous stack，可以在goroutine初试创建时分配非常小的栈空间（1.3为4k，后续1.5roadmap中说到会减到2k），在使用过程中自动进行增长和收缩。这使得我们可以在golang中创建很多很多的goroutine而不用担心内存耗尽。这激发我们编写各种各样的并发模型而不用太担心其可能对内存照成很大的开销。\n\n## 实现原理\n\ngolang在每次执行函数调用的时候，首先，其runtime会检测当前的栈空间是否足够使用，如果不够使用，会触发类似“缺页中断”，Go 的runtime会保存此事函数的上下文环境，然后malloc一块内存，将旧堆栈的内存copy到新的堆栈，并做一些合理的调整。当函数返回的时候，函数会在新的堆栈中继续运行，仿佛整个过程啥事都没发生过。所以理论上来说goroutine可以使用“无限大的堆栈空间”\n\n## 实现细节\n\nGo的运行库中，每个goroutine对应一个结构体G（类似于linux操作系统的中进程控制块），此结构中保存有stackbase 和stackguard用于定义其使用的栈信息，每次函数调用时候都会检测当前函数需要使用的栈空间是否够用，如果不够用就进行扩张。\n\n接下来我们分析golang的汇编代码进行分析\n\n    package main \n    import  \"fmt\"  \n    func main(){ \n        a := 1 \n        strb := \"hello \" \n        a = a + 1 \n        strb += \"world\" \n        fmt.Print(a, strb) \n        main()  \n    }\n    \n\ngo tool 6g -S continuousStack.go | head -8\n\n    \"\".main t=1 size=352 value=0 args=0 locals=0xb8\n    000000 00000 (continuousStack.go:5) TEXT         \"\".main+0(SB),$184-0\n    000000 00000 (continuousStack.go:5) MOVQ    (TLS),CX\n    0x0009 00009 (continuousStack.go:5) LEAQ    -56(SP),AX\n    0x000e 00014 (continuousStack.go:5) CMPQ    AX,(CX)\n    0x0011 00017 (continuousStack.go:5) JHI ,26\n    0x0013 00019 (continuousStack.go:5) CALL,runtime.morestack00_noctxt(SB)\n    0x0018 00024 (continuousStack.go:5) JMP ,0\n    0x001a 00026 (continuousStack.go:5) SUBQ    $184,SP\n    \n\n从上面可以看到，在进入main函数之后，首先从TLS中取得第一个字段，也就是g－>stackguard字段，然后将当前SP值减去函数预计将要使用的局部堆栈空间56byte，如果得到的值小于stackguard则表示当前栈空间不够使用，需要调用runtime.morestack分配更大的堆栈空间。\n\nmore：\\[连续栈\\]\\[1\\]\n\n## 参考资料\n\n1.  https://github.com/tiancaiamao/go-internals/blob/master/ebook/03.5.md\n2.  https://docs.google.com/document/d/1wAaf1rYoM4S4gtnPh0zOlGzWtrZFQ5suE8qr2sD8uWQ/pub\n3.  http://stackoverflow.com/questions/6270945/linux-stack-sizes\n4.  http://www.unix.com/unix-for-dummies-questions-and-answers/174134-kernel-stack-vs-user-mode-stack.html\n","slug":"goroutine-contiguous-stack","published":1,"date":"2014-10-19T16:00:00.000Z","updated":"2015-03-25T22:47:24.000Z","comments":1,"photos":[],"link":"","_id":"cigortt9y000qkfy7h65cqx0k"},{"title":"goroutine 调度器","layout":"post","_content":"\n\n\n对golang的goroutine的内部实现感觉非常神秘，通过这段时间的学习基本了解了golang调度器实现原理。 \n\n#### golang调度器 \n\n虽然golang的最小调度单元为协程（goroutine），但是操作系统最小的调度单元依然还是线程，所以golang scheduler（golang调度器）其要做的工作是如何将众多的goroutine放在有限的线程上进行高效而公平的调度。 \n\n操作系统的调度不失为高效和公平，比如[CFS][1]调度算法，那么go为何要引入goroutine？原因很多，有人会说goroutine 相比于linux的pthread机制使用很方便。但是核心原因为goroutine的轻量级，无论是从进程到线程，还是从线程到协程，其核心都是为了使得我们的调度单元更加轻量级，我们可以轻易得创建几万几十万的goroutine而不用担心内存耗尽等问题。golang引入goroutine试图在语言内核层做到足够高性能得同时（充分利用多核优势、使用epoll高效处理网络／IO、实现垃圾回收等机制）尽量简化编程。\n\n ps：人类社会的发展是生产工具不断发展解放生产力的过程，语言的发展也是一样，从机器语言到汇编语言、C语言、面向对象C＋＋\\ Java以及到现在层出不穷的动态语言。编程语言作为生产工具，其发展核心目的为最大化IT工作人员的生产力。从这点看，我很看好go语言，极简得编程之道简直大爱。 \n \n 以下基于Daniel Morsing的[一篇文章][2]介绍goroutine调度器。 \n \n 首先基于线程，用户态协程可以选择以下3种调度机制。 \n \n \n**N:1**  即多个用户态协程运行在一个os线程上，这种方式的优点是可以很快得进行上下文切换，但是缺点是不能利用多核优势。 \n\n**1:1** 一个用户态协程对应一个os线程，这种方式得优点是可以利用到多核的优势，但是协程的调度完全依赖于os线程的调度，而os线程的调度的上线文切换的代价又比较大，从而导致这种模型调度的上下文切换代价比较大。 \n\n**M:N** golang scheduler 使用的m:n调度模型，即任意数量的用户态协程可以运行在任意数量的os线程上，这样不仅可以使得上线文切换更加轻量级，同时又可以充分利用多核优势。 为了实现这种调度机制，golang 引入如下3个大结构 <!--more-->\n\n![our-cast.jpg](/media/files/2014/09/24/our-cast.jpg)\n\nM：os线程（即操作系统内核提供的线程） \n\nG：goroutine，其包含了调度一个协程所需要的堆栈以及instruction pointer（IP指令指针），以及其他一些重要的调度信息。 \n\nP：M与P的中介，实现m:n 调度模型的关键，M必须拿到P才能对G进行调度，P其实限定了golang调度其的最大并发度。 \n\n![in-motion.jpg](/media/files/2014/09/24/in-motion.jpg)\n\n\n如上图所示，2个M分别拿到context P在运行G，M只有拿到context P才能执行goroutine。被执行的goroutine在运行过程中调用 go func() ，会创建一个新的对应func() 的goroutine，并将这个goruotine加入到runqueue（就绪待调度的goroutine队列，如上图灰色部分所示），当前运行的goroutine在达到调度点（系统调用、网络IO、等待channel等）的时候，P会挂起当前运行的goroutine，从runqueue中pop一个goroutine，重新设置当前M的执行上下文继续执行（即设置为pop出来的goroutine对应的运行堆栈以及IP（instruction Point））。\n\n仔细观察我们可以发现，与golang的前一个版本的调度器不同，当前并不是采用全局的runqueue队列，而是每个P都对应有自己的一个local的runqueue，这样可以避免每一次调度都进行一次锁竞争，在32核机器上，锁竞争会导致性能变得非常糟糕。 \n\nP的数量由用户设置的GOMAXPROCS决定，当前不论GOMAXPROCS设置为多大，P的数量最大为256。由于M必须拿到P才能够对G进行调度，所以P实际上限制了go的最大并发度，256对于现在的服务器已经足够使用了。因为P并不会由于当前调度的goroutine阻塞而不可用，只要当前还有可调度的goroutine，P始终会使用M继续进行调度运行。所以其实P只需要设置为当前CPU的最大核数即可。 \n\n如果一切正常，调度器会以这样一种方式简单得运行，以下分析goroutine在两种例外情况下的行为。 \n\n**系统调用** \n\n![syscall.jpg](/media/files/2014/09/24/syscall.jpg)\n\ngolang语言能够控制线程M在用户态执行情况下的调度行为，但是却不能控制线程在陷入内核之后的行为。即在我们调用system call陷入内核没有返回之前，go其实已经控制不了。那怎么办呢？如果当前陷入system call的线程在持有P的情况下Block很长时间，会导致其他可运行的goroutine由于没有可用的P而不能得到调度。\n\n 为了保证调度的并发型，即保证拿到P的M能够持续进行调度而不是block，golang 调度器也没有很好的办法，只能在进入系统调用之前从线程池拿一个线程或者新建一个线程的方式，将当前P交给新的线程M1从而使得runqueue中的goroutine可以继续进行调度。当前M0则block在system call中等待G0返回。 在G0返回之后需要继续运行，而继续运行的条件是必须拥有一个P，如果当前没有可用的P，则将G0放到全局的runqueue中等待调度，M0退出或者放入线程池睡觉去。而如果有空闲的P，M0在拿到P之后继续进行调度。（P的数量很好的控制了并发度） \n \n P在当前local runqueue中的G全部调度完之后从global runqueue中获取G进行调度，同样系统也会定期检查golobal runqueue中的G，以确保被放入global runqueue中的goroutine不会被饿死。\n \n  PS：golang对这层调度其实做了一定的优化，不是在一开始进行系统调用之前就新建一个新的M，而是使用一个全局监控的monitor（<span style=\"color: #333333;\">sysmon），定期检查进入系统调用的M，只有进入时间过长才会新建一个M。另外golang 底层基本对所有的IO都异步化了，比如网络IO，golang底层在调用read返回EAGAIN错误的时候会将当前goroutine挂起，然后使用epoll监听这个网络fd上的可读事件，在当有数据可读的时候唤醒对应的goroutine继续进行调度。其中epoll事件管理线程即golang虚拟机的sysmon线程。Go语言网络库的基础实现相详见<a href=\"http://skoo.me/go/2014/04/21/go-net-core/\" target=\"_blank\">此文</a>。</span> \n  \n  \n**stealing work** \n\n![steal.jpg](/media/files/2014/09/24/steal.jpg)\n\n为了使得调度更加公平和充分，golang引入了<a href=\"http://supertech.csail.mit.edu/papers/steal.pdf\" target=\"_blank\">work steal</a>调度算法。 在P local runqueue上的goroutine全部调度完了之后，对应的M不会傻傻得等在那里睡觉，而首先会尝试从global  runqueue中获取goroutine进行调度。如果golbal runqueue中没有goroutine，如上图所示，当前M会从别的M对应P的local runqueue中抢一半的goroutine放入自己的P中进行调度。\n\n \n#### goroutine状态迁移 \n\n从上面的介绍可以发现，调度器会使goroutine在各种状态来回切换。下图使用\"goroutine状态迁移图\"来形象得描述goroutine在调度周期中的生老病死。 \n\n![goroutinestate.jpg](/media/files/2014/09/24/goroutinestate.jpg)\n\n [1]: http://www.ibm.com/developerworks/library/l-completely-fair-scheduler/\n [2]: http://morsmachine.dk/go-scheduler\n","source":"_posts/2014-09-24-goroutine-scheduler.markdown","raw":"---\ntitle: 'goroutine 调度器'\nlayout: post\ntags:\n    - golang\n    - goroutine\n---\n\n\n\n对golang的goroutine的内部实现感觉非常神秘，通过这段时间的学习基本了解了golang调度器实现原理。 \n\n#### golang调度器 \n\n虽然golang的最小调度单元为协程（goroutine），但是操作系统最小的调度单元依然还是线程，所以golang scheduler（golang调度器）其要做的工作是如何将众多的goroutine放在有限的线程上进行高效而公平的调度。 \n\n操作系统的调度不失为高效和公平，比如[CFS][1]调度算法，那么go为何要引入goroutine？原因很多，有人会说goroutine 相比于linux的pthread机制使用很方便。但是核心原因为goroutine的轻量级，无论是从进程到线程，还是从线程到协程，其核心都是为了使得我们的调度单元更加轻量级，我们可以轻易得创建几万几十万的goroutine而不用担心内存耗尽等问题。golang引入goroutine试图在语言内核层做到足够高性能得同时（充分利用多核优势、使用epoll高效处理网络／IO、实现垃圾回收等机制）尽量简化编程。\n\n ps：人类社会的发展是生产工具不断发展解放生产力的过程，语言的发展也是一样，从机器语言到汇编语言、C语言、面向对象C＋＋\\ Java以及到现在层出不穷的动态语言。编程语言作为生产工具，其发展核心目的为最大化IT工作人员的生产力。从这点看，我很看好go语言，极简得编程之道简直大爱。 \n \n 以下基于Daniel Morsing的[一篇文章][2]介绍goroutine调度器。 \n \n 首先基于线程，用户态协程可以选择以下3种调度机制。 \n \n \n**N:1**  即多个用户态协程运行在一个os线程上，这种方式的优点是可以很快得进行上下文切换，但是缺点是不能利用多核优势。 \n\n**1:1** 一个用户态协程对应一个os线程，这种方式得优点是可以利用到多核的优势，但是协程的调度完全依赖于os线程的调度，而os线程的调度的上线文切换的代价又比较大，从而导致这种模型调度的上下文切换代价比较大。 \n\n**M:N** golang scheduler 使用的m:n调度模型，即任意数量的用户态协程可以运行在任意数量的os线程上，这样不仅可以使得上线文切换更加轻量级，同时又可以充分利用多核优势。 为了实现这种调度机制，golang 引入如下3个大结构 <!--more-->\n\n![our-cast.jpg](/media/files/2014/09/24/our-cast.jpg)\n\nM：os线程（即操作系统内核提供的线程） \n\nG：goroutine，其包含了调度一个协程所需要的堆栈以及instruction pointer（IP指令指针），以及其他一些重要的调度信息。 \n\nP：M与P的中介，实现m:n 调度模型的关键，M必须拿到P才能对G进行调度，P其实限定了golang调度其的最大并发度。 \n\n![in-motion.jpg](/media/files/2014/09/24/in-motion.jpg)\n\n\n如上图所示，2个M分别拿到context P在运行G，M只有拿到context P才能执行goroutine。被执行的goroutine在运行过程中调用 go func() ，会创建一个新的对应func() 的goroutine，并将这个goruotine加入到runqueue（就绪待调度的goroutine队列，如上图灰色部分所示），当前运行的goroutine在达到调度点（系统调用、网络IO、等待channel等）的时候，P会挂起当前运行的goroutine，从runqueue中pop一个goroutine，重新设置当前M的执行上下文继续执行（即设置为pop出来的goroutine对应的运行堆栈以及IP（instruction Point））。\n\n仔细观察我们可以发现，与golang的前一个版本的调度器不同，当前并不是采用全局的runqueue队列，而是每个P都对应有自己的一个local的runqueue，这样可以避免每一次调度都进行一次锁竞争，在32核机器上，锁竞争会导致性能变得非常糟糕。 \n\nP的数量由用户设置的GOMAXPROCS决定，当前不论GOMAXPROCS设置为多大，P的数量最大为256。由于M必须拿到P才能够对G进行调度，所以P实际上限制了go的最大并发度，256对于现在的服务器已经足够使用了。因为P并不会由于当前调度的goroutine阻塞而不可用，只要当前还有可调度的goroutine，P始终会使用M继续进行调度运行。所以其实P只需要设置为当前CPU的最大核数即可。 \n\n如果一切正常，调度器会以这样一种方式简单得运行，以下分析goroutine在两种例外情况下的行为。 \n\n**系统调用** \n\n![syscall.jpg](/media/files/2014/09/24/syscall.jpg)\n\ngolang语言能够控制线程M在用户态执行情况下的调度行为，但是却不能控制线程在陷入内核之后的行为。即在我们调用system call陷入内核没有返回之前，go其实已经控制不了。那怎么办呢？如果当前陷入system call的线程在持有P的情况下Block很长时间，会导致其他可运行的goroutine由于没有可用的P而不能得到调度。\n\n 为了保证调度的并发型，即保证拿到P的M能够持续进行调度而不是block，golang 调度器也没有很好的办法，只能在进入系统调用之前从线程池拿一个线程或者新建一个线程的方式，将当前P交给新的线程M1从而使得runqueue中的goroutine可以继续进行调度。当前M0则block在system call中等待G0返回。 在G0返回之后需要继续运行，而继续运行的条件是必须拥有一个P，如果当前没有可用的P，则将G0放到全局的runqueue中等待调度，M0退出或者放入线程池睡觉去。而如果有空闲的P，M0在拿到P之后继续进行调度。（P的数量很好的控制了并发度） \n \n P在当前local runqueue中的G全部调度完之后从global runqueue中获取G进行调度，同样系统也会定期检查golobal runqueue中的G，以确保被放入global runqueue中的goroutine不会被饿死。\n \n  PS：golang对这层调度其实做了一定的优化，不是在一开始进行系统调用之前就新建一个新的M，而是使用一个全局监控的monitor（<span style=\"color: #333333;\">sysmon），定期检查进入系统调用的M，只有进入时间过长才会新建一个M。另外golang 底层基本对所有的IO都异步化了，比如网络IO，golang底层在调用read返回EAGAIN错误的时候会将当前goroutine挂起，然后使用epoll监听这个网络fd上的可读事件，在当有数据可读的时候唤醒对应的goroutine继续进行调度。其中epoll事件管理线程即golang虚拟机的sysmon线程。Go语言网络库的基础实现相详见<a href=\"http://skoo.me/go/2014/04/21/go-net-core/\" target=\"_blank\">此文</a>。</span> \n  \n  \n**stealing work** \n\n![steal.jpg](/media/files/2014/09/24/steal.jpg)\n\n为了使得调度更加公平和充分，golang引入了<a href=\"http://supertech.csail.mit.edu/papers/steal.pdf\" target=\"_blank\">work steal</a>调度算法。 在P local runqueue上的goroutine全部调度完了之后，对应的M不会傻傻得等在那里睡觉，而首先会尝试从global  runqueue中获取goroutine进行调度。如果golbal runqueue中没有goroutine，如上图所示，当前M会从别的M对应P的local runqueue中抢一半的goroutine放入自己的P中进行调度。\n\n \n#### goroutine状态迁移 \n\n从上面的介绍可以发现，调度器会使goroutine在各种状态来回切换。下图使用\"goroutine状态迁移图\"来形象得描述goroutine在调度周期中的生老病死。 \n\n![goroutinestate.jpg](/media/files/2014/09/24/goroutinestate.jpg)\n\n [1]: http://www.ibm.com/developerworks/library/l-completely-fair-scheduler/\n [2]: http://morsmachine.dk/go-scheduler\n","slug":"goroutine-scheduler","published":1,"date":"2014-09-23T16:00:00.000Z","updated":"2015-03-30T13:28:53.000Z","comments":1,"photos":[],"link":"","_id":"cigortta1000tkfy7kofgnjsg"},{"title":" 存储一致性之复制","layout":"post","_content":"\n[1]: http://www.ibm.com/developerworks/library/l-completely-fair-scheduler/\n[2]: http://book.mixu.net/distsys/replication.html\n[3]: https://www.google.com/search?q=understanding+replication+in+databases+and+distributed+systems\n[4]: http://book.mixu.net/distsys/abstractions.html\n[5]: http://research.google.com/pubs/pub36971.html\n[6]: http://research.google.com/archive/spanner.html\n[7]: http://research.google.com/archive/chubby.html\n[8]: http://scholar.google.com/scholar?q=Replication+techniques+for+availability\n[9]: http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx\n[10]: http://research.microsoft.com/users/lamport/pubs/lamport-paxos.pdf\n[11]: http://research.microsoft.com/users/lamport/pubs/paxos-simple.pdf\n[12]: http://research.google.com/archive/paxos_made_live.html\n[13]: http://scholar.google.com/scholar?q=Paxos+Made+Practical\n[14]: http://groups.csail.mit.edu/tds/paxos.html\n[15]: http://research.microsoft.com/lampson/58-Consensus/Acrobat.pdf\n[16]: http://research.microsoft.com/en-us/um/people/lamport/pubs/reconfiguration-tutorial.pdf\n[17]: http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.20.4762\n[18]: https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf\n[19]: http://www.youtube.com/watch?v=YbZ3zDzDnrw\n[20]: http://research.yahoo.com/pub/3274\n[21]: http://research.yahoo.com/pub/3514\n\n难得一见的关于[分布式理论系列文章][1]，非常之精彩，通读这一系列文章，发现作者没有站在专家的立场上以复杂得方式分析复杂的问题，而是从工程师的角度非常恰当得介绍一系列分布式的理论，对分布式理论各个方面总结概括得非常到位，令人赏心悦目。\n\n\n以下为自己翻译的第四部分，英文原文详见: [Replication][2]。\n\n# 译文\n\n## 1 复制问题 \n\n复制问题是分布式系统中众多问题之一，我选择把重点放在与此相关的一些方面，比如leader选举、故障检测、互斥锁定以及全局快照，因为往往这些问题是大家最感兴趣的。例如，很多并行数据库的区别之一往往就是它们的复制。复制功能本身引入了很多的问题，如leader选举、错误检测、一致性以及原子光广播等。\n\n复制是一个多节通讯问题。什么样的布局和消息交互模式能够满足我们想要的性能及可用性需求？在网络分区及节点故障情况下如何保证容错性、持久性？\n\n实现复制的方式有很多，这里不会讨论某种具体的方式而是从更高的概括性角度讨论复制相关的问题。这样有助于从整体方面进行把握，而不是局限于某个具体形态。因为我的目标是探索设计的空间而不是具体的某个算法。\n\n首先我们简单定义下复制问题：假设初始状态一样的数据库，客户端发送请求来改变各副本的状态。\n\n\n![replication-both-300x206](/media/files/2014/09/replication-both-300x206.png)\n\n\n通信的流程可以被分解为如下阶段：\n\n1. （Request）客户端发送请求到服务端\n2. （Synchronous）进行同步副本复制\n3. （Response）响应返回给客户端\n4. （Asynchronous）进行异步副本复制 （同步复制下这步缺省）\n\n这种松散的模式是基于此文[understanding replication in databases and distributed systems][3]的。注意其中每阶段消息的具体通信方式取决于特定的算法，我会尝试绕过具体的算法讨论这些问题。\n\n基于如上的基本步骤，我们能够创造什么样的消息交互方式呢？不同的消息交互方式会对性能和可行性造成什么样的影响？\n\n\n### 1.2 Synchronous replication-同步复制\n\n所述第一种模式为同步复制，如下图所示:\n\n![replication-sync-300x274](/media/files/2014/09/replication-sync-300x274.png)\n\n可以看到流程如下：首先，客户端发起请求；然后，进行同步复制；最后，客户端待同步完成后获得应答。在同步阶段，S1与其他服务节点通信，直到收到所有其他节点的响应，最后，S1通知客户端操作结果（成功或者失败）.\n\n整个过程似乎简单明了。抛开同步的具体算法，我们讨论这种消息交互的特点：首先，它是Write N of N的方式，在响应返回之前，它必须被所有服务节点看到并确认。\n\n**优缺点**\n\n从性能角度来说，这意味着系统性能取决于最慢的那台服务器（木桶短板），并且系统对网络延迟非常敏感。\n\n给定 Write N of N 方式，系统无法忍受任何一台服务器的故障。当一台服务器故障，系统不能完成N个节点的写操作，系统即无法继续运行。在此情况下，只能提供对数据的读取服务，在这样的系统设计下，修改更新将不能继续进行。\n\n这样的安排可以提供很强的持久化保证。当响应成功返回，客户端可以确认所有N台服务器都已经收到并且持久化。所有N个副本丢失才会导致这个更新丢失。\n\n### 1.3 Asynchronous replication – 异步复制\n\n让我们对比下第二种模式—异步复制（也称为被动复制、拉复制或懒惰复制），正如你已经猜到的，z是同步模式的对立模式。\n\n![replication-async-300x263](/media/files/2014/09/replication-async-300x263.png)\n\n这里，主节点（leader/coordinator）在更新刷新到本地之后会立即响应客户端。不会阻塞得进行同步工作，客户端不用被迫等待很多轮服务器间通讯。 在以后的某个阶段会进行异步复制，具体的方式取决于特定的算法。\n\n我们抛开具体的算法细节来讨论这种模式。ok，这是 Write 1 of N的方式；响应会立刻返回，在随后的某个时间点，更新才会传播到其他节点。 \n\n**优缺点**\n\n从性能的角度来说，这意味着系统非常快：客户端不需要花费任何额外的时间等待系统内部做好自己的工作，该系统也不容易受到网络延迟的影响。\n\n这样的方式只能提供较弱的，概率性的持久性保证。如果一切正常，该数据最终复制被到所有N台机器，然而，如果在此之前包含数据的那台服务器故障，那么数据可能会永久丢失。\n\n给定 Write 1 of N的方式，只要有一个节点可用，该系统继续保持可用。但是这种偷懒的做法没有提供很好的持久性和一致性保证，如果故障发生，可以继续写入系统，但是不能保证可以读取到你之前写入的数据。\n\n最后，值得一提的是被动复制无法保证系统中所有节点总是包含相同的状态。如果允许在多个节点进行写，且不需要其他节点同步协调，那么会遇到冲突或者分歧的风险：在不同的地方可能会读到不同的结果（特别是节点出现故障和恢复的情况下），全局约束无发得到保障。\n\n我还没有讨论这两种通讯模式下的读模式。读模式需要遵循写入模式，在读的情况下，我们往往希望尽可能少得与节点通信，这些将会下文的多数派（quorum）部分详细讨论。\n\n以上我们只讨论了两种基本的模式，并没有深入某个特定的算法。至此，我们应该能够想到很多可能的消息交互方式以及它们在性能、持久性、可用性等方面的特点。\n\n## 2 An overview of major replication approaches\n\n在讨论了两种基本的复制方式：同步异步之后，让我们来看看主要的复制算法。\n\n有许多不同的方法对复制技术进行分类，基于第一部分同步异步之后，接下来会从以下两个方面进行介绍。\n\n1. 防止分歧（单拷贝系统）的复制方法\n2. 可能产生分歧（多master系统）的复制方法\n\n第一种方法具有“behave like a single system”的特点，在局部故障发生的时候，系统保证只有单一副本是出于激活状态（不会产生副本间分歧），此外，该系统可以保证副本总是一致的，也就是所谓的共识问题(consensus problem)。\n\n所谓共识，就是一些进程（或者计算机），就一个对象的值达成一致协定。更加正式概括如下：\n\n* Agreement: 约定，每一个正确的process必须决定相同的值。\n\n* Intergrity: 完整性，每一个正确的process至多决定一个值，并且如果决定了这个值，这个值肯定是其中一个process提出的。\n\n* Termination：收敛性，所有process最终会达成一个一致的决定。\n\n* Validity：有效性\n\n互斥问题、leader选举、多播以及原子广播都是属于需要达成共识的问题。维护一致性的复制系统必须通过某种方式解决这个问题。\n\n对维护副本一致性算法可以进行如下分类:\n\n* 1N 消息 （异步 primary/backup）\n* 2N 消息 （同步 primary/backup）\n* 4N 消息 （两阶段提交、Multi-Paxos）\n* 6N 消息  (3阶段提交协议、基本Paxos（没有Leader)）\n\n这些算法在容错性方面各不相同，我之所以通过消息的交互次数来区分这些协议主要目的是想回答一个问题，即**“每多一次附加的消息交互为了换来什么？”**。\n\n下图引用Ryan Barret的图片来描述不同算法的基本特点。\n\n![google-transact09](/media/files/2014/09/google-transact09.png)\n\n图中包含**一致性**、**延迟**、**吞吐量**、**数据丢失**及**故障切换**这些系统特性。我们可以追溯到之前提到的两种复制方法：同步复制及异步复制；当选择等待(blocking)，你会得到更差的性能却更强的数据保证。当我们讨论分区容忍性时（网络延迟或故障）两阶段协议（2PC）和多数派协议（Quorum）在吞吐量上会存在很大差别。\n\n图中，弱一致性算法和最终一致性算法被笼统得归类为**gossip**，我会在接下来的第五章详细讨论一些弱一致性复制方法－gossip及quorum。\n\n值得注意的是，弱一致性系统通用的算法较少，却有很多可选的方法。因为对待这样的分布式系统可以简单得看成是多个节点而非整体系统，这类系统没有明确得需要解决的问题，更多的是告诉大家（使用）我是弱一致性的，具备所有弱一致性系统所具有的特点。\n\n接下来我们先来看维护单一副本一致性的系统。\n\n### 2.1 Primay/backup replication\n\n主从复制可能是最基本最常用的复制方法，所有的更新都发送到主节点，然后将操作日志通过网络复制到备份节点，有两个变种：\n\n* 异步主从复制\n* 同步主从复制\n同步需要两次信息交换（“更新” + “确认”），而异步只需要一次“更新”。\n\n主从复制非常之普遍。例如，默认情况下，MySQL和MongoDB的复制使用异步主从方式。所有操作都是由主节点串行并持久化之后异步复制到备份服务器。\n\n正如我们在前面异步背景下讨论的，任何一种异步复制算法只能提供弱持久化保证。在Mysql中表现为复制滞后，如果主失败，尚未被发送到备份则有可能会导致更新丢失。\n\n同步主从方式保证数据在从节点持久化之后响应客户端，这就需要客户端等待，但是这种方式同样只能提供比较弱的保证，考虑如下简单的失败场景：\n\n* 主副本收到写请求并发送到从节点\n* 从副本持久化并响应主副本\n* 主副本在响应客户端之前出故障\n\n这种情况下，客户端只能假设请求失败，但是从节点却提交了更新，如果直接将从节点提升为主节点，则会出错，这时候就必须人工介入了。\n\n这里简化了讨论，虽然所有主备算法遵循基本一样的消息交换方法，但是在故障恢复等方面会有所不同。\n基于主从复制的方案只能提供尽力而为的保证（节点的异常很容易会造成数据丢失或者错误更新），并且非常容易受到网络延迟的影响。\n\n基于主备方式的关键是，它们只能提供尽力而为的保证（节点在不合时宜的失败或者不正确的更新都有可能导致更新丢失）。此外，P/B方案也非常容易受到网络分区的影响。\n\n为了避免突然的故障导致不能保证一致性，我们需要添加新一轮消息，也就是接下来讨论的”两阶段提交协议”（2PC）。\n\n### 2.2 Two phase commit (2PC)\n\n两阶段提交（2PC）在许多经典的关系数据库中使用，例如，MySQL 集群使用2PC协议实现同步复制。消息基本如下\n\n[ Coordinator ] -> OK to commit?         [ Peers ]\n<- Yes / No\n\n[ Coordinator ] -> Commit / Rollback [ Peers ]\n<- ACK\n\n* 第一阶段（投票阶段）\n\ncoordinator协调者发送更新操作到所有的参与者（participants）,每个参与者处理更新并且投票提交请求(commit)或者取消提交(abort)，当选择提交请求的时候，更新操作会持久化到临时区（write-ahead log），直到第二阶段完成之前，更新始终是临时的。\n\n* 第二阶段（决定阶段）\n\ncoordinator决定最终结果并且通知参与者。如果所有参与者都投票“提交请求(commit)”,更新会从临时区区出来进行持久化。\n\n在提交请求并持久化之前进行第二阶段的确认是有效的，因为这样在相关节点失败情况下允许回滚操作。然而在之前提到的主备协议中没有回滚，这会导致多个副本之间产生分歧。\n\n2PC协议很容易出现阻塞，因为单个节点的故障（参与者或协调者）都会导致系统无法继续运行。然而恢复由于有第二个阶段的存在恢复往往是可行的。注意2PC协议假设数据是持久保存的并且所有节点不会丢失数据并且不会永远crash。实际上在持久化存储失效情况下数据丢失仍旧是可能的。\n\n两阶段协议的恢复细节非常复杂这这里不会详细进行说明，其主要工作是保证数据持久化并且保证恢复正确（即根据这一轮提交的结果进行redoing或者undoing）\n\n正如我们在[上一节][4]中提到的CAP，2PC协议属于CA，不具有分区容错特性。2PC不能处理网络分区的错误场景，在节点失效（或者分区）情况下只能等到恢复之后才能继续运行。如果coordinator失败必须进行人工介入。2PC协议同样对网络延迟非常敏感。因为2PC还是采用了write N of N的方式，直到最慢的节点确认之后写入才能继续进行。\n\n2PC协议在性能和容错性方面做了权衡取舍，在传统的关系型数据库中非常流行。然而，当前新的系统经常使用具有分区容错性的一致性算法。因为此类算法可以在短暂的网络分区之后自动恢复并且能够更加优雅得处理节点之间的延迟。\n\n接下来让我们继续分析分区容错性一致性算法。\n\n### 2.3 Partition tolerant consensus algorithms(分区容错性一致性算法)\n\n我们接下来讨论的分区容错一致性算法为维护单副本一致性的容错算法。还有另外一类容错算法:容忍拜占庭（Byzantine)错误，这样的算法很少应用于商业系统，因为这类系统非常昂贵并且难以实现，因此这里不会涉及到此类算法。\n\n谈到具备分区容忍特性的一致性算法，其中最知名的为Paxos算法。但是由于它非常难以实现和解释。我会把重点放在更加容易教授和实现的算法—Raft算法。让我们先来看下网络分区和分区容忍一致性算法的一般特性。\n\n> 什么是网络分区？\n\n网络分区是指：到一个或者多个节点的网络链接出现故障。那些无法到达的节点本身可能继续保持活跃，甚至可以接受来自客户端的请求。正如我们在前面章节所学到的CAP理论，在发生网络分区的时候并不是所有的系统都能够从容应对。\n\n网络分区之所以如此棘手，是由于在分区发生的时候几乎不可能区分节点是故障宕机还是网络故障导致不可达。如果是网络分区，但是节点并没有出现故障，系统很可能分裂成两个同时激活的分区。下面两张图说明了网络分区和节点出现故障的情况，非常相似。\n\n* 系统包含两个节点，节点故障 vs 网络分区:\n\n![system-of-2-300x87](/media/files/2014/09/system-of-2-300x87.png)\n\n* 系统包含三个节点，节点故障 vs 网络分区:\n\n![system-of-3-300x138](/media/files/2014/09/system-of-2-300x87.png)\n\n\n保证单副本强一致性的系统必须使用某些方法来打破这种对称的僵局：否则，它会分裂成独立的系统，不能再维持单副本一致性。\n\n由于CAP理论表明网络分区是不可能避免的，所以具备分区容忍能力的系统在网络分区发生的时候必须确保只有一个分区仍然有效。\n\n> Majority decisions（多数派决定）\n\n这就是为什么分区容忍一致性算法依赖于多数派投票（即CAP理论）。在更新的时候，依赖于多数派节点，而不是所有节点（2PC协议），这使得此类协议可以容忍少数节点宕机以及网络故障导致的延迟和不可达。在N个节点中只需要（N/2 + 1）个节点存活并且可达，系统继续正常运行。\n\n分区容忍一致性算法使用奇数个节点（例如：3、5、7）。2个节点无法形成有效多数派；如果节点数为3，则可以容忍1个节点故障；节点数为5则可以容忍2个节点故障。\n\n在网络分区发生的时候，两个分区将不对称。其中一个分区包含多数派（N/2 +1）个数个节点。少数派分区将停止处理操作，以防止两个分区发生分歧；多数派分区继续正常运行。这样可以确保系统中只有一个分区能够正常运行。\n\n多数派在容忍分歧方面同样非常有效：如果出现骚动或者失败，节点的投票可能各不相同，但是多数派的决定只有一个，暂时的分歧会导致协议block但是不会违反单副本一致的特性。\n\n> Role (角色)\n\n构建此类系统有两种方法：\n\n1. 所有的节点角色都相同，包含相同的功能\n2. 节点具有单独不同的角色和不同的功能\n\n一致性复制算法一般选择第2种方式：即选定某节点为leader或者master的方式，这样可以使得系统更加高效。这是由于所有的更新操作必须通过leader节点序列化，非leader节点只需要转发请求即可。（减小一轮消息交互）\n\n固定不同的角色并不排除系统在leader节点故障情况下的恢复。正常情况下指定不同角色并不表示在失败之后重新分配角色不能使得系统恢复；而是说明系统在选举出leader之后可以一直持续正常运行直到出现下一次节点或者网络故障。\n\nPaxos和Raft算法使用不同角色的方式。在一般情况下，leader节点（在paxos中为”proposer”）负责协调（即2PC中的coordination），其他节点则为follower（在paxos中为”accptors”或者”leaderners”）。\n\n> Epoch(轮次)\n\nPaxos算法和Raft算法每一轮正常的流程称为epoch（Raft中为”term”)。在对应每一个epoch期间只有一个节点被指定为leader。\n\n![epoch](/media/files/2014/09/epoch.png)\n\n在选举完成之后，同一个leader始终会成为该轮次（epoch）阶段的coordinator（协调者）。从上图(摘自Raft)可以看到，leader节点的宕机会导致该轮次（epoch）立即结束。\n\nEpoch(轮次)在协议中充当逻辑时钟。这样可以允许节点能够辨别某些宕机或者delay之后继续加入的节点—“那些被分区或者停止运作的节点对应的epoch会比当前的小”；这使得某些尚未成功提交的请求被忽略，以确保不会使系统产生二义性。\n\n> Leader changes via duels\n\n所有节点刚开始的角色都是follower；在启动之后其中一个节点会被选举会leader。在正常操作流程中，leader会保持和follower之间的心跳以使系统可以检测leader失效或者产生网络分区。\n\n当follower节点检测到leader无响应，它会切换到中间状态（Raft中成为”candidate”状态）。在这个状态下，节点对当前自身的epoch/term做自增（epoch++），并发起leader选举竞选成为此轮epoch新的leader。\n\n为了成为leader，必须获得过半数的投票。分配选票的方式为FIFO方式，leader会最终被选举出来。一般来说，在每次竞选尝试中会随机等待一段时间以减少同时进行竞选的节点数。\n\n> Numbered proposals within an epoch（一个轮次中带编号的请求）\n\n在每一轮次中，leader会对每次需要表决的值进行提案（即序列化update command），在每一个轮次中，每个提案对应的数字是唯一且严格递增的。\n\n> Normal operation\n\n在正常运行期间，所有提案都必须经过leader节点。当客户端提交一个提案（如更新操作），leader联系所有多数派中的节点，如果当前没有leader竞选请求存在（基于多数派中follower的响应），leader提案值有效。并且如果其多数派中的follower accept该提案，那么这个提案被接受。\n\n由于很可能另外一个节点也正尝试作为一个leader进行提案，必须保证，一旦一个提案被accept，它的值永远无法被改变。否则，一个已经成功提交的请求可能会被撤销。Lamport在paxos算法中描述如下：\n\n> P2: If a proposal with value v is chosen, then every higher-numbered proposal that is chosen has value v.\n\n这一限制需要所有的follower和propser遵循:一旦一个提案值被一个多数派接受，那么这个提案值不能被改变.（注意”提案值不能改变”对应于算法的一次执行。典型的复制算法对每一次提交执行一次一致性算法，为了解释得更加简单易懂，往往大家专注于算法的一直执行进行详细讨论）\n\n为了保证这个特性，提案者必须首先询问follower他们已经接受的编号最大的提案对应的值。如果提案者发现已经存在一个提案，那么它必须试图完成这个已经存在的提案，而不是进行重新提案。Lamport在paxos算法中描述如下：\n\n> P2b. If a proposal with value v is chosen, then every higher-numbered proposal issued by any proposer has value v.\n\n更加具体的：\n\n> P2c. For any v and n, if a proposal with value v and number n is issued [by a leader], then there is a set S consisting of a majority of acceptors [followers] such that either (a) no acceptor in S has accepted any proposal numbered less than n, or (b) v is the value of the highest-numbered proposal among all proposals numbered less than n accepted by the followers in S.\n\n这是Paxos算法的核心，同样也是其他类Paxos衍生算法的核心。提案的值直到协议的第二个阶段才能选定。提案者某些情况下必须重新进行第一阶段以保证他们可以自由得对当前一轮提案赋予自己的值。\n\n如果之前已经有多个提案存在(可能是还未决定的提案)，那么会选举标号最大的提案对应的值。提案者只有在其多数派中的节点没有一个提案竞争者的前提下才能选取自己的提案值。同时提案者要求follower见到此提案的同时，不能accept比这个提案编号更小的提案。\n\n把这两个部分结合起来，在Paxos算法中达成一个决定需要两轮消息交互。\n\n[ Proposer ] -> Prepare(n)]                                      [ Followers ]\n\n<- Promise(n; previous proposal number\nand previous value if accepted a\nproposal in the past)\n\n[ Proposer ] -> AcceptRequest(n, own value or the value          [ Followers ]\n\nassociated with the highest proposal number\nreported by the followers)\n\n<- Accepted(n, value)\n\n\n在prepare阶段，proposer（提案者）可以了解任何处于竞争状态或者之前已经决定的提案。第二个阶段（accept阶段）选举一个新的值或者之前已经被accept过的值。在某些情况下，假设同时两个proposer处于active状态（即同时进行提案）或者多数派节点故障情况下，可能没有一个propossal被多数派accept。但是这是可以接受的，因为成功的提案最终会收敛为一个有效的值。\n\n事实上，根据FLP理论，当消息传递边界不存在的情况下，一致性算法只能在safety或者liveness间二选一。Paxos算法选择放弃liveness保证safety：即提案可能无休止的进行下去，直到没有竞争leader（多个proposal）并且一个多数派accept提案。\n\n当然，实现这种算法非常复杂，即使在专家手中，一些很小的关注点可能会导致非常大的代码量。\n\n实用优化：\n\n* 通过leader租期（而不是心跳）避免重复的leader选举。\n* 在leader确定的稳定状态下避免第一阶段的propose消息交互。\n* 确保follower和proposers持久化的消息不被损坏。\n* 集群中节点的角色以安全的方式转换（在Paxos算法中依赖任意一个多数派总是有一个节点是相交的）\n* 在副本节点crash，磁盘丢失或者新节点加入情况下需要以安全有效的方式进行副本恢复。\n* 在合理的时间（均衡存储、容错需求）进行快照以及垃圾回收需要保证安全性。\n\ngoogle “Paxos made live”这篇文章详细讨论了这系列挑战。\n\n### 2.4 Partition-tolerant consensus algorithms: Paxos, Raft, ZAB\n\n希望以上的讨论让你对“分区容忍性算法”如何工作有了基本的认识。我建议你通过进一步阅读了解不同算法的实现细节。\n\n#### 2.4.1 Paxos\n\npaxos算法是设计具备分区容忍特性的强一致性系统必须了解的算法。该算法被许多google的系统使用。[BigTable/Megastore][5]、GFS、[Spanner][6]中使用的[Chubby lock Mananger][7]。\n\nPaxos以希腊的一座岛屿的名词命名。由Leslie Lamport在1998年发表的”The Part-Time Parliament”文章中首次发表。Paxos算法被认为难以实现的，所以后续工业界发表了一系列文章探讨其实现细节（在后面的further reading 中可以看到）。\n\nPaxos算法中描述的往往是一致性算法的一次执行。而实际执行过程中，往往需要考虑高效得运行多轮一致性算法。这使得很多有兴趣（致力于）搭建基于Paxos协议的系统的开发者设计了很多基于paxos协议的扩展协议。此外，实际实现过程中还有很多挑战，比如如何维护集群成员的成员关系等。\n\n#### 2.4.2 ZAB\n\nZAB是Apache Zookeeper所使用的原子广播协议。Zookeeper 为分布式系统提供了协调者的角色（coordination primitives）。很多基于Hadoop的分布式系统（HBase、Storm、Kafka）都使用Zookeeper作为协调者（coordination）。Zookeeper基本上算是Chubby的开源实现版本。从技术上来讲原子广播和单纯的一致性协议问题有所不同，但是这类算法同样归属于”强一致性分布式容错算法”。\n\n#### 2.4.3 Raft\n\nRaft是近期（2013年）加入本家族的算法。Raft比Paxos算法更加易于理解和学习，但是提供和Paxos算法同样的保证。特别的是，该算法的不同的部分被更加清理得分离开来，发表的paper中详细讨论了集群成员关系的变化。Raft在近期被类似zookeeper的开源系统etcd使用。\n\n### 2.5 Replication methods with strong consistency\n\n在以上本章节中，我们讨论了强一致性复制方法。从同步和异步开始对比，从简单开始逐步到能够容忍更加复杂故障的算法。以下总结了各种算法的关键特征：\n\nPrimary/Backup\n\n* Single, static master\n* Replicated log, slaves are not involved in executing operations\n* No bounds on replication delay\n* Not partition tolerant\n* Manual/ad-hoc failover, not fault tolerant, “hot backup”\n\n2PC\n\n* Unanimous vote: commit or abort\n* Static master\n* 2PC cannot survive simultaneous failure of the coordinator and a node during a commit\n* Not partition tolerant, tail latency sensitive\n\nPaxos\n\n* Majority vote\n* Dynamic master\n* Robust to n/2-1 simultaneous failures as part of protocol\n* Less sensitive to tail latency\n\n#### 2.5.1 Further reading\n\n**Primary-backup and 2PC**\n\n* [Replication techniques for availability][8] - Robbert van Renesse & Rachid Guerraoui, 2010\n\n* [Concurrency Control and Recovery in Database Systems][9]\n\n**Paxos**\n\n* [The Part-Time Parliament][10] – Leslie Lamport\n* [Paxos Made Simple][11] – Leslie Lamport, 2001\n* [Paxos Made Live - An Engineering Perspective][12] – Chandra et al\n* [Paxos Made Practical][13] – Mazieres, 2007\n* [Revisiting the Paxos Algorithm][14] – Lynch et al\n* [How to build a highly available system with consensus][15] – Butler Lampson\n* [Reconfiguring a State Machine - Lamport et al – changing][16] cluster membership\n* [Implementing Fault-Tolerant Services Using the State Machine Approach: a Tutorial][17] – Fred Schneider\n\n**Raft and ZAB**\n\n* [In Search of an Understandable Consensus Algorithm][18], Diego Ongaro, John Ousterhout, 2013\n* [Raft Lecture][19] – User Study\n* [A simple totally ordered broadcast protocol][20]- Junqueira, Reed\n* [ZooKeeper Atomic Broadcast][21]\n","source":"_posts/2014-09-02-stroage_consistency_replicate.markdown","raw":"---\ntitle: ' 存储一致性之复制'\nlayout: post\ntags:\n    - storage\n    - consistency\n    - avaliable\n---\n\n[1]: http://www.ibm.com/developerworks/library/l-completely-fair-scheduler/\n[2]: http://book.mixu.net/distsys/replication.html\n[3]: https://www.google.com/search?q=understanding+replication+in+databases+and+distributed+systems\n[4]: http://book.mixu.net/distsys/abstractions.html\n[5]: http://research.google.com/pubs/pub36971.html\n[6]: http://research.google.com/archive/spanner.html\n[7]: http://research.google.com/archive/chubby.html\n[8]: http://scholar.google.com/scholar?q=Replication+techniques+for+availability\n[9]: http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx\n[10]: http://research.microsoft.com/users/lamport/pubs/lamport-paxos.pdf\n[11]: http://research.microsoft.com/users/lamport/pubs/paxos-simple.pdf\n[12]: http://research.google.com/archive/paxos_made_live.html\n[13]: http://scholar.google.com/scholar?q=Paxos+Made+Practical\n[14]: http://groups.csail.mit.edu/tds/paxos.html\n[15]: http://research.microsoft.com/lampson/58-Consensus/Acrobat.pdf\n[16]: http://research.microsoft.com/en-us/um/people/lamport/pubs/reconfiguration-tutorial.pdf\n[17]: http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.20.4762\n[18]: https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf\n[19]: http://www.youtube.com/watch?v=YbZ3zDzDnrw\n[20]: http://research.yahoo.com/pub/3274\n[21]: http://research.yahoo.com/pub/3514\n\n难得一见的关于[分布式理论系列文章][1]，非常之精彩，通读这一系列文章，发现作者没有站在专家的立场上以复杂得方式分析复杂的问题，而是从工程师的角度非常恰当得介绍一系列分布式的理论，对分布式理论各个方面总结概括得非常到位，令人赏心悦目。\n\n\n以下为自己翻译的第四部分，英文原文详见: [Replication][2]。\n\n# 译文\n\n## 1 复制问题 \n\n复制问题是分布式系统中众多问题之一，我选择把重点放在与此相关的一些方面，比如leader选举、故障检测、互斥锁定以及全局快照，因为往往这些问题是大家最感兴趣的。例如，很多并行数据库的区别之一往往就是它们的复制。复制功能本身引入了很多的问题，如leader选举、错误检测、一致性以及原子光广播等。\n\n复制是一个多节通讯问题。什么样的布局和消息交互模式能够满足我们想要的性能及可用性需求？在网络分区及节点故障情况下如何保证容错性、持久性？\n\n实现复制的方式有很多，这里不会讨论某种具体的方式而是从更高的概括性角度讨论复制相关的问题。这样有助于从整体方面进行把握，而不是局限于某个具体形态。因为我的目标是探索设计的空间而不是具体的某个算法。\n\n首先我们简单定义下复制问题：假设初始状态一样的数据库，客户端发送请求来改变各副本的状态。\n\n\n![replication-both-300x206](/media/files/2014/09/replication-both-300x206.png)\n\n\n通信的流程可以被分解为如下阶段：\n\n1. （Request）客户端发送请求到服务端\n2. （Synchronous）进行同步副本复制\n3. （Response）响应返回给客户端\n4. （Asynchronous）进行异步副本复制 （同步复制下这步缺省）\n\n这种松散的模式是基于此文[understanding replication in databases and distributed systems][3]的。注意其中每阶段消息的具体通信方式取决于特定的算法，我会尝试绕过具体的算法讨论这些问题。\n\n基于如上的基本步骤，我们能够创造什么样的消息交互方式呢？不同的消息交互方式会对性能和可行性造成什么样的影响？\n\n\n### 1.2 Synchronous replication-同步复制\n\n所述第一种模式为同步复制，如下图所示:\n\n![replication-sync-300x274](/media/files/2014/09/replication-sync-300x274.png)\n\n可以看到流程如下：首先，客户端发起请求；然后，进行同步复制；最后，客户端待同步完成后获得应答。在同步阶段，S1与其他服务节点通信，直到收到所有其他节点的响应，最后，S1通知客户端操作结果（成功或者失败）.\n\n整个过程似乎简单明了。抛开同步的具体算法，我们讨论这种消息交互的特点：首先，它是Write N of N的方式，在响应返回之前，它必须被所有服务节点看到并确认。\n\n**优缺点**\n\n从性能角度来说，这意味着系统性能取决于最慢的那台服务器（木桶短板），并且系统对网络延迟非常敏感。\n\n给定 Write N of N 方式，系统无法忍受任何一台服务器的故障。当一台服务器故障，系统不能完成N个节点的写操作，系统即无法继续运行。在此情况下，只能提供对数据的读取服务，在这样的系统设计下，修改更新将不能继续进行。\n\n这样的安排可以提供很强的持久化保证。当响应成功返回，客户端可以确认所有N台服务器都已经收到并且持久化。所有N个副本丢失才会导致这个更新丢失。\n\n### 1.3 Asynchronous replication – 异步复制\n\n让我们对比下第二种模式—异步复制（也称为被动复制、拉复制或懒惰复制），正如你已经猜到的，z是同步模式的对立模式。\n\n![replication-async-300x263](/media/files/2014/09/replication-async-300x263.png)\n\n这里，主节点（leader/coordinator）在更新刷新到本地之后会立即响应客户端。不会阻塞得进行同步工作，客户端不用被迫等待很多轮服务器间通讯。 在以后的某个阶段会进行异步复制，具体的方式取决于特定的算法。\n\n我们抛开具体的算法细节来讨论这种模式。ok，这是 Write 1 of N的方式；响应会立刻返回，在随后的某个时间点，更新才会传播到其他节点。 \n\n**优缺点**\n\n从性能的角度来说，这意味着系统非常快：客户端不需要花费任何额外的时间等待系统内部做好自己的工作，该系统也不容易受到网络延迟的影响。\n\n这样的方式只能提供较弱的，概率性的持久性保证。如果一切正常，该数据最终复制被到所有N台机器，然而，如果在此之前包含数据的那台服务器故障，那么数据可能会永久丢失。\n\n给定 Write 1 of N的方式，只要有一个节点可用，该系统继续保持可用。但是这种偷懒的做法没有提供很好的持久性和一致性保证，如果故障发生，可以继续写入系统，但是不能保证可以读取到你之前写入的数据。\n\n最后，值得一提的是被动复制无法保证系统中所有节点总是包含相同的状态。如果允许在多个节点进行写，且不需要其他节点同步协调，那么会遇到冲突或者分歧的风险：在不同的地方可能会读到不同的结果（特别是节点出现故障和恢复的情况下），全局约束无发得到保障。\n\n我还没有讨论这两种通讯模式下的读模式。读模式需要遵循写入模式，在读的情况下，我们往往希望尽可能少得与节点通信，这些将会下文的多数派（quorum）部分详细讨论。\n\n以上我们只讨论了两种基本的模式，并没有深入某个特定的算法。至此，我们应该能够想到很多可能的消息交互方式以及它们在性能、持久性、可用性等方面的特点。\n\n## 2 An overview of major replication approaches\n\n在讨论了两种基本的复制方式：同步异步之后，让我们来看看主要的复制算法。\n\n有许多不同的方法对复制技术进行分类，基于第一部分同步异步之后，接下来会从以下两个方面进行介绍。\n\n1. 防止分歧（单拷贝系统）的复制方法\n2. 可能产生分歧（多master系统）的复制方法\n\n第一种方法具有“behave like a single system”的特点，在局部故障发生的时候，系统保证只有单一副本是出于激活状态（不会产生副本间分歧），此外，该系统可以保证副本总是一致的，也就是所谓的共识问题(consensus problem)。\n\n所谓共识，就是一些进程（或者计算机），就一个对象的值达成一致协定。更加正式概括如下：\n\n* Agreement: 约定，每一个正确的process必须决定相同的值。\n\n* Intergrity: 完整性，每一个正确的process至多决定一个值，并且如果决定了这个值，这个值肯定是其中一个process提出的。\n\n* Termination：收敛性，所有process最终会达成一个一致的决定。\n\n* Validity：有效性\n\n互斥问题、leader选举、多播以及原子广播都是属于需要达成共识的问题。维护一致性的复制系统必须通过某种方式解决这个问题。\n\n对维护副本一致性算法可以进行如下分类:\n\n* 1N 消息 （异步 primary/backup）\n* 2N 消息 （同步 primary/backup）\n* 4N 消息 （两阶段提交、Multi-Paxos）\n* 6N 消息  (3阶段提交协议、基本Paxos（没有Leader)）\n\n这些算法在容错性方面各不相同，我之所以通过消息的交互次数来区分这些协议主要目的是想回答一个问题，即**“每多一次附加的消息交互为了换来什么？”**。\n\n下图引用Ryan Barret的图片来描述不同算法的基本特点。\n\n![google-transact09](/media/files/2014/09/google-transact09.png)\n\n图中包含**一致性**、**延迟**、**吞吐量**、**数据丢失**及**故障切换**这些系统特性。我们可以追溯到之前提到的两种复制方法：同步复制及异步复制；当选择等待(blocking)，你会得到更差的性能却更强的数据保证。当我们讨论分区容忍性时（网络延迟或故障）两阶段协议（2PC）和多数派协议（Quorum）在吞吐量上会存在很大差别。\n\n图中，弱一致性算法和最终一致性算法被笼统得归类为**gossip**，我会在接下来的第五章详细讨论一些弱一致性复制方法－gossip及quorum。\n\n值得注意的是，弱一致性系统通用的算法较少，却有很多可选的方法。因为对待这样的分布式系统可以简单得看成是多个节点而非整体系统，这类系统没有明确得需要解决的问题，更多的是告诉大家（使用）我是弱一致性的，具备所有弱一致性系统所具有的特点。\n\n接下来我们先来看维护单一副本一致性的系统。\n\n### 2.1 Primay/backup replication\n\n主从复制可能是最基本最常用的复制方法，所有的更新都发送到主节点，然后将操作日志通过网络复制到备份节点，有两个变种：\n\n* 异步主从复制\n* 同步主从复制\n同步需要两次信息交换（“更新” + “确认”），而异步只需要一次“更新”。\n\n主从复制非常之普遍。例如，默认情况下，MySQL和MongoDB的复制使用异步主从方式。所有操作都是由主节点串行并持久化之后异步复制到备份服务器。\n\n正如我们在前面异步背景下讨论的，任何一种异步复制算法只能提供弱持久化保证。在Mysql中表现为复制滞后，如果主失败，尚未被发送到备份则有可能会导致更新丢失。\n\n同步主从方式保证数据在从节点持久化之后响应客户端，这就需要客户端等待，但是这种方式同样只能提供比较弱的保证，考虑如下简单的失败场景：\n\n* 主副本收到写请求并发送到从节点\n* 从副本持久化并响应主副本\n* 主副本在响应客户端之前出故障\n\n这种情况下，客户端只能假设请求失败，但是从节点却提交了更新，如果直接将从节点提升为主节点，则会出错，这时候就必须人工介入了。\n\n这里简化了讨论，虽然所有主备算法遵循基本一样的消息交换方法，但是在故障恢复等方面会有所不同。\n基于主从复制的方案只能提供尽力而为的保证（节点的异常很容易会造成数据丢失或者错误更新），并且非常容易受到网络延迟的影响。\n\n基于主备方式的关键是，它们只能提供尽力而为的保证（节点在不合时宜的失败或者不正确的更新都有可能导致更新丢失）。此外，P/B方案也非常容易受到网络分区的影响。\n\n为了避免突然的故障导致不能保证一致性，我们需要添加新一轮消息，也就是接下来讨论的”两阶段提交协议”（2PC）。\n\n### 2.2 Two phase commit (2PC)\n\n两阶段提交（2PC）在许多经典的关系数据库中使用，例如，MySQL 集群使用2PC协议实现同步复制。消息基本如下\n\n[ Coordinator ] -> OK to commit?         [ Peers ]\n<- Yes / No\n\n[ Coordinator ] -> Commit / Rollback [ Peers ]\n<- ACK\n\n* 第一阶段（投票阶段）\n\ncoordinator协调者发送更新操作到所有的参与者（participants）,每个参与者处理更新并且投票提交请求(commit)或者取消提交(abort)，当选择提交请求的时候，更新操作会持久化到临时区（write-ahead log），直到第二阶段完成之前，更新始终是临时的。\n\n* 第二阶段（决定阶段）\n\ncoordinator决定最终结果并且通知参与者。如果所有参与者都投票“提交请求(commit)”,更新会从临时区区出来进行持久化。\n\n在提交请求并持久化之前进行第二阶段的确认是有效的，因为这样在相关节点失败情况下允许回滚操作。然而在之前提到的主备协议中没有回滚，这会导致多个副本之间产生分歧。\n\n2PC协议很容易出现阻塞，因为单个节点的故障（参与者或协调者）都会导致系统无法继续运行。然而恢复由于有第二个阶段的存在恢复往往是可行的。注意2PC协议假设数据是持久保存的并且所有节点不会丢失数据并且不会永远crash。实际上在持久化存储失效情况下数据丢失仍旧是可能的。\n\n两阶段协议的恢复细节非常复杂这这里不会详细进行说明，其主要工作是保证数据持久化并且保证恢复正确（即根据这一轮提交的结果进行redoing或者undoing）\n\n正如我们在[上一节][4]中提到的CAP，2PC协议属于CA，不具有分区容错特性。2PC不能处理网络分区的错误场景，在节点失效（或者分区）情况下只能等到恢复之后才能继续运行。如果coordinator失败必须进行人工介入。2PC协议同样对网络延迟非常敏感。因为2PC还是采用了write N of N的方式，直到最慢的节点确认之后写入才能继续进行。\n\n2PC协议在性能和容错性方面做了权衡取舍，在传统的关系型数据库中非常流行。然而，当前新的系统经常使用具有分区容错性的一致性算法。因为此类算法可以在短暂的网络分区之后自动恢复并且能够更加优雅得处理节点之间的延迟。\n\n接下来让我们继续分析分区容错性一致性算法。\n\n### 2.3 Partition tolerant consensus algorithms(分区容错性一致性算法)\n\n我们接下来讨论的分区容错一致性算法为维护单副本一致性的容错算法。还有另外一类容错算法:容忍拜占庭（Byzantine)错误，这样的算法很少应用于商业系统，因为这类系统非常昂贵并且难以实现，因此这里不会涉及到此类算法。\n\n谈到具备分区容忍特性的一致性算法，其中最知名的为Paxos算法。但是由于它非常难以实现和解释。我会把重点放在更加容易教授和实现的算法—Raft算法。让我们先来看下网络分区和分区容忍一致性算法的一般特性。\n\n> 什么是网络分区？\n\n网络分区是指：到一个或者多个节点的网络链接出现故障。那些无法到达的节点本身可能继续保持活跃，甚至可以接受来自客户端的请求。正如我们在前面章节所学到的CAP理论，在发生网络分区的时候并不是所有的系统都能够从容应对。\n\n网络分区之所以如此棘手，是由于在分区发生的时候几乎不可能区分节点是故障宕机还是网络故障导致不可达。如果是网络分区，但是节点并没有出现故障，系统很可能分裂成两个同时激活的分区。下面两张图说明了网络分区和节点出现故障的情况，非常相似。\n\n* 系统包含两个节点，节点故障 vs 网络分区:\n\n![system-of-2-300x87](/media/files/2014/09/system-of-2-300x87.png)\n\n* 系统包含三个节点，节点故障 vs 网络分区:\n\n![system-of-3-300x138](/media/files/2014/09/system-of-2-300x87.png)\n\n\n保证单副本强一致性的系统必须使用某些方法来打破这种对称的僵局：否则，它会分裂成独立的系统，不能再维持单副本一致性。\n\n由于CAP理论表明网络分区是不可能避免的，所以具备分区容忍能力的系统在网络分区发生的时候必须确保只有一个分区仍然有效。\n\n> Majority decisions（多数派决定）\n\n这就是为什么分区容忍一致性算法依赖于多数派投票（即CAP理论）。在更新的时候，依赖于多数派节点，而不是所有节点（2PC协议），这使得此类协议可以容忍少数节点宕机以及网络故障导致的延迟和不可达。在N个节点中只需要（N/2 + 1）个节点存活并且可达，系统继续正常运行。\n\n分区容忍一致性算法使用奇数个节点（例如：3、5、7）。2个节点无法形成有效多数派；如果节点数为3，则可以容忍1个节点故障；节点数为5则可以容忍2个节点故障。\n\n在网络分区发生的时候，两个分区将不对称。其中一个分区包含多数派（N/2 +1）个数个节点。少数派分区将停止处理操作，以防止两个分区发生分歧；多数派分区继续正常运行。这样可以确保系统中只有一个分区能够正常运行。\n\n多数派在容忍分歧方面同样非常有效：如果出现骚动或者失败，节点的投票可能各不相同，但是多数派的决定只有一个，暂时的分歧会导致协议block但是不会违反单副本一致的特性。\n\n> Role (角色)\n\n构建此类系统有两种方法：\n\n1. 所有的节点角色都相同，包含相同的功能\n2. 节点具有单独不同的角色和不同的功能\n\n一致性复制算法一般选择第2种方式：即选定某节点为leader或者master的方式，这样可以使得系统更加高效。这是由于所有的更新操作必须通过leader节点序列化，非leader节点只需要转发请求即可。（减小一轮消息交互）\n\n固定不同的角色并不排除系统在leader节点故障情况下的恢复。正常情况下指定不同角色并不表示在失败之后重新分配角色不能使得系统恢复；而是说明系统在选举出leader之后可以一直持续正常运行直到出现下一次节点或者网络故障。\n\nPaxos和Raft算法使用不同角色的方式。在一般情况下，leader节点（在paxos中为”proposer”）负责协调（即2PC中的coordination），其他节点则为follower（在paxos中为”accptors”或者”leaderners”）。\n\n> Epoch(轮次)\n\nPaxos算法和Raft算法每一轮正常的流程称为epoch（Raft中为”term”)。在对应每一个epoch期间只有一个节点被指定为leader。\n\n![epoch](/media/files/2014/09/epoch.png)\n\n在选举完成之后，同一个leader始终会成为该轮次（epoch）阶段的coordinator（协调者）。从上图(摘自Raft)可以看到，leader节点的宕机会导致该轮次（epoch）立即结束。\n\nEpoch(轮次)在协议中充当逻辑时钟。这样可以允许节点能够辨别某些宕机或者delay之后继续加入的节点—“那些被分区或者停止运作的节点对应的epoch会比当前的小”；这使得某些尚未成功提交的请求被忽略，以确保不会使系统产生二义性。\n\n> Leader changes via duels\n\n所有节点刚开始的角色都是follower；在启动之后其中一个节点会被选举会leader。在正常操作流程中，leader会保持和follower之间的心跳以使系统可以检测leader失效或者产生网络分区。\n\n当follower节点检测到leader无响应，它会切换到中间状态（Raft中成为”candidate”状态）。在这个状态下，节点对当前自身的epoch/term做自增（epoch++），并发起leader选举竞选成为此轮epoch新的leader。\n\n为了成为leader，必须获得过半数的投票。分配选票的方式为FIFO方式，leader会最终被选举出来。一般来说，在每次竞选尝试中会随机等待一段时间以减少同时进行竞选的节点数。\n\n> Numbered proposals within an epoch（一个轮次中带编号的请求）\n\n在每一轮次中，leader会对每次需要表决的值进行提案（即序列化update command），在每一个轮次中，每个提案对应的数字是唯一且严格递增的。\n\n> Normal operation\n\n在正常运行期间，所有提案都必须经过leader节点。当客户端提交一个提案（如更新操作），leader联系所有多数派中的节点，如果当前没有leader竞选请求存在（基于多数派中follower的响应），leader提案值有效。并且如果其多数派中的follower accept该提案，那么这个提案被接受。\n\n由于很可能另外一个节点也正尝试作为一个leader进行提案，必须保证，一旦一个提案被accept，它的值永远无法被改变。否则，一个已经成功提交的请求可能会被撤销。Lamport在paxos算法中描述如下：\n\n> P2: If a proposal with value v is chosen, then every higher-numbered proposal that is chosen has value v.\n\n这一限制需要所有的follower和propser遵循:一旦一个提案值被一个多数派接受，那么这个提案值不能被改变.（注意”提案值不能改变”对应于算法的一次执行。典型的复制算法对每一次提交执行一次一致性算法，为了解释得更加简单易懂，往往大家专注于算法的一直执行进行详细讨论）\n\n为了保证这个特性，提案者必须首先询问follower他们已经接受的编号最大的提案对应的值。如果提案者发现已经存在一个提案，那么它必须试图完成这个已经存在的提案，而不是进行重新提案。Lamport在paxos算法中描述如下：\n\n> P2b. If a proposal with value v is chosen, then every higher-numbered proposal issued by any proposer has value v.\n\n更加具体的：\n\n> P2c. For any v and n, if a proposal with value v and number n is issued [by a leader], then there is a set S consisting of a majority of acceptors [followers] such that either (a) no acceptor in S has accepted any proposal numbered less than n, or (b) v is the value of the highest-numbered proposal among all proposals numbered less than n accepted by the followers in S.\n\n这是Paxos算法的核心，同样也是其他类Paxos衍生算法的核心。提案的值直到协议的第二个阶段才能选定。提案者某些情况下必须重新进行第一阶段以保证他们可以自由得对当前一轮提案赋予自己的值。\n\n如果之前已经有多个提案存在(可能是还未决定的提案)，那么会选举标号最大的提案对应的值。提案者只有在其多数派中的节点没有一个提案竞争者的前提下才能选取自己的提案值。同时提案者要求follower见到此提案的同时，不能accept比这个提案编号更小的提案。\n\n把这两个部分结合起来，在Paxos算法中达成一个决定需要两轮消息交互。\n\n[ Proposer ] -> Prepare(n)]                                      [ Followers ]\n\n<- Promise(n; previous proposal number\nand previous value if accepted a\nproposal in the past)\n\n[ Proposer ] -> AcceptRequest(n, own value or the value          [ Followers ]\n\nassociated with the highest proposal number\nreported by the followers)\n\n<- Accepted(n, value)\n\n\n在prepare阶段，proposer（提案者）可以了解任何处于竞争状态或者之前已经决定的提案。第二个阶段（accept阶段）选举一个新的值或者之前已经被accept过的值。在某些情况下，假设同时两个proposer处于active状态（即同时进行提案）或者多数派节点故障情况下，可能没有一个propossal被多数派accept。但是这是可以接受的，因为成功的提案最终会收敛为一个有效的值。\n\n事实上，根据FLP理论，当消息传递边界不存在的情况下，一致性算法只能在safety或者liveness间二选一。Paxos算法选择放弃liveness保证safety：即提案可能无休止的进行下去，直到没有竞争leader（多个proposal）并且一个多数派accept提案。\n\n当然，实现这种算法非常复杂，即使在专家手中，一些很小的关注点可能会导致非常大的代码量。\n\n实用优化：\n\n* 通过leader租期（而不是心跳）避免重复的leader选举。\n* 在leader确定的稳定状态下避免第一阶段的propose消息交互。\n* 确保follower和proposers持久化的消息不被损坏。\n* 集群中节点的角色以安全的方式转换（在Paxos算法中依赖任意一个多数派总是有一个节点是相交的）\n* 在副本节点crash，磁盘丢失或者新节点加入情况下需要以安全有效的方式进行副本恢复。\n* 在合理的时间（均衡存储、容错需求）进行快照以及垃圾回收需要保证安全性。\n\ngoogle “Paxos made live”这篇文章详细讨论了这系列挑战。\n\n### 2.4 Partition-tolerant consensus algorithms: Paxos, Raft, ZAB\n\n希望以上的讨论让你对“分区容忍性算法”如何工作有了基本的认识。我建议你通过进一步阅读了解不同算法的实现细节。\n\n#### 2.4.1 Paxos\n\npaxos算法是设计具备分区容忍特性的强一致性系统必须了解的算法。该算法被许多google的系统使用。[BigTable/Megastore][5]、GFS、[Spanner][6]中使用的[Chubby lock Mananger][7]。\n\nPaxos以希腊的一座岛屿的名词命名。由Leslie Lamport在1998年发表的”The Part-Time Parliament”文章中首次发表。Paxos算法被认为难以实现的，所以后续工业界发表了一系列文章探讨其实现细节（在后面的further reading 中可以看到）。\n\nPaxos算法中描述的往往是一致性算法的一次执行。而实际执行过程中，往往需要考虑高效得运行多轮一致性算法。这使得很多有兴趣（致力于）搭建基于Paxos协议的系统的开发者设计了很多基于paxos协议的扩展协议。此外，实际实现过程中还有很多挑战，比如如何维护集群成员的成员关系等。\n\n#### 2.4.2 ZAB\n\nZAB是Apache Zookeeper所使用的原子广播协议。Zookeeper 为分布式系统提供了协调者的角色（coordination primitives）。很多基于Hadoop的分布式系统（HBase、Storm、Kafka）都使用Zookeeper作为协调者（coordination）。Zookeeper基本上算是Chubby的开源实现版本。从技术上来讲原子广播和单纯的一致性协议问题有所不同，但是这类算法同样归属于”强一致性分布式容错算法”。\n\n#### 2.4.3 Raft\n\nRaft是近期（2013年）加入本家族的算法。Raft比Paxos算法更加易于理解和学习，但是提供和Paxos算法同样的保证。特别的是，该算法的不同的部分被更加清理得分离开来，发表的paper中详细讨论了集群成员关系的变化。Raft在近期被类似zookeeper的开源系统etcd使用。\n\n### 2.5 Replication methods with strong consistency\n\n在以上本章节中，我们讨论了强一致性复制方法。从同步和异步开始对比，从简单开始逐步到能够容忍更加复杂故障的算法。以下总结了各种算法的关键特征：\n\nPrimary/Backup\n\n* Single, static master\n* Replicated log, slaves are not involved in executing operations\n* No bounds on replication delay\n* Not partition tolerant\n* Manual/ad-hoc failover, not fault tolerant, “hot backup”\n\n2PC\n\n* Unanimous vote: commit or abort\n* Static master\n* 2PC cannot survive simultaneous failure of the coordinator and a node during a commit\n* Not partition tolerant, tail latency sensitive\n\nPaxos\n\n* Majority vote\n* Dynamic master\n* Robust to n/2-1 simultaneous failures as part of protocol\n* Less sensitive to tail latency\n\n#### 2.5.1 Further reading\n\n**Primary-backup and 2PC**\n\n* [Replication techniques for availability][8] - Robbert van Renesse & Rachid Guerraoui, 2010\n\n* [Concurrency Control and Recovery in Database Systems][9]\n\n**Paxos**\n\n* [The Part-Time Parliament][10] – Leslie Lamport\n* [Paxos Made Simple][11] – Leslie Lamport, 2001\n* [Paxos Made Live - An Engineering Perspective][12] – Chandra et al\n* [Paxos Made Practical][13] – Mazieres, 2007\n* [Revisiting the Paxos Algorithm][14] – Lynch et al\n* [How to build a highly available system with consensus][15] – Butler Lampson\n* [Reconfiguring a State Machine - Lamport et al – changing][16] cluster membership\n* [Implementing Fault-Tolerant Services Using the State Machine Approach: a Tutorial][17] – Fred Schneider\n\n**Raft and ZAB**\n\n* [In Search of an Understandable Consensus Algorithm][18], Diego Ongaro, John Ousterhout, 2013\n* [Raft Lecture][19] – User Study\n* [A simple totally ordered broadcast protocol][20]- Junqueira, Reed\n* [ZooKeeper Atomic Broadcast][21]\n","slug":"stroage_consistency_replicate","published":1,"date":"2014-09-01T16:00:00.000Z","updated":"2015-03-27T00:22:41.000Z","comments":1,"photos":[],"link":"","_id":"cigortta3000xkfy76phgltfq"},{"title":" 存储系统一致性与可用性（二）","layout":"post","_content":"\n\n# 一个分布式kv引擎中使用的一致性协议\n\n发现现在很多开源系统在高可用，性能，可扩展性等很多方面没有一个很完善的方案。希望设计一个 k/v 存储系统，系统支持强一致性，并且在 C，A，P 这三方面都较为均衡，系统具有一定可扩展性。\n\n以下详细介绍自己实现的一个分布式kv系统中使用的分布式协议方案，系统使用 Primary/backup 与 Qurom 协议相结合的协议方案来实现系统的强一致性及高可用性(Qurom 协议 + leader(Master))。\n\n## 1 分布式协议\n\n### 1.1 协议正常运行流程\n\n以下以多数派为3情况即三个副本情况进行举例说明。\n\n![master](/media/files/2014/08/master.png)\n\nLeader在接收到对应的写请求W之后，首先对W进行序列化（与Basic Paxos协议相比，Leader这里的行为相当于其proposer和acceptor），然后将W 记录到日志中，并且是同步的方式（与basic paxos协议相比leader这里的行为相当于learner），同时在记录日志的同时，Leader并行得将序列化之后的写操作W 发送到各个Follower。\n\nFollower在接受到对应的proposal W之后判断当前接收到的W的序列号是否是递增的（即是否比前一请求的序列号大1），如果是，将对应的的W同步到日志中，并且应答Leader。（与Basic Paxos相比，Follower这里的行为相当于是acceptor和learner）。\n\nLeader在对应的W已经同步到日志中并且至少接收到半数Follower的应答后，就可以将对应的写W放入到memtable，然后应答客户端。\n\n至此，client可以从Leader上查询到最新提交的值，但是在Follower却不能，因为Follower并不知道有一个多数派已经成功提交了写请求，所以不能将对应的W放入memtable。\n\n所以Leader会周期性得将对应的已经提交memtable的最大的序列号(commit 点)发送到对应的Follower，Follower在接收到该消息之后，将小于等于该序列号的所有日志放入到memtable中，此时，才可以在Follower上查询到W的最新值。\n\npoints：\n\n* 在副本数为3的情况下，只要有两个节点存活(即多数派存活)，系统可继续正常运行。 \n* 写请求之间的发送是并行的，没有必要等第一个请求完成之后再发送第二个请求。 \n* 对磁盘的写操作和数据发送过程是并行的。 \n\n可以看到以上协议，在副本书为3情况下，只需要一个RTT(Round-Trip-Time)即可完成一次成功得写入操作\n\n更多优化：采用group commit 的方式在一个请求中多包装几次写请求。\n\n### 1.2 Leader选举与恢复\n\nLeader 选举与恢复成功的前提是：保证之前已经成功提交的数据不能丢失。 （因为这些数据很可能已经成功返回给客户端了，系统不能否认已经成功执行的写操作）。\n\n在有三个副本的情况下，至少有两个节点将数据同步到日志中才能成功返回客户端， 所以在 Leader 宕机之后， 只要有一个多数派存活 （这里为 2 个节点） ， 那么其中至少一个节点包含了当前所有已经成功提交的数据。所以 Leader 选举算法的基本原理为：在当前有一个多数派 Follower 参与的情况下 （当前情况为 2 个节点） ， 选举 LSN(log sequence number) 最大的节点作为新的 Leader即可。如下图中节点 B 满足条件被选举为新的 Leader。\n\nLeader 宕机之后重新选举出来的 Leader 必须进行恢复。 如下图所示， 在 B 成为 Leader之后，对于从 CMT (commit点)到 LSN 之间的数据都必须重新提案执行。因为处于 CMT 与 LSN 之间的数据可能已经提交成功返回客户端 (CMT 不是实时持久化)。也可能是还没有提交成功的数据，为了保证可用性， 即已经成功写入的数据在系统恢复之后状态还是成功写入， Leader 必须将CMT 与 LSN 之间的数据进行恢复， 在至少一个 Follower 追上 Leader 的时候， 系统才能重新形成一个能够正常运行的多数派，才能够继续接受客户端的写操作。因为 follower 只能接收序列号连贯的请求， 若 follower 没有追上 Leader， 那么即使 Leader继续接收写请求，也不能得到 follower 的正常应答） 。\n\n![leaderfailover](/media/files/2014/08/leaderfailover.png)\n\nps: 可以看到这里 CMT和LSN的格式都是类似1.20这样的格式，这里1为epoch，20为序列号，epoch的作用在follower恢复小结会介绍。\n\n### 1.3 Leader选举与恢复\n\nFollower 的恢复与本地恢复方式有所不同，这里的恢复分为本地恢复和远程恢复。\n\n![followerfailover](/media/files/2014/08/followerfailover.png)\n\n如图2.6所示，checkpoint之前的数据已经全部持久化存入数据库引擎，没有必要再进行恢复，对于checkpoint与CMT之间的数据，都已经成功提交，所以可以从本地日志中直接读取进行恢复，在CMT与LSN之间的数据为尚未确认的数据，需要进行远程恢复；这些数据可能是已经成功提交的数据，也有可能需要被恢复的数据，还有可能需要被抛弃的数据。\n\n* 成功提交数据的情况\n\n![follower1](/media/files/2014/08/follower1.png)\n\n如上图所示，在 state0 状态，三个节点正常运行，并且日志都已经记录到 1.30；但是在 state1 状态 Follower B 宕机， 还有一个 Follower 存活， 系统可继续运行， 随后 Leader A 收到之前发送写请求的应答 （LSN 为 1.21 到 1.30 的请求的应答），更新 CMT 为 1.30， 并且将CMT 更新发送到 Follower C； 到了 state2 状态， Follower B 恢复， 此时其从 CMT 到 LSN 的日志记录 （l.20 到 1.30） 需要进行恢复， Follower 向 Leader 发送当前需要确认的日志记录的LSN （即 1.21） ， Leader 收到请求后将 LSN 为 1.21 到 1.30 的日志记录打包发送给 Follower，Follower 收到之后判断当前日志是否已经记录，对于 LSN 为 1.21 到 1.30 的数据，显然Follower 都已经记录，无需重复记录，并且由于这些日志记录的状态为 CMT，所以直接更新 CMT。\n\n* 需要抛弃数据的情况\n\n![follower2](/media/files/2014/08/follower2.png)\n\n如上图 所示，在 state1 的时候 A,C 两节点的宕机，系统暂停服务，在 state2 的时候A 节点恢复，系统进行 leader 选举，A 成为 leader 并进行恢复后继续接受接收了一些写请求。在 state 3 的时候 C 节点恢复。但是此时 A,B 两节点的从 1.30 到 1.40 之间的数据与 C节点对应阶段的数据其实是不一致的。 如果再进行一次宕机， 即 A 宕机器之后， 虽然有 B,C两个节点存活，其实这个系统进入了一个不一致的模棱两可的状态（即实际上 B 节点的1.20~1.40 直接的数据是有效的，但是在只有 B，C 两节点情况下）。\n\n那么如何来解决这个问题呢？ 引入 epoch（选举轮次） 来解决这个问题， 在每次 Leader 宕机恢复之后， 升级 epoch 之后才能够继续对新的写请求进行序列化。这样就能够重用之前使用过的序列号(sequence)，而不造成节点恢复时可能造成的不一致性。\n\n如下图所示，在 state3 节点 C 进行恢复的时候，对于 1.21 至 1.31 的数据能够正常恢复，但是对于其记录的从 1.32 至 1.40 的数据由于之前的 Leader 宕机并且对序列号为 22 至40 的数据重新进行了序列化，所以其 1.30 到 1.40 的数据作废，必须从 Leader 获取从 2.32到 2.40 的数据进行恢复，然后进入下一状态才能继续正常运行。\n\n![follower3](/media/files/2014/08/follower3.png)\n\n### 1.4 关于多leader无法避免的问题\n\n简单的问题在分布式环境中就会变得不那么简单。 Leader 的选举问题有时候无法避免出现两个 leader 的情况。\n\n![multileader](/media/files/2014/08/multileader.png)\n\n在 state1 的时候，A 出现网络异常，刷新 leader 租赁期超时。这个时候 A 在这个超时事件上结束 leader A，但是这时候由于操作系统进程调度等方面的原因， A 节点还是处于leader 状态，B 与 C 发现在 zookeeper 上的 leader 节点丢失，进行 leader 选举，C 节点成为 leader， 这个时候系统中出现了两个 leader （这种情况就是典型的脑裂情况） 。 这个时候如果继续运行下去，系统会出错。\n\n那么为了保证一致性。提高系统的可用性，只能退而求其次，在同时存在两 Leader 的情况下，保证只有一个 Leader 能够正常运行。\n\n本协议是一多数派协议， 有天然的抗脑裂特性。 其实 B 节点只可能与一个对应的 Leader建立相匹配的 follower 关系，也就是说，在同一时刻，A,C 节点只可能有一个节点与节点 B建立可运行的多数派，所以能够保证写的一致性。\n\n对于读，显然也不能通过仅仅读取 leader 来保证数据强一致。 强一致性读实现方式：\n\nleader 读数据 + zookeeper 确认 leader：但是这种方式在系统分布式处理的时候会使得系统的可扩展性瓶颈在 zookeeper。 （ps： 协议借助 zookeeper 尽心 leader选举，zookeeper 上有最新的 leader 的信息） leader 读数据 + follower 读状态方式实现强一致性读： 即从 leader 读最新的数据，然后随便读一个 follower， 查看 leader 与 follower 的选举轮次号 epoch 是否一致。一致，则返回数据，不一致则要主动去 zookeeper 上查找当前 leader 节点信息，从当前新的 leader 上读取最新数据。 \n\n### 1.5 节点状态变迁\n\n![statechange](/media/files/2014/08/statechange.png)","source":"_posts/2014-08-12-stroage_consistency_avaliable_post2.markdown","raw":"---\ntitle: ' 存储系统一致性与可用性（二）'\nlayout: post\ntags:\n    - storage\n    - consistency\n    - avaliable\n---\n\n\n# 一个分布式kv引擎中使用的一致性协议\n\n发现现在很多开源系统在高可用，性能，可扩展性等很多方面没有一个很完善的方案。希望设计一个 k/v 存储系统，系统支持强一致性，并且在 C，A，P 这三方面都较为均衡，系统具有一定可扩展性。\n\n以下详细介绍自己实现的一个分布式kv系统中使用的分布式协议方案，系统使用 Primary/backup 与 Qurom 协议相结合的协议方案来实现系统的强一致性及高可用性(Qurom 协议 + leader(Master))。\n\n## 1 分布式协议\n\n### 1.1 协议正常运行流程\n\n以下以多数派为3情况即三个副本情况进行举例说明。\n\n![master](/media/files/2014/08/master.png)\n\nLeader在接收到对应的写请求W之后，首先对W进行序列化（与Basic Paxos协议相比，Leader这里的行为相当于其proposer和acceptor），然后将W 记录到日志中，并且是同步的方式（与basic paxos协议相比leader这里的行为相当于learner），同时在记录日志的同时，Leader并行得将序列化之后的写操作W 发送到各个Follower。\n\nFollower在接受到对应的proposal W之后判断当前接收到的W的序列号是否是递增的（即是否比前一请求的序列号大1），如果是，将对应的的W同步到日志中，并且应答Leader。（与Basic Paxos相比，Follower这里的行为相当于是acceptor和learner）。\n\nLeader在对应的W已经同步到日志中并且至少接收到半数Follower的应答后，就可以将对应的写W放入到memtable，然后应答客户端。\n\n至此，client可以从Leader上查询到最新提交的值，但是在Follower却不能，因为Follower并不知道有一个多数派已经成功提交了写请求，所以不能将对应的W放入memtable。\n\n所以Leader会周期性得将对应的已经提交memtable的最大的序列号(commit 点)发送到对应的Follower，Follower在接收到该消息之后，将小于等于该序列号的所有日志放入到memtable中，此时，才可以在Follower上查询到W的最新值。\n\npoints：\n\n* 在副本数为3的情况下，只要有两个节点存活(即多数派存活)，系统可继续正常运行。 \n* 写请求之间的发送是并行的，没有必要等第一个请求完成之后再发送第二个请求。 \n* 对磁盘的写操作和数据发送过程是并行的。 \n\n可以看到以上协议，在副本书为3情况下，只需要一个RTT(Round-Trip-Time)即可完成一次成功得写入操作\n\n更多优化：采用group commit 的方式在一个请求中多包装几次写请求。\n\n### 1.2 Leader选举与恢复\n\nLeader 选举与恢复成功的前提是：保证之前已经成功提交的数据不能丢失。 （因为这些数据很可能已经成功返回给客户端了，系统不能否认已经成功执行的写操作）。\n\n在有三个副本的情况下，至少有两个节点将数据同步到日志中才能成功返回客户端， 所以在 Leader 宕机之后， 只要有一个多数派存活 （这里为 2 个节点） ， 那么其中至少一个节点包含了当前所有已经成功提交的数据。所以 Leader 选举算法的基本原理为：在当前有一个多数派 Follower 参与的情况下 （当前情况为 2 个节点） ， 选举 LSN(log sequence number) 最大的节点作为新的 Leader即可。如下图中节点 B 满足条件被选举为新的 Leader。\n\nLeader 宕机之后重新选举出来的 Leader 必须进行恢复。 如下图所示， 在 B 成为 Leader之后，对于从 CMT (commit点)到 LSN 之间的数据都必须重新提案执行。因为处于 CMT 与 LSN 之间的数据可能已经提交成功返回客户端 (CMT 不是实时持久化)。也可能是还没有提交成功的数据，为了保证可用性， 即已经成功写入的数据在系统恢复之后状态还是成功写入， Leader 必须将CMT 与 LSN 之间的数据进行恢复， 在至少一个 Follower 追上 Leader 的时候， 系统才能重新形成一个能够正常运行的多数派，才能够继续接受客户端的写操作。因为 follower 只能接收序列号连贯的请求， 若 follower 没有追上 Leader， 那么即使 Leader继续接收写请求，也不能得到 follower 的正常应答） 。\n\n![leaderfailover](/media/files/2014/08/leaderfailover.png)\n\nps: 可以看到这里 CMT和LSN的格式都是类似1.20这样的格式，这里1为epoch，20为序列号，epoch的作用在follower恢复小结会介绍。\n\n### 1.3 Leader选举与恢复\n\nFollower 的恢复与本地恢复方式有所不同，这里的恢复分为本地恢复和远程恢复。\n\n![followerfailover](/media/files/2014/08/followerfailover.png)\n\n如图2.6所示，checkpoint之前的数据已经全部持久化存入数据库引擎，没有必要再进行恢复，对于checkpoint与CMT之间的数据，都已经成功提交，所以可以从本地日志中直接读取进行恢复，在CMT与LSN之间的数据为尚未确认的数据，需要进行远程恢复；这些数据可能是已经成功提交的数据，也有可能需要被恢复的数据，还有可能需要被抛弃的数据。\n\n* 成功提交数据的情况\n\n![follower1](/media/files/2014/08/follower1.png)\n\n如上图所示，在 state0 状态，三个节点正常运行，并且日志都已经记录到 1.30；但是在 state1 状态 Follower B 宕机， 还有一个 Follower 存活， 系统可继续运行， 随后 Leader A 收到之前发送写请求的应答 （LSN 为 1.21 到 1.30 的请求的应答），更新 CMT 为 1.30， 并且将CMT 更新发送到 Follower C； 到了 state2 状态， Follower B 恢复， 此时其从 CMT 到 LSN 的日志记录 （l.20 到 1.30） 需要进行恢复， Follower 向 Leader 发送当前需要确认的日志记录的LSN （即 1.21） ， Leader 收到请求后将 LSN 为 1.21 到 1.30 的日志记录打包发送给 Follower，Follower 收到之后判断当前日志是否已经记录，对于 LSN 为 1.21 到 1.30 的数据，显然Follower 都已经记录，无需重复记录，并且由于这些日志记录的状态为 CMT，所以直接更新 CMT。\n\n* 需要抛弃数据的情况\n\n![follower2](/media/files/2014/08/follower2.png)\n\n如上图 所示，在 state1 的时候 A,C 两节点的宕机，系统暂停服务，在 state2 的时候A 节点恢复，系统进行 leader 选举，A 成为 leader 并进行恢复后继续接受接收了一些写请求。在 state 3 的时候 C 节点恢复。但是此时 A,B 两节点的从 1.30 到 1.40 之间的数据与 C节点对应阶段的数据其实是不一致的。 如果再进行一次宕机， 即 A 宕机器之后， 虽然有 B,C两个节点存活，其实这个系统进入了一个不一致的模棱两可的状态（即实际上 B 节点的1.20~1.40 直接的数据是有效的，但是在只有 B，C 两节点情况下）。\n\n那么如何来解决这个问题呢？ 引入 epoch（选举轮次） 来解决这个问题， 在每次 Leader 宕机恢复之后， 升级 epoch 之后才能够继续对新的写请求进行序列化。这样就能够重用之前使用过的序列号(sequence)，而不造成节点恢复时可能造成的不一致性。\n\n如下图所示，在 state3 节点 C 进行恢复的时候，对于 1.21 至 1.31 的数据能够正常恢复，但是对于其记录的从 1.32 至 1.40 的数据由于之前的 Leader 宕机并且对序列号为 22 至40 的数据重新进行了序列化，所以其 1.30 到 1.40 的数据作废，必须从 Leader 获取从 2.32到 2.40 的数据进行恢复，然后进入下一状态才能继续正常运行。\n\n![follower3](/media/files/2014/08/follower3.png)\n\n### 1.4 关于多leader无法避免的问题\n\n简单的问题在分布式环境中就会变得不那么简单。 Leader 的选举问题有时候无法避免出现两个 leader 的情况。\n\n![multileader](/media/files/2014/08/multileader.png)\n\n在 state1 的时候，A 出现网络异常，刷新 leader 租赁期超时。这个时候 A 在这个超时事件上结束 leader A，但是这时候由于操作系统进程调度等方面的原因， A 节点还是处于leader 状态，B 与 C 发现在 zookeeper 上的 leader 节点丢失，进行 leader 选举，C 节点成为 leader， 这个时候系统中出现了两个 leader （这种情况就是典型的脑裂情况） 。 这个时候如果继续运行下去，系统会出错。\n\n那么为了保证一致性。提高系统的可用性，只能退而求其次，在同时存在两 Leader 的情况下，保证只有一个 Leader 能够正常运行。\n\n本协议是一多数派协议， 有天然的抗脑裂特性。 其实 B 节点只可能与一个对应的 Leader建立相匹配的 follower 关系，也就是说，在同一时刻，A,C 节点只可能有一个节点与节点 B建立可运行的多数派，所以能够保证写的一致性。\n\n对于读，显然也不能通过仅仅读取 leader 来保证数据强一致。 强一致性读实现方式：\n\nleader 读数据 + zookeeper 确认 leader：但是这种方式在系统分布式处理的时候会使得系统的可扩展性瓶颈在 zookeeper。 （ps： 协议借助 zookeeper 尽心 leader选举，zookeeper 上有最新的 leader 的信息） leader 读数据 + follower 读状态方式实现强一致性读： 即从 leader 读最新的数据，然后随便读一个 follower， 查看 leader 与 follower 的选举轮次号 epoch 是否一致。一致，则返回数据，不一致则要主动去 zookeeper 上查找当前 leader 节点信息，从当前新的 leader 上读取最新数据。 \n\n### 1.5 节点状态变迁\n\n![statechange](/media/files/2014/08/statechange.png)","slug":"stroage_consistency_avaliable_post2","published":1,"date":"2014-08-11T16:00:00.000Z","updated":"2015-03-26T23:00:42.000Z","comments":1,"photos":[],"link":"","_id":"cigortta60014kfy7c1d01zkd"},{"title":" 存储系统一致性与可用性","layout":"post","_content":"\n### 1. 关于可用性\n\n提供系统可用性的关键是在相关组件失效情况下，系统能多快恢复并继续正确得提供服务。\n\n#### 1.1 如何恢复服务\n\nHow to recover a single node from power failure。\n\n* wait for reboot\n\nData is durable, but service is unavailable temporarily。\n\n* Use multiple nodes to provide service\n\nAnother node takes over to provide service, How to make sure nodes respond in the same way?\n\n对于无状态的服务器而言，没有关系。对于有状态的服务器而言,如何去保证 take over的服务器提供正确的服务?\n\n### 1.2 两种基本方法\n\n* 多副本\n* 纠删码\n纠删码基本原理是把一份数据分割成多份，计算出若干冗余块，比如 切割成26份数据再使用纠删算法计算4份冗余块，可以使得任意丢4块数据还能恢复数据进行服务，核心还是通过数据冗余来实现一定的高可用。以下内容只针对简单多副本展开讨论，本文不对纠删码进行详述。\n\n对于简单多副本要保证多个副本的状态是一致的，才能保证另一个副本对应服务器能提供正确的服务。如何实现多副本之间的一致性?以下讨论相关理论及实现.\n\n## 2. 理论\n\n实现多副本一致的一个基本理论为 replicated stat machine，即副本状态机：副本所处的初始状态相同；每个副本执行的操作顺序相同，并且每一个操作都相同； 那么最终所有副本的状态是一致。\n\n![OpaaOPb](/media/files/2014/07/OPaOPb.png)\n\n那如何保证在多个 client 并发操作下的保证操作的顺序性？以下分析 primary-backup协议，Qurom，paoxs 协议等协议\n\n\n\n\n### 2.1 Master Slave\n\nMaster / Slave 相对来讲是较简单和自然而然的方式，Master 决定操作的顺序，Slave 节点执行序列化之后的操作。\n\n![Primary.png](/media/files/2014/07/Primary.png)\n\n\nMaster Slave 协议较为简单， 但是其可用性不是很高， 在其中某一个节点宕机的情况下，系统无法继续运行。\n\n如下图所示，在 state0 状态 Master 与 Slave 的 LSN(log sequence number)为 10，处于一致的状态，在 state1 时 Slave 发生宕机，Master 继续接受写请求，到 state2 时 Master 也生宕机，在 state3 时 Slave 恢复，但是 Master 与 Slave 的状态不一致，所以 Slave 即不能提供读也不能提供写， 此时即发生阻塞。 若想继续提供服务， 必须进行人工干预。 Master/slaver\n协议是一种阻塞式的协议。 （数据库中的 “两阶段提交协议” 就是一种典型的阻塞式的协议）\n\n![3.png](/media/files/2014/07/3.png)\n\n\n### 2.2  Quroum/(NRW)\n\nQuorum 机制是一种简单有效的副本管理机制。NWR 协议是其中的典型，NWR 为Amazon 公司设计的分布式 kv 系统 dynamo 中使用的分布式副本协议。\n\n其中 N 为副本的数目， W 是写成功需要写的份数， R 为读成功需要读的份数， 保证 R+W>N，即能读到正确的数据。\n\n假设 N=3，W=1，R=3，即如果副本数目为 3，写 1 份即算成功，那么至少读 3 份才能读到写入的数据。 并且为了防止写丢失， 还必须用 “last write wins” 的方式解决写冲突问题。\n\nNRW 可能出现数据丢失情况\n\n如下图 所示， 例如用户先调用 OP1 写数据 obj1 的 A 副本成功返回， 然后用户调用 OP2写数据 obj1 的副本 C 成功后返回，此时使用”last write wins”的进行合并使得最终副本 A 和C 上的数据完全一致。但是如果 A 与 B 上的时钟不一致，比如 A 的时钟比 C 快，那么在数据整合的时候会出现先写的数据副本 A 上的操作覆盖后写的数据 C， 导致后写的 OP2 丢失，这显然不是用户想看到的信息。所以系统必须排除这种状况才能保证数据的最终一致性， 否则就是弱一致性。\n\n![qurom.png](/media/files/2014/07/qurom.png)\n\n\n时钟同步：在分布式系统中本身就是很难事件的东西。\n\n结论:NRW 其实也只是看上去很美的东西。但是 dynamo 为了使得系统能够有更好的可扩展性及可用性， 放宽对一致性的要求， 不支持强一致性， 而支持最终一致性甚至是弱一致性。 （弱一致性估计是很难使用户接受）\n\n\n### 2.3 paxos协议\n\npaxos 协议中每个节点都是对等的，每个节点都可以接收客户端的写请求，并尝试完成客户端的写请求，其核心是基于一个多数派的抢占机制式协议。\n\n![basicPaxos.png](/media/files/2014/07/basicPaxos.png)\n\n\n### 2.4 关于CAP\n\n各种分布式协议所面临的问题就是分布式文件系统的三大难题:副本一致性，系统可用性，分区容忍性（网络异常的容忍能力）。\n\nCAP理论明确提出了不要妄图设计一种对 CAP 三大属性都完全拥有的系统，因为这种系统在理论上就已经被证明不存在。设计系统的时候需要在 C、A、P 这三方面有所折中。\n\nPrimary/backup：MySQL：具有完全的 C，很糟糕的 A，很糟糕的 P， （任何一个节点宕机都会导致服务中断）\nQuroum 协议：Dynamo/cassandra，有一定的 C，有较好的 A，也有较好的 P，是一种较为平衡的分布式协议。\nPaxos 协议：Spanner , Chubbuy, Zookeeper, megastore 具有完全的 C，较好的 A， 较好的 P。\n发现很多开源系统在高可用、性能、可扩展性等很多方面没有一个完善的方案。希望设计一个 k/v 存储系统，系统支持强一致性，并且在 C，A，P 这三方面都较为均衡，系统具有一定可扩展性，详见下文。\n\n\n\n\n","source":"_posts/2014-07-26-stroage_consistency_avaliable_post1.markdown","raw":"---\ntitle: ' 存储系统一致性与可用性'\nlayout: post\ntags:\n    - storage\n    - consistency\n    - avaliable\n---\n\n### 1. 关于可用性\n\n提供系统可用性的关键是在相关组件失效情况下，系统能多快恢复并继续正确得提供服务。\n\n#### 1.1 如何恢复服务\n\nHow to recover a single node from power failure。\n\n* wait for reboot\n\nData is durable, but service is unavailable temporarily。\n\n* Use multiple nodes to provide service\n\nAnother node takes over to provide service, How to make sure nodes respond in the same way?\n\n对于无状态的服务器而言，没有关系。对于有状态的服务器而言,如何去保证 take over的服务器提供正确的服务?\n\n### 1.2 两种基本方法\n\n* 多副本\n* 纠删码\n纠删码基本原理是把一份数据分割成多份，计算出若干冗余块，比如 切割成26份数据再使用纠删算法计算4份冗余块，可以使得任意丢4块数据还能恢复数据进行服务，核心还是通过数据冗余来实现一定的高可用。以下内容只针对简单多副本展开讨论，本文不对纠删码进行详述。\n\n对于简单多副本要保证多个副本的状态是一致的，才能保证另一个副本对应服务器能提供正确的服务。如何实现多副本之间的一致性?以下讨论相关理论及实现.\n\n## 2. 理论\n\n实现多副本一致的一个基本理论为 replicated stat machine，即副本状态机：副本所处的初始状态相同；每个副本执行的操作顺序相同，并且每一个操作都相同； 那么最终所有副本的状态是一致。\n\n![OpaaOPb](/media/files/2014/07/OPaOPb.png)\n\n那如何保证在多个 client 并发操作下的保证操作的顺序性？以下分析 primary-backup协议，Qurom，paoxs 协议等协议\n\n\n\n\n### 2.1 Master Slave\n\nMaster / Slave 相对来讲是较简单和自然而然的方式，Master 决定操作的顺序，Slave 节点执行序列化之后的操作。\n\n![Primary.png](/media/files/2014/07/Primary.png)\n\n\nMaster Slave 协议较为简单， 但是其可用性不是很高， 在其中某一个节点宕机的情况下，系统无法继续运行。\n\n如下图所示，在 state0 状态 Master 与 Slave 的 LSN(log sequence number)为 10，处于一致的状态，在 state1 时 Slave 发生宕机，Master 继续接受写请求，到 state2 时 Master 也生宕机，在 state3 时 Slave 恢复，但是 Master 与 Slave 的状态不一致，所以 Slave 即不能提供读也不能提供写， 此时即发生阻塞。 若想继续提供服务， 必须进行人工干预。 Master/slaver\n协议是一种阻塞式的协议。 （数据库中的 “两阶段提交协议” 就是一种典型的阻塞式的协议）\n\n![3.png](/media/files/2014/07/3.png)\n\n\n### 2.2  Quroum/(NRW)\n\nQuorum 机制是一种简单有效的副本管理机制。NWR 协议是其中的典型，NWR 为Amazon 公司设计的分布式 kv 系统 dynamo 中使用的分布式副本协议。\n\n其中 N 为副本的数目， W 是写成功需要写的份数， R 为读成功需要读的份数， 保证 R+W>N，即能读到正确的数据。\n\n假设 N=3，W=1，R=3，即如果副本数目为 3，写 1 份即算成功，那么至少读 3 份才能读到写入的数据。 并且为了防止写丢失， 还必须用 “last write wins” 的方式解决写冲突问题。\n\nNRW 可能出现数据丢失情况\n\n如下图 所示， 例如用户先调用 OP1 写数据 obj1 的 A 副本成功返回， 然后用户调用 OP2写数据 obj1 的副本 C 成功后返回，此时使用”last write wins”的进行合并使得最终副本 A 和C 上的数据完全一致。但是如果 A 与 B 上的时钟不一致，比如 A 的时钟比 C 快，那么在数据整合的时候会出现先写的数据副本 A 上的操作覆盖后写的数据 C， 导致后写的 OP2 丢失，这显然不是用户想看到的信息。所以系统必须排除这种状况才能保证数据的最终一致性， 否则就是弱一致性。\n\n![qurom.png](/media/files/2014/07/qurom.png)\n\n\n时钟同步：在分布式系统中本身就是很难事件的东西。\n\n结论:NRW 其实也只是看上去很美的东西。但是 dynamo 为了使得系统能够有更好的可扩展性及可用性， 放宽对一致性的要求， 不支持强一致性， 而支持最终一致性甚至是弱一致性。 （弱一致性估计是很难使用户接受）\n\n\n### 2.3 paxos协议\n\npaxos 协议中每个节点都是对等的，每个节点都可以接收客户端的写请求，并尝试完成客户端的写请求，其核心是基于一个多数派的抢占机制式协议。\n\n![basicPaxos.png](/media/files/2014/07/basicPaxos.png)\n\n\n### 2.4 关于CAP\n\n各种分布式协议所面临的问题就是分布式文件系统的三大难题:副本一致性，系统可用性，分区容忍性（网络异常的容忍能力）。\n\nCAP理论明确提出了不要妄图设计一种对 CAP 三大属性都完全拥有的系统，因为这种系统在理论上就已经被证明不存在。设计系统的时候需要在 C、A、P 这三方面有所折中。\n\nPrimary/backup：MySQL：具有完全的 C，很糟糕的 A，很糟糕的 P， （任何一个节点宕机都会导致服务中断）\nQuroum 协议：Dynamo/cassandra，有一定的 C，有较好的 A，也有较好的 P，是一种较为平衡的分布式协议。\nPaxos 协议：Spanner , Chubbuy, Zookeeper, megastore 具有完全的 C，较好的 A， 较好的 P。\n发现很多开源系统在高可用、性能、可扩展性等很多方面没有一个完善的方案。希望设计一个 k/v 存储系统，系统支持强一致性，并且在 C，A，P 这三方面都较为均衡，系统具有一定可扩展性，详见下文。\n\n\n\n\n","slug":"stroage_consistency_avaliable_post1","published":1,"date":"2014-07-25T16:00:00.000Z","updated":"2015-06-27T05:36:35.000Z","comments":1,"photos":[],"link":"","_id":"cigorttaa0018kfy7903q54v5"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cigortt870000kfy7posfs523","tag_id":"cigortt8d0001kfy7u4tm924f","_id":"cigortt8f0004kfy7g7imwdp6"},{"post_id":"cigortt870000kfy7posfs523","tag_id":"cigortt8f0002kfy761c9ldfz","_id":"cigortt8f0005kfy73y3cyeki"},{"post_id":"cigortt870000kfy7posfs523","tag_id":"cigortt8f0003kfy7j74v6vvp","_id":"cigortt8f0006kfy7tghlmqqv"},{"post_id":"cigortt9t000ckfy7xnslj86s","tag_id":"cigortt9u000dkfy7v6q8kwri","_id":"cigortt9u000fkfy7tatyx1ra"},{"post_id":"cigortt9t000ckfy7xnslj86s","tag_id":"cigortt9u000ekfy7izshuel3","_id":"cigortt9u000gkfy70xh5qyfk"},{"post_id":"cigortt9v000hkfy7hx6kv4ab","tag_id":"cigortt9v000ikfy7gfkxby8b","_id":"cigortt9v000kkfy78kieut01"},{"post_id":"cigortt9v000hkfy7hx6kv4ab","tag_id":"cigortt9v000jkfy79udl63lt","_id":"cigortt9w000lkfy7akr21jwk"},{"post_id":"cigortt9w000mkfy7k3w7mqum","tag_id":"cigortt8d0001kfy7u4tm924f","_id":"cigortt9x000okfy7azxt83ew"},{"post_id":"cigortt9w000mkfy7k3w7mqum","tag_id":"cigortt9x000nkfy7vp4zgrdg","_id":"cigortt9x000pkfy76y23mm2v"},{"post_id":"cigortt9y000qkfy7h65cqx0k","tag_id":"cigortt8d0001kfy7u4tm924f","_id":"cigortt9z000rkfy736yjgdgl"},{"post_id":"cigortt9y000qkfy7h65cqx0k","tag_id":"cigortt8f0003kfy7j74v6vvp","_id":"cigortta0000skfy7em7woowr"},{"post_id":"cigortta1000tkfy7kofgnjsg","tag_id":"cigortt8d0001kfy7u4tm924f","_id":"cigortta2000vkfy78107s654"},{"post_id":"cigortta1000tkfy7kofgnjsg","tag_id":"cigortta2000ukfy7dnvdv44f","_id":"cigortta2000wkfy7u06ull3d"},{"post_id":"cigortta3000xkfy76phgltfq","tag_id":"cigortta4000ykfy7tyfc2d70","_id":"cigortta40011kfy7opryqcks"},{"post_id":"cigortta3000xkfy76phgltfq","tag_id":"cigortta4000zkfy7ffcr3lbn","_id":"cigortta50012kfy7ocosn4k1"},{"post_id":"cigortta3000xkfy76phgltfq","tag_id":"cigortta40010kfy73n5jlh4n","_id":"cigortta50013kfy7kgj3lg5r"},{"post_id":"cigortta60014kfy7c1d01zkd","tag_id":"cigortta4000ykfy7tyfc2d70","_id":"cigortta80015kfy7gm75zxhz"},{"post_id":"cigortta60014kfy7c1d01zkd","tag_id":"cigortta4000zkfy7ffcr3lbn","_id":"cigortta90016kfy76qog80ir"},{"post_id":"cigortta60014kfy7c1d01zkd","tag_id":"cigortta40010kfy73n5jlh4n","_id":"cigortta90017kfy7pv02yy29"},{"post_id":"cigorttaa0018kfy7903q54v5","tag_id":"cigortta4000ykfy7tyfc2d70","_id":"cigorttaa0019kfy7upb23gdm"},{"post_id":"cigorttaa0018kfy7903q54v5","tag_id":"cigortta4000zkfy7ffcr3lbn","_id":"cigorttaa001akfy7fsw2achu"},{"post_id":"cigorttaa0018kfy7903q54v5","tag_id":"cigortta40010kfy73n5jlh4n","_id":"cigorttaa001bkfy77du36rij"}],"Tag":[{"name":"golang","_id":"cigortt8d0001kfy7u4tm924f"},{"name":"nginx","_id":"cigortt8f0002kfy761c9ldfz"},{"name":"performance","_id":"cigortt8f0003kfy7j74v6vvp"},{"name":"云存储","_id":"cigortt9u000dkfy7v6q8kwri"},{"name":"CDN","_id":"cigortt9u000ekfy7izshuel3"},{"name":"交流","_id":"cigortt9v000ikfy7gfkxby8b"},{"name":"云存储、cdn","_id":"cigortt9v000jkfy79udl63lt"},{"name":"runtime","_id":"cigortt9x000nkfy7vp4zgrdg"},{"name":"goroutine","_id":"cigortta2000ukfy7dnvdv44f"},{"name":"storage","_id":"cigortta4000ykfy7tyfc2d70"},{"name":"consistency","_id":"cigortta4000zkfy7ffcr3lbn"},{"name":"avaliable","_id":"cigortta40010kfy73n5jlh4n"}]}}